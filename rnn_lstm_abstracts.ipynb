{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"id":"yjEtq_pVau5_","executionInfo":{"status":"ok","timestamp":1700538389753,"user_tz":360,"elapsed":139,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["import pandas as pd\n","import random\n","import torch\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"QuWT4cm3au6B","executionInfo":{"status":"ok","timestamp":1700538392070,"user_tz":360,"elapsed":151,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n","\n","    Args:\n","        seed_value (int): The seed value to set for random number generators.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","    # Additional steps for deterministic behavior\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Set the seed\n","set_seed(42)  # You can replace 42 with any other seed value of your choice\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CojPHh3Ra3x5","executionInfo":{"status":"ok","timestamp":1700538393800,"user_tz":360,"elapsed":1014,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"b39f17c7-d737-45d5-e851-0be12ce658bd"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"pbGtczHTau6C","executionInfo":{"status":"ok","timestamp":1700538436185,"user_tz":360,"elapsed":8829,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"61b6421d-4295-4c12-9055-4b8559779e8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Abstract Name                                               Text       Label\n","0   ###24491034  The emergence of HIV as a chronic condition me...  BACKGROUND\n","1   ###24491034  This paper describes the design and evaluation...  BACKGROUND\n","2   ###24491034  This study is designed as a randomised control...     METHODS\n","3   ###24491034  The intervention group will participate in the...     METHODS\n","4   ###24491034  The program is based on self-efficacy theory a...     METHODS"],"text/html":["\n","  <div id=\"df-b281bb68-f30f-4584-a2d6-dd2d8e6c43cc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Abstract Name</th>\n","      <th>Text</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>###24491034</td>\n","      <td>The emergence of HIV as a chronic condition me...</td>\n","      <td>BACKGROUND</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>###24491034</td>\n","      <td>This paper describes the design and evaluation...</td>\n","      <td>BACKGROUND</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>###24491034</td>\n","      <td>This study is designed as a randomised control...</td>\n","      <td>METHODS</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>###24491034</td>\n","      <td>The intervention group will participate in the...</td>\n","      <td>METHODS</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>###24491034</td>\n","      <td>The program is based on self-efficacy theory a...</td>\n","      <td>METHODS</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b281bb68-f30f-4584-a2d6-dd2d8e6c43cc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b281bb68-f30f-4584-a2d6-dd2d8e6c43cc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b281bb68-f30f-4584-a2d6-dd2d8e6c43cc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-96742776-c787-40b9-a131-94ae02c2184c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96742776-c787-40b9-a131-94ae02c2184c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-96742776-c787-40b9-a131-94ae02c2184c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":31}],"source":["train_data = pd.read_csv('/content/drive/MyDrive/NLP_Final_Project/Dataset/train.csv')\n","train_data.head()"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cRTZCAfau6C","executionInfo":{"status":"ok","timestamp":1700538436560,"user_tz":360,"elapsed":377,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"4b2cd883-58e3-46a3-ebee-c130652c9354"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RESULTS        765314\n","METHODS        713017\n","CONCLUSIONS    338732\n","BACKGROUND     191771\n","OBJECTIVE      186381\n","Name: Label, dtype: int64"]},"metadata":{},"execution_count":32}],"source":["train_data['Label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"9-e-dIIGau6D"},"source":["#### Preparing the indexed vocabulary of the text descriptions for one-hot representations"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"6D0139Wdau6D","executionInfo":{"status":"ok","timestamp":1700538452285,"user_tz":360,"elapsed":12885,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["word_to_idx = {}\n","idx = 0\n","max_sequence_length = 0\n","\n","for sent in train_data['Text']:\n","    # Tokenize the text\n","    sent_split = sent.split()\n","\n","    # Record the max sequence length to be later used for padding or truncating\n","    if len(sent_split) > max_sequence_length:\n","        max_sequence_length = len(sent_split)\n","\n","    # Record the unique index for each word in the textual descriptions\n","    for word in sent_split:\n","        if word not in word_to_idx:\n","            word_to_idx[word] = idx\n","            idx = idx + 1\n","\n","# Record the total input size to be length of the indexed dictionary\n","input_size = len(word_to_idx) + 2 # to be on the safer side"]},{"cell_type":"markdown","metadata":{"id":"HN0Jb4u_au6E"},"source":["#### Defining a transform to convert the ordinal target values to one-hot encoded torch tensors"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"pGuo0R7rau6E","executionInfo":{"status":"ok","timestamp":1700538464062,"user_tz":360,"elapsed":124,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["target_mapper = {\n","    \"BACKGROUND\" : 0,\n","    \"OBJECTIVE\" : 1,\n","    \"METHODS\" : 2,\n","    \"RESULTS\" : 3,\n","    \"CONCLUSIONS\" : 4\n","}\n","\n","# The transform function wil skip the word which is not present in the initial dictionary\n","def target_transform_func(target):\n","    mapped_target = target_mapper[target]\n","    target_onehot = torch.zeros(5, dtype=torch.float).scatter_(dim=0, index=torch.tensor(mapped_target), value=1)\n","    return target_onehot\n","\n","target_transform = transforms.Lambda(target_transform_func)"]},{"cell_type":"markdown","metadata":{"id":"MbAkGVvzau6E"},"source":["#### Function to pad or truncate text sequences to the specified length"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"gxKUjqy6au6E","executionInfo":{"status":"ok","timestamp":1700538466165,"user_tz":360,"elapsed":123,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["def pad_or_truncate(sequence, length):\n","    if len(sequence) < length:\n","        return torch.cat([sequence, torch.zeros(length - len(sequence), dtype=torch.long)])\n","    else:\n","        return sequence[:length]"]},{"cell_type":"markdown","metadata":{"id":"dE4RGsxKau6E"},"source":["#### Defining a transform to convert the textual descriptions to one-hot encoded torch tensors using the indexed voacbulary and pad/truncate them to recorded maximum length sequence"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"AbljthmWau6F","executionInfo":{"status":"ok","timestamp":1700538468893,"user_tz":360,"elapsed":124,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["# The transform function wil skip the word which is not present in the initial dictionary\n","def text_transform_func(text):\n","    sentence_onehot = [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","    sentence_onehot = torch.tensor(sentence_onehot, dtype=torch.long)\n","    sentence_onehot = pad_or_truncate(sentence_onehot, max_sequence_length)\n","    return sentence_onehot\n","\n","text_transform = transforms.Lambda(text_transform_func)"]},{"cell_type":"markdown","metadata":{"id":"7DhrhPg1au6F"},"source":["### Defining a sub-class of Dataset class to create dataset with the tranformation of text and label"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"JUIS6MvPau6F","executionInfo":{"status":"ok","timestamp":1700538470651,"user_tz":360,"elapsed":125,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["class AbstractDatasetText(Dataset):\n","    def __init__(self, dataset_file, transform=None, target_transform=None):\n","        # Read the data file at the start\n","        self.data = pd.read_csv(dataset_file)\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Reading the text description\n","        text = self.data['Text'][idx]\n","        # Reading the target label\n","        label = self.data['Label'][idx]\n","        # Apply the text transform to text description to get one-hot encoded torch tensors\n","        if self.transform:\n","            text_transformed = self.transform(text)\n","        # Apply target transform to labels to get the one-hot torch tensors from ordinal values\n","        if self.target_transform:\n","            label_transformed = self.target_transform(label)\n","        return text_transformed, label_transformed"]},{"cell_type":"markdown","metadata":{"id":"gfFyP4aMau6F"},"source":["### Creating the dataset and dataloaders for train, validation and test dataset"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"3v2X5Xjzau6F","executionInfo":{"status":"ok","timestamp":1700538485513,"user_tz":360,"elapsed":8956,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["# We make minibatches in a data loader and we specify the size of the minibatch here.\n","train_batch_size = 2048\n","test_batch_size = 64\n","val_batch_size = 64\n","\n","# Defining the dataset files\n","train_dataset_file = \"/content/drive/MyDrive/NLP_Final_Project/Dataset/train.csv\"\n","test_dataset_file = \"/content/drive/MyDrive/NLP_Final_Project/Dataset/test.csv\"\n","val_dataset_file = \"/content/drive/MyDrive/NLP_Final_Project/Dataset/dev.csv\"\n","\n","# Configure the dataset and dataloaders for training data\n","train_dataset_text = AbstractDatasetText(train_dataset_file, transform=text_transform, target_transform=target_transform)\n","train_loader_text = DataLoader(train_dataset_text, batch_size=train_batch_size, shuffle=True)\n","\n","# Configure the dataset and dataloaders for test data\n","test_dataset_text = AbstractDatasetText(test_dataset_file, transform=text_transform, target_transform=target_transform)\n","test_loader_text = DataLoader(test_dataset_text, batch_size=test_batch_size, shuffle=True)\n","\n","# Configure the dataset and dataloaders for validation data\n","val_dataset_text = AbstractDatasetText(val_dataset_file, transform=text_transform, target_transform=target_transform)\n","val_loader_text = DataLoader(val_dataset_text, batch_size=val_batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"mNrvsMVPau6F"},"source":["### Defining an RNN Model for training purposes on the multi-class classification"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CjmeWROxau6F","executionInfo":{"status":"ok","timestamp":1700538486423,"user_tz":360,"elapsed":913,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"c7268cf3-b1d2-4058-a433-649985bfec92"},"outputs":[{"output_type":"stream","name":"stdout","text":["AbstractsModel(\n","  (embedding): Embedding(468389, 200)\n","  (gru): GRU(200, 64, num_layers=2, batch_first=True)\n","  (fc1): Linear(in_features=64, out_features=32, bias=True)\n","  (fc2): Linear(in_features=32, out_features=16, bias=True)\n","  (fc3): Linear(in_features=16, out_features=5, bias=True)\n",")\n"]}],"source":["class AbstractsModel(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, num_classes=5):\n","        super(AbstractsModel, self).__init__()\n","        # Specifying Embedding and GRU layers to process the text data\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n","        # Specifying Linear Layer to process the text data after the RNN Layer\n","        self.fc1 = nn.Linear(hidden_size, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, num_classes)\n","\n","    def forward(self, x):\n","        # Applying embedding and RNN operation to text data for extraction of features\n","        out = self.embedding(x)\n","        out, _ = self.gru(out)\n","        # Apply mean pooling\n","        out = torch.mean(out, dim=1)\n","        # A fully connected layer to predict the outputs\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        return out\n","\n","# Initializing the Model Settings\n","embedding_size = 200\n","hidden_size = 64\n","num_layers = 2\n","\n","# Create an instance of the network\n","abstract_model = AbstractsModel(input_size, embedding_size, hidden_size, num_layers)\n","\n","# Print the model architecture\n","print(abstract_model)"]},{"cell_type":"markdown","metadata":{"id":"vClomnV-au6G"},"source":["### Training and Optimization"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJkfAOWLau6G","executionInfo":{"status":"ok","timestamp":1700535665973,"user_tz":360,"elapsed":2032331,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"1f99d769-03b4-4149-f588-815b19e1fcea"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch [1/5], Step [361/1072], Loss: 0.5212\n","Epoch [1/5], Step [362/1072], Loss: 0.4864\n","Epoch [1/5], Step [363/1072], Loss: 0.4927\n","Epoch [1/5], Step [364/1072], Loss: 0.4783\n","Epoch [1/5], Step [365/1072], Loss: 0.4852\n","Epoch [1/5], Step [366/1072], Loss: 0.5271\n","Epoch [1/5], Step [367/1072], Loss: 0.5046\n","Epoch [1/5], Step [368/1072], Loss: 0.4932\n","Epoch [1/5], Step [369/1072], Loss: 0.4970\n","Epoch [1/5], Step [370/1072], Loss: 0.4820\n","Epoch [1/5], Step [371/1072], Loss: 0.4948\n","Epoch [1/5], Step [372/1072], Loss: 0.4758\n","Epoch [1/5], Step [373/1072], Loss: 0.4881\n","Epoch [1/5], Step [374/1072], Loss: 0.5133\n","Epoch [1/5], Step [375/1072], Loss: 0.5058\n","Epoch [1/5], Step [376/1072], Loss: 0.5078\n","Epoch [1/5], Step [377/1072], Loss: 0.5096\n","Epoch [1/5], Step [378/1072], Loss: 0.4714\n","Epoch [1/5], Step [379/1072], Loss: 0.5101\n","Epoch [1/5], Step [380/1072], Loss: 0.4617\n","Epoch [1/5], Step [381/1072], Loss: 0.4585\n","Epoch [1/5], Step [382/1072], Loss: 0.5125\n","Epoch [1/5], Step [383/1072], Loss: 0.5162\n","Epoch [1/5], Step [384/1072], Loss: 0.4753\n","Epoch [1/5], Step [385/1072], Loss: 0.4965\n","Epoch [1/5], Step [386/1072], Loss: 0.4970\n","Epoch [1/5], Step [387/1072], Loss: 0.5003\n","Epoch [1/5], Step [388/1072], Loss: 0.4718\n","Epoch [1/5], Step [389/1072], Loss: 0.4932\n","Epoch [1/5], Step [390/1072], Loss: 0.5215\n","Epoch [1/5], Step [391/1072], Loss: 0.4825\n","Epoch [1/5], Step [392/1072], Loss: 0.4942\n","Epoch [1/5], Step [393/1072], Loss: 0.4926\n","Epoch [1/5], Step [394/1072], Loss: 0.4587\n","Epoch [1/5], Step [395/1072], Loss: 0.4714\n","Epoch [1/5], Step [396/1072], Loss: 0.4820\n","Epoch [1/5], Step [397/1072], Loss: 0.4496\n","Epoch [1/5], Step [398/1072], Loss: 0.4868\n","Epoch [1/5], Step [399/1072], Loss: 0.4824\n","Epoch [1/5], Step [400/1072], Loss: 0.4806\n","Epoch [1/5], Step [401/1072], Loss: 0.4989\n","Epoch [1/5], Step [402/1072], Loss: 0.4656\n","Epoch [1/5], Step [403/1072], Loss: 0.4868\n","Epoch [1/5], Step [404/1072], Loss: 0.4855\n","Epoch [1/5], Step [405/1072], Loss: 0.5075\n","Epoch [1/5], Step [406/1072], Loss: 0.4750\n","Epoch [1/5], Step [407/1072], Loss: 0.5351\n","Epoch [1/5], Step [408/1072], Loss: 0.5134\n","Epoch [1/5], Step [409/1072], Loss: 0.4738\n","Epoch [1/5], Step [410/1072], Loss: 0.4606\n","Epoch [1/5], Step [411/1072], Loss: 0.4972\n","Epoch [1/5], Step [412/1072], Loss: 0.5021\n","Epoch [1/5], Step [413/1072], Loss: 0.4953\n","Epoch [1/5], Step [414/1072], Loss: 0.4689\n","Epoch [1/5], Step [415/1072], Loss: 0.4692\n","Epoch [1/5], Step [416/1072], Loss: 0.4981\n","Epoch [1/5], Step [417/1072], Loss: 0.4622\n","Epoch [1/5], Step [418/1072], Loss: 0.5165\n","Epoch [1/5], Step [419/1072], Loss: 0.4854\n","Epoch [1/5], Step [420/1072], Loss: 0.4919\n","Epoch [1/5], Step [421/1072], Loss: 0.4718\n","Epoch [1/5], Step [422/1072], Loss: 0.4805\n","Epoch [1/5], Step [423/1072], Loss: 0.4736\n","Epoch [1/5], Step [424/1072], Loss: 0.4984\n","Epoch [1/5], Step [425/1072], Loss: 0.4567\n","Epoch [1/5], Step [426/1072], Loss: 0.4996\n","Epoch [1/5], Step [427/1072], Loss: 0.5085\n","Epoch [1/5], Step [428/1072], Loss: 0.4710\n","Epoch [1/5], Step [429/1072], Loss: 0.4695\n","Epoch [1/5], Step [430/1072], Loss: 0.4790\n","Epoch [1/5], Step [431/1072], Loss: 0.4889\n","Epoch [1/5], Step [432/1072], Loss: 0.4834\n","Epoch [1/5], Step [433/1072], Loss: 0.4899\n","Epoch [1/5], Step [434/1072], Loss: 0.4843\n","Epoch [1/5], Step [435/1072], Loss: 0.4953\n","Epoch [1/5], Step [436/1072], Loss: 0.4881\n","Epoch [1/5], Step [437/1072], Loss: 0.4903\n","Epoch [1/5], Step [438/1072], Loss: 0.5294\n","Epoch [1/5], Step [439/1072], Loss: 0.4598\n","Epoch [1/5], Step [440/1072], Loss: 0.4482\n","Epoch [1/5], Step [441/1072], Loss: 0.4796\n","Epoch [1/5], Step [442/1072], Loss: 0.4891\n","Epoch [1/5], Step [443/1072], Loss: 0.4842\n","Epoch [1/5], Step [444/1072], Loss: 0.4917\n","Epoch [1/5], Step [445/1072], Loss: 0.4671\n","Epoch [1/5], Step [446/1072], Loss: 0.4886\n","Epoch [1/5], Step [447/1072], Loss: 0.4905\n","Epoch [1/5], Step [448/1072], Loss: 0.4750\n","Epoch [1/5], Step [449/1072], Loss: 0.5167\n","Epoch [1/5], Step [450/1072], Loss: 0.4664\n","Epoch [1/5], Step [451/1072], Loss: 0.4747\n","Epoch [1/5], Step [452/1072], Loss: 0.4988\n","Epoch [1/5], Step [453/1072], Loss: 0.4977\n","Epoch [1/5], Step [454/1072], Loss: 0.4930\n","Epoch [1/5], Step [455/1072], Loss: 0.4464\n","Epoch [1/5], Step [456/1072], Loss: 0.4595\n","Epoch [1/5], Step [457/1072], Loss: 0.4663\n","Epoch [1/5], Step [458/1072], Loss: 0.4835\n","Epoch [1/5], Step [459/1072], Loss: 0.4593\n","Epoch [1/5], Step [460/1072], Loss: 0.4781\n","Epoch [1/5], Step [461/1072], Loss: 0.4490\n","Epoch [1/5], Step [462/1072], Loss: 0.4632\n","Epoch [1/5], Step [463/1072], Loss: 0.5378\n","Epoch [1/5], Step [464/1072], Loss: 0.4635\n","Epoch [1/5], Step [465/1072], Loss: 0.4372\n","Epoch [1/5], Step [466/1072], Loss: 0.4611\n","Epoch [1/5], Step [467/1072], Loss: 0.4866\n","Epoch [1/5], Step [468/1072], Loss: 0.4852\n","Epoch [1/5], Step [469/1072], Loss: 0.4896\n","Epoch [1/5], Step [470/1072], Loss: 0.4577\n","Epoch [1/5], Step [471/1072], Loss: 0.4682\n","Epoch [1/5], Step [472/1072], Loss: 0.4965\n","Epoch [1/5], Step [473/1072], Loss: 0.4856\n","Epoch [1/5], Step [474/1072], Loss: 0.4417\n","Epoch [1/5], Step [475/1072], Loss: 0.4627\n","Epoch [1/5], Step [476/1072], Loss: 0.4995\n","Epoch [1/5], Step [477/1072], Loss: 0.4775\n","Epoch [1/5], Step [478/1072], Loss: 0.4552\n","Epoch [1/5], Step [479/1072], Loss: 0.4557\n","Epoch [1/5], Step [480/1072], Loss: 0.4640\n","Epoch [1/5], Step [481/1072], Loss: 0.4766\n","Epoch [1/5], Step [482/1072], Loss: 0.4207\n","Epoch [1/5], Step [483/1072], Loss: 0.4911\n","Epoch [1/5], Step [484/1072], Loss: 0.4799\n","Epoch [1/5], Step [485/1072], Loss: 0.4553\n","Epoch [1/5], Step [486/1072], Loss: 0.4526\n","Epoch [1/5], Step [487/1072], Loss: 0.4739\n","Epoch [1/5], Step [488/1072], Loss: 0.4545\n","Epoch [1/5], Step [489/1072], Loss: 0.4766\n","Epoch [1/5], Step [490/1072], Loss: 0.4380\n","Epoch [1/5], Step [491/1072], Loss: 0.4820\n","Epoch [1/5], Step [492/1072], Loss: 0.4706\n","Epoch [1/5], Step [493/1072], Loss: 0.4982\n","Epoch [1/5], Step [494/1072], Loss: 0.4558\n","Epoch [1/5], Step [495/1072], Loss: 0.4400\n","Epoch [1/5], Step [496/1072], Loss: 0.4851\n","Epoch [1/5], Step [497/1072], Loss: 0.4672\n","Epoch [1/5], Step [498/1072], Loss: 0.4821\n","Epoch [1/5], Step [499/1072], Loss: 0.5013\n","Epoch [1/5], Step [500/1072], Loss: 0.4718\n","Epoch [1/5], Step [501/1072], Loss: 0.4909\n","Epoch [1/5], Step [502/1072], Loss: 0.4656\n","Epoch [1/5], Step [503/1072], Loss: 0.4798\n","Epoch [1/5], Step [504/1072], Loss: 0.4605\n","Epoch [1/5], Step [505/1072], Loss: 0.4817\n","Epoch [1/5], Step [506/1072], Loss: 0.4723\n","Epoch [1/5], Step [507/1072], Loss: 0.4890\n","Epoch [1/5], Step [508/1072], Loss: 0.4872\n","Epoch [1/5], Step [509/1072], Loss: 0.4940\n","Epoch [1/5], Step [510/1072], Loss: 0.4820\n","Epoch [1/5], Step [511/1072], Loss: 0.4695\n","Epoch [1/5], Step [512/1072], Loss: 0.4634\n","Epoch [1/5], Step [513/1072], Loss: 0.4769\n","Epoch [1/5], Step [514/1072], Loss: 0.4614\n","Epoch [1/5], Step [515/1072], Loss: 0.4504\n","Epoch [1/5], Step [516/1072], Loss: 0.4423\n","Epoch [1/5], Step [517/1072], Loss: 0.4313\n","Epoch [1/5], Step [518/1072], Loss: 0.4514\n","Epoch [1/5], Step [519/1072], Loss: 0.4596\n","Epoch [1/5], Step [520/1072], Loss: 0.5034\n","Epoch [1/5], Step [521/1072], Loss: 0.4626\n","Epoch [1/5], Step [522/1072], Loss: 0.4720\n","Epoch [1/5], Step [523/1072], Loss: 0.4375\n","Epoch [1/5], Step [524/1072], Loss: 0.4764\n","Epoch [1/5], Step [525/1072], Loss: 0.5016\n","Epoch [1/5], Step [526/1072], Loss: 0.4680\n","Epoch [1/5], Step [527/1072], Loss: 0.4378\n","Epoch [1/5], Step [528/1072], Loss: 0.4746\n","Epoch [1/5], Step [529/1072], Loss: 0.4560\n","Epoch [1/5], Step [530/1072], Loss: 0.4641\n","Epoch [1/5], Step [531/1072], Loss: 0.4830\n","Epoch [1/5], Step [532/1072], Loss: 0.4527\n","Epoch [1/5], Step [533/1072], Loss: 0.4672\n","Epoch [1/5], Step [534/1072], Loss: 0.4575\n","Epoch [1/5], Step [535/1072], Loss: 0.4450\n","Epoch [1/5], Step [536/1072], Loss: 0.4939\n","Epoch [1/5], Step [537/1072], Loss: 0.4774\n","Epoch [1/5], Step [538/1072], Loss: 0.4539\n","Epoch [1/5], Step [539/1072], Loss: 0.4903\n","Epoch [1/5], Step [540/1072], Loss: 0.4594\n","Epoch [1/5], Step [541/1072], Loss: 0.4766\n","Epoch [1/5], Step [542/1072], Loss: 0.4530\n","Epoch [1/5], Step [543/1072], Loss: 0.4687\n","Epoch [1/5], Step [544/1072], Loss: 0.4732\n","Epoch [1/5], Step [545/1072], Loss: 0.4653\n","Epoch [1/5], Step [546/1072], Loss: 0.4730\n","Epoch [1/5], Step [547/1072], Loss: 0.4551\n","Epoch [1/5], Step [548/1072], Loss: 0.4830\n","Epoch [1/5], Step [549/1072], Loss: 0.4489\n","Epoch [1/5], Step [550/1072], Loss: 0.4406\n","Epoch [1/5], Step [551/1072], Loss: 0.4619\n","Epoch [1/5], Step [552/1072], Loss: 0.4892\n","Epoch [1/5], Step [553/1072], Loss: 0.4859\n","Epoch [1/5], Step [554/1072], Loss: 0.4525\n","Epoch [1/5], Step [555/1072], Loss: 0.4894\n","Epoch [1/5], Step [556/1072], Loss: 0.4443\n","Epoch [1/5], Step [557/1072], Loss: 0.4538\n","Epoch [1/5], Step [558/1072], Loss: 0.4647\n","Epoch [1/5], Step [559/1072], Loss: 0.5038\n","Epoch [1/5], Step [560/1072], Loss: 0.4568\n","Epoch [1/5], Step [561/1072], Loss: 0.4308\n","Epoch [1/5], Step [562/1072], Loss: 0.4673\n","Epoch [1/5], Step [563/1072], Loss: 0.4506\n","Epoch [1/5], Step [564/1072], Loss: 0.4532\n","Epoch [1/5], Step [565/1072], Loss: 0.4676\n","Epoch [1/5], Step [566/1072], Loss: 0.4613\n","Epoch [1/5], Step [567/1072], Loss: 0.4387\n","Epoch [1/5], Step [568/1072], Loss: 0.4497\n","Epoch [1/5], Step [569/1072], Loss: 0.4502\n","Epoch [1/5], Step [570/1072], Loss: 0.4805\n","Epoch [1/5], Step [571/1072], Loss: 0.4178\n","Epoch [1/5], Step [572/1072], Loss: 0.4605\n","Epoch [1/5], Step [573/1072], Loss: 0.4396\n","Epoch [1/5], Step [574/1072], Loss: 0.4322\n","Epoch [1/5], Step [575/1072], Loss: 0.4381\n","Epoch [1/5], Step [576/1072], Loss: 0.4773\n","Epoch [1/5], Step [577/1072], Loss: 0.4446\n","Epoch [1/5], Step [578/1072], Loss: 0.4391\n","Epoch [1/5], Step [579/1072], Loss: 0.5066\n","Epoch [1/5], Step [580/1072], Loss: 0.4473\n","Epoch [1/5], Step [581/1072], Loss: 0.4569\n","Epoch [1/5], Step [582/1072], Loss: 0.4465\n","Epoch [1/5], Step [583/1072], Loss: 0.4475\n","Epoch [1/5], Step [584/1072], Loss: 0.4911\n","Epoch [1/5], Step [585/1072], Loss: 0.4742\n","Epoch [1/5], Step [586/1072], Loss: 0.4523\n","Epoch [1/5], Step [587/1072], Loss: 0.4454\n","Epoch [1/5], Step [588/1072], Loss: 0.4690\n","Epoch [1/5], Step [589/1072], Loss: 0.4378\n","Epoch [1/5], Step [590/1072], Loss: 0.4714\n","Epoch [1/5], Step [591/1072], Loss: 0.4868\n","Epoch [1/5], Step [592/1072], Loss: 0.4468\n","Epoch [1/5], Step [593/1072], Loss: 0.4614\n","Epoch [1/5], Step [594/1072], Loss: 0.4595\n","Epoch [1/5], Step [595/1072], Loss: 0.4361\n","Epoch [1/5], Step [596/1072], Loss: 0.4771\n","Epoch [1/5], Step [597/1072], Loss: 0.4499\n","Epoch [1/5], Step [598/1072], Loss: 0.4563\n","Epoch [1/5], Step [599/1072], Loss: 0.4677\n","Epoch [1/5], Step [600/1072], Loss: 0.4834\n","Epoch [1/5], Step [601/1072], Loss: 0.4664\n","Epoch [1/5], Step [602/1072], Loss: 0.4723\n","Epoch [1/5], Step [603/1072], Loss: 0.4538\n","Epoch [1/5], Step [604/1072], Loss: 0.4433\n","Epoch [1/5], Step [605/1072], Loss: 0.4651\n","Epoch [1/5], Step [606/1072], Loss: 0.4548\n","Epoch [1/5], Step [607/1072], Loss: 0.4303\n","Epoch [1/5], Step [608/1072], Loss: 0.4589\n","Epoch [1/5], Step [609/1072], Loss: 0.4443\n","Epoch [1/5], Step [610/1072], Loss: 0.5105\n","Epoch [1/5], Step [611/1072], Loss: 0.4316\n","Epoch [1/5], Step [612/1072], Loss: 0.4621\n","Epoch [1/5], Step [613/1072], Loss: 0.4262\n","Epoch [1/5], Step [614/1072], Loss: 0.4758\n","Epoch [1/5], Step [615/1072], Loss: 0.4810\n","Epoch [1/5], Step [616/1072], Loss: 0.4483\n","Epoch [1/5], Step [617/1072], Loss: 0.4629\n","Epoch [1/5], Step [618/1072], Loss: 0.4698\n","Epoch [1/5], Step [619/1072], Loss: 0.4495\n","Epoch [1/5], Step [620/1072], Loss: 0.4717\n","Epoch [1/5], Step [621/1072], Loss: 0.4560\n","Epoch [1/5], Step [622/1072], Loss: 0.4896\n","Epoch [1/5], Step [623/1072], Loss: 0.4118\n","Epoch [1/5], Step [624/1072], Loss: 0.4185\n","Epoch [1/5], Step [625/1072], Loss: 0.3969\n","Epoch [1/5], Step [626/1072], Loss: 0.4799\n","Epoch [1/5], Step [627/1072], Loss: 0.4507\n","Epoch [1/5], Step [628/1072], Loss: 0.4529\n","Epoch [1/5], Step [629/1072], Loss: 0.4601\n","Epoch [1/5], Step [630/1072], Loss: 0.4380\n","Epoch [1/5], Step [631/1072], Loss: 0.4475\n","Epoch [1/5], Step [632/1072], Loss: 0.4717\n","Epoch [1/5], Step [633/1072], Loss: 0.4438\n","Epoch [1/5], Step [634/1072], Loss: 0.4548\n","Epoch [1/5], Step [635/1072], Loss: 0.4811\n","Epoch [1/5], Step [636/1072], Loss: 0.4511\n","Epoch [1/5], Step [637/1072], Loss: 0.4351\n","Epoch [1/5], Step [638/1072], Loss: 0.4474\n","Epoch [1/5], Step [639/1072], Loss: 0.4446\n","Epoch [1/5], Step [640/1072], Loss: 0.4559\n","Epoch [1/5], Step [641/1072], Loss: 0.4573\n","Epoch [1/5], Step [642/1072], Loss: 0.4432\n","Epoch [1/5], Step [643/1072], Loss: 0.4164\n","Epoch [1/5], Step [644/1072], Loss: 0.4308\n","Epoch [1/5], Step [645/1072], Loss: 0.4214\n","Epoch [1/5], Step [646/1072], Loss: 0.4674\n","Epoch [1/5], Step [647/1072], Loss: 0.4575\n","Epoch [1/5], Step [648/1072], Loss: 0.4176\n","Epoch [1/5], Step [649/1072], Loss: 0.4443\n","Epoch [1/5], Step [650/1072], Loss: 0.4383\n","Epoch [1/5], Step [651/1072], Loss: 0.4993\n","Epoch [1/5], Step [652/1072], Loss: 0.4305\n","Epoch [1/5], Step [653/1072], Loss: 0.4579\n","Epoch [1/5], Step [654/1072], Loss: 0.4308\n","Epoch [1/5], Step [655/1072], Loss: 0.4441\n","Epoch [1/5], Step [656/1072], Loss: 0.4266\n","Epoch [1/5], Step [657/1072], Loss: 0.4537\n","Epoch [1/5], Step [658/1072], Loss: 0.4566\n","Epoch [1/5], Step [659/1072], Loss: 0.4523\n","Epoch [1/5], Step [660/1072], Loss: 0.4451\n","Epoch [1/5], Step [661/1072], Loss: 0.4630\n","Epoch [1/5], Step [662/1072], Loss: 0.4453\n","Epoch [1/5], Step [663/1072], Loss: 0.5003\n","Epoch [1/5], Step [664/1072], Loss: 0.4533\n","Epoch [1/5], Step [665/1072], Loss: 0.4404\n","Epoch [1/5], Step [666/1072], Loss: 0.4642\n","Epoch [1/5], Step [667/1072], Loss: 0.4583\n","Epoch [1/5], Step [668/1072], Loss: 0.4461\n","Epoch [1/5], Step [669/1072], Loss: 0.4560\n","Epoch [1/5], Step [670/1072], Loss: 0.4615\n","Epoch [1/5], Step [671/1072], Loss: 0.4528\n","Epoch [1/5], Step [672/1072], Loss: 0.4537\n","Epoch [1/5], Step [673/1072], Loss: 0.4352\n","Epoch [1/5], Step [674/1072], Loss: 0.4537\n","Epoch [1/5], Step [675/1072], Loss: 0.4634\n","Epoch [1/5], Step [676/1072], Loss: 0.4488\n","Epoch [1/5], Step [677/1072], Loss: 0.4436\n","Epoch [1/5], Step [678/1072], Loss: 0.4669\n","Epoch [1/5], Step [679/1072], Loss: 0.4491\n","Epoch [1/5], Step [680/1072], Loss: 0.4151\n","Epoch [1/5], Step [681/1072], Loss: 0.4268\n","Epoch [1/5], Step [682/1072], Loss: 0.4302\n","Epoch [1/5], Step [683/1072], Loss: 0.4562\n","Epoch [1/5], Step [684/1072], Loss: 0.4102\n","Epoch [1/5], Step [685/1072], Loss: 0.4434\n","Epoch [1/5], Step [686/1072], Loss: 0.4409\n","Epoch [1/5], Step [687/1072], Loss: 0.4278\n","Epoch [1/5], Step [688/1072], Loss: 0.4490\n","Epoch [1/5], Step [689/1072], Loss: 0.4801\n","Epoch [1/5], Step [690/1072], Loss: 0.4195\n","Epoch [1/5], Step [691/1072], Loss: 0.4567\n","Epoch [1/5], Step [692/1072], Loss: 0.4499\n","Epoch [1/5], Step [693/1072], Loss: 0.4587\n","Epoch [1/5], Step [694/1072], Loss: 0.4364\n","Epoch [1/5], Step [695/1072], Loss: 0.4494\n","Epoch [1/5], Step [696/1072], Loss: 0.4484\n","Epoch [1/5], Step [697/1072], Loss: 0.4539\n","Epoch [1/5], Step [698/1072], Loss: 0.4590\n","Epoch [1/5], Step [699/1072], Loss: 0.4431\n","Epoch [1/5], Step [700/1072], Loss: 0.4641\n","Epoch [1/5], Step [701/1072], Loss: 0.4397\n","Epoch [1/5], Step [702/1072], Loss: 0.4408\n","Epoch [1/5], Step [703/1072], Loss: 0.4346\n","Epoch [1/5], Step [704/1072], Loss: 0.4450\n","Epoch [1/5], Step [705/1072], Loss: 0.4379\n","Epoch [1/5], Step [706/1072], Loss: 0.4155\n","Epoch [1/5], Step [707/1072], Loss: 0.4683\n","Epoch [1/5], Step [708/1072], Loss: 0.4599\n","Epoch [1/5], Step [709/1072], Loss: 0.4690\n","Epoch [1/5], Step [710/1072], Loss: 0.4582\n","Epoch [1/5], Step [711/1072], Loss: 0.4416\n","Epoch [1/5], Step [712/1072], Loss: 0.4880\n","Epoch [1/5], Step [713/1072], Loss: 0.4458\n","Epoch [1/5], Step [714/1072], Loss: 0.4185\n","Epoch [1/5], Step [715/1072], Loss: 0.4538\n","Epoch [1/5], Step [716/1072], Loss: 0.4448\n","Epoch [1/5], Step [717/1072], Loss: 0.4663\n","Epoch [1/5], Step [718/1072], Loss: 0.4566\n","Epoch [1/5], Step [719/1072], Loss: 0.4327\n","Epoch [1/5], Step [720/1072], Loss: 0.4396\n","Epoch [1/5], Step [721/1072], Loss: 0.4459\n","Epoch [1/5], Step [722/1072], Loss: 0.4683\n","Epoch [1/5], Step [723/1072], Loss: 0.4493\n","Epoch [1/5], Step [724/1072], Loss: 0.4410\n","Epoch [1/5], Step [725/1072], Loss: 0.4764\n","Epoch [1/5], Step [726/1072], Loss: 0.4323\n","Epoch [1/5], Step [727/1072], Loss: 0.4372\n","Epoch [1/5], Step [728/1072], Loss: 0.4662\n","Epoch [1/5], Step [729/1072], Loss: 0.4431\n","Epoch [1/5], Step [730/1072], Loss: 0.4288\n","Epoch [1/5], Step [731/1072], Loss: 0.4547\n","Epoch [1/5], Step [732/1072], Loss: 0.4428\n","Epoch [1/5], Step [733/1072], Loss: 0.4672\n","Epoch [1/5], Step [734/1072], Loss: 0.4447\n","Epoch [1/5], Step [735/1072], Loss: 0.4585\n","Epoch [1/5], Step [736/1072], Loss: 0.4369\n","Epoch [1/5], Step [737/1072], Loss: 0.4494\n","Epoch [1/5], Step [738/1072], Loss: 0.4540\n","Epoch [1/5], Step [739/1072], Loss: 0.4325\n","Epoch [1/5], Step [740/1072], Loss: 0.4428\n","Epoch [1/5], Step [741/1072], Loss: 0.4609\n","Epoch [1/5], Step [742/1072], Loss: 0.4294\n","Epoch [1/5], Step [743/1072], Loss: 0.4349\n","Epoch [1/5], Step [744/1072], Loss: 0.4328\n","Epoch [1/5], Step [745/1072], Loss: 0.4629\n","Epoch [1/5], Step [746/1072], Loss: 0.4223\n","Epoch [1/5], Step [747/1072], Loss: 0.4284\n","Epoch [1/5], Step [748/1072], Loss: 0.4382\n","Epoch [1/5], Step [749/1072], Loss: 0.4155\n","Epoch [1/5], Step [750/1072], Loss: 0.4194\n","Epoch [1/5], Step [751/1072], Loss: 0.4584\n","Epoch [1/5], Step [752/1072], Loss: 0.4379\n","Epoch [1/5], Step [753/1072], Loss: 0.4698\n","Epoch [1/5], Step [754/1072], Loss: 0.4302\n","Epoch [1/5], Step [755/1072], Loss: 0.4301\n","Epoch [1/5], Step [756/1072], Loss: 0.4640\n","Epoch [1/5], Step [757/1072], Loss: 0.4097\n","Epoch [1/5], Step [758/1072], Loss: 0.4729\n","Epoch [1/5], Step [759/1072], Loss: 0.4369\n","Epoch [1/5], Step [760/1072], Loss: 0.4247\n","Epoch [1/5], Step [761/1072], Loss: 0.4276\n","Epoch [1/5], Step [762/1072], Loss: 0.4394\n","Epoch [1/5], Step [763/1072], Loss: 0.4798\n","Epoch [1/5], Step [764/1072], Loss: 0.4546\n","Epoch [1/5], Step [765/1072], Loss: 0.4462\n","Epoch [1/5], Step [766/1072], Loss: 0.4155\n","Epoch [1/5], Step [767/1072], Loss: 0.4143\n","Epoch [1/5], Step [768/1072], Loss: 0.4405\n","Epoch [1/5], Step [769/1072], Loss: 0.4773\n","Epoch [1/5], Step [770/1072], Loss: 0.4119\n","Epoch [1/5], Step [771/1072], Loss: 0.4425\n","Epoch [1/5], Step [772/1072], Loss: 0.4498\n","Epoch [1/5], Step [773/1072], Loss: 0.4224\n","Epoch [1/5], Step [774/1072], Loss: 0.4364\n","Epoch [1/5], Step [775/1072], Loss: 0.4149\n","Epoch [1/5], Step [776/1072], Loss: 0.4404\n","Epoch [1/5], Step [777/1072], Loss: 0.4315\n","Epoch [1/5], Step [778/1072], Loss: 0.4312\n","Epoch [1/5], Step [779/1072], Loss: 0.4580\n","Epoch [1/5], Step [780/1072], Loss: 0.4251\n","Epoch [1/5], Step [781/1072], Loss: 0.4414\n","Epoch [1/5], Step [782/1072], Loss: 0.4244\n","Epoch [1/5], Step [783/1072], Loss: 0.4500\n","Epoch [1/5], Step [784/1072], Loss: 0.4518\n","Epoch [1/5], Step [785/1072], Loss: 0.4114\n","Epoch [1/5], Step [786/1072], Loss: 0.4339\n","Epoch [1/5], Step [787/1072], Loss: 0.4267\n","Epoch [1/5], Step [788/1072], Loss: 0.4469\n","Epoch [1/5], Step [789/1072], Loss: 0.4857\n","Epoch [1/5], Step [790/1072], Loss: 0.4449\n","Epoch [1/5], Step [791/1072], Loss: 0.4602\n","Epoch [1/5], Step [792/1072], Loss: 0.4439\n","Epoch [1/5], Step [793/1072], Loss: 0.4082\n","Epoch [1/5], Step [794/1072], Loss: 0.4196\n","Epoch [1/5], Step [795/1072], Loss: 0.4588\n","Epoch [1/5], Step [796/1072], Loss: 0.4296\n","Epoch [1/5], Step [797/1072], Loss: 0.4384\n","Epoch [1/5], Step [798/1072], Loss: 0.4249\n","Epoch [1/5], Step [799/1072], Loss: 0.4473\n","Epoch [1/5], Step [800/1072], Loss: 0.4151\n","Epoch [1/5], Step [801/1072], Loss: 0.4582\n","Epoch [1/5], Step [802/1072], Loss: 0.4269\n","Epoch [1/5], Step [803/1072], Loss: 0.4340\n","Epoch [1/5], Step [804/1072], Loss: 0.4473\n","Epoch [1/5], Step [805/1072], Loss: 0.4534\n","Epoch [1/5], Step [806/1072], Loss: 0.4087\n","Epoch [1/5], Step [807/1072], Loss: 0.4171\n","Epoch [1/5], Step [808/1072], Loss: 0.4165\n","Epoch [1/5], Step [809/1072], Loss: 0.4181\n","Epoch [1/5], Step [810/1072], Loss: 0.4407\n","Epoch [1/5], Step [811/1072], Loss: 0.4545\n","Epoch [1/5], Step [812/1072], Loss: 0.4597\n","Epoch [1/5], Step [813/1072], Loss: 0.4319\n","Epoch [1/5], Step [814/1072], Loss: 0.4449\n","Epoch [1/5], Step [815/1072], Loss: 0.4358\n","Epoch [1/5], Step [816/1072], Loss: 0.4133\n","Epoch [1/5], Step [817/1072], Loss: 0.4013\n","Epoch [1/5], Step [818/1072], Loss: 0.4749\n","Epoch [1/5], Step [819/1072], Loss: 0.4427\n","Epoch [1/5], Step [820/1072], Loss: 0.4472\n","Epoch [1/5], Step [821/1072], Loss: 0.4270\n","Epoch [1/5], Step [822/1072], Loss: 0.4393\n","Epoch [1/5], Step [823/1072], Loss: 0.4213\n","Epoch [1/5], Step [824/1072], Loss: 0.4706\n","Epoch [1/5], Step [825/1072], Loss: 0.4004\n","Epoch [1/5], Step [826/1072], Loss: 0.4358\n","Epoch [1/5], Step [827/1072], Loss: 0.4702\n","Epoch [1/5], Step [828/1072], Loss: 0.4455\n","Epoch [1/5], Step [829/1072], Loss: 0.4323\n","Epoch [1/5], Step [830/1072], Loss: 0.4427\n","Epoch [1/5], Step [831/1072], Loss: 0.4556\n","Epoch [1/5], Step [832/1072], Loss: 0.4299\n","Epoch [1/5], Step [833/1072], Loss: 0.4426\n","Epoch [1/5], Step [834/1072], Loss: 0.3979\n","Epoch [1/5], Step [835/1072], Loss: 0.4345\n","Epoch [1/5], Step [836/1072], Loss: 0.4235\n","Epoch [1/5], Step [837/1072], Loss: 0.4277\n","Epoch [1/5], Step [838/1072], Loss: 0.4371\n","Epoch [1/5], Step [839/1072], Loss: 0.4563\n","Epoch [1/5], Step [840/1072], Loss: 0.4427\n","Epoch [1/5], Step [841/1072], Loss: 0.4390\n","Epoch [1/5], Step [842/1072], Loss: 0.4539\n","Epoch [1/5], Step [843/1072], Loss: 0.4480\n","Epoch [1/5], Step [844/1072], Loss: 0.4636\n","Epoch [1/5], Step [845/1072], Loss: 0.4768\n","Epoch [1/5], Step [846/1072], Loss: 0.4265\n","Epoch [1/5], Step [847/1072], Loss: 0.4284\n","Epoch [1/5], Step [848/1072], Loss: 0.4683\n","Epoch [1/5], Step [849/1072], Loss: 0.4383\n","Epoch [1/5], Step [850/1072], Loss: 0.4492\n","Epoch [1/5], Step [851/1072], Loss: 0.4195\n","Epoch [1/5], Step [852/1072], Loss: 0.4407\n","Epoch [1/5], Step [853/1072], Loss: 0.4245\n","Epoch [1/5], Step [854/1072], Loss: 0.4437\n","Epoch [1/5], Step [855/1072], Loss: 0.4410\n","Epoch [1/5], Step [856/1072], Loss: 0.4481\n","Epoch [1/5], Step [857/1072], Loss: 0.4239\n","Epoch [1/5], Step [858/1072], Loss: 0.4360\n","Epoch [1/5], Step [859/1072], Loss: 0.4176\n","Epoch [1/5], Step [860/1072], Loss: 0.4257\n","Epoch [1/5], Step [861/1072], Loss: 0.4492\n","Epoch [1/5], Step [862/1072], Loss: 0.4804\n","Epoch [1/5], Step [863/1072], Loss: 0.4495\n","Epoch [1/5], Step [864/1072], Loss: 0.4026\n","Epoch [1/5], Step [865/1072], Loss: 0.4547\n","Epoch [1/5], Step [866/1072], Loss: 0.4299\n","Epoch [1/5], Step [867/1072], Loss: 0.4290\n","Epoch [1/5], Step [868/1072], Loss: 0.4463\n","Epoch [1/5], Step [869/1072], Loss: 0.4243\n","Epoch [1/5], Step [870/1072], Loss: 0.4497\n","Epoch [1/5], Step [871/1072], Loss: 0.4130\n","Epoch [1/5], Step [872/1072], Loss: 0.4506\n","Epoch [1/5], Step [873/1072], Loss: 0.4406\n","Epoch [1/5], Step [874/1072], Loss: 0.4468\n","Epoch [1/5], Step [875/1072], Loss: 0.4478\n","Epoch [1/5], Step [876/1072], Loss: 0.4398\n","Epoch [1/5], Step [877/1072], Loss: 0.4392\n","Epoch [1/5], Step [878/1072], Loss: 0.4239\n","Epoch [1/5], Step [879/1072], Loss: 0.4439\n","Epoch [1/5], Step [880/1072], Loss: 0.4532\n","Epoch [1/5], Step [881/1072], Loss: 0.4448\n","Epoch [1/5], Step [882/1072], Loss: 0.4219\n","Epoch [1/5], Step [883/1072], Loss: 0.4203\n","Epoch [1/5], Step [884/1072], Loss: 0.4196\n","Epoch [1/5], Step [885/1072], Loss: 0.4246\n","Epoch [1/5], Step [886/1072], Loss: 0.4049\n","Epoch [1/5], Step [887/1072], Loss: 0.4167\n","Epoch [1/5], Step [888/1072], Loss: 0.4000\n","Epoch [1/5], Step [889/1072], Loss: 0.3806\n","Epoch [1/5], Step [890/1072], Loss: 0.4372\n","Epoch [1/5], Step [891/1072], Loss: 0.4042\n","Epoch [1/5], Step [892/1072], Loss: 0.4276\n","Epoch [1/5], Step [893/1072], Loss: 0.4403\n","Epoch [1/5], Step [894/1072], Loss: 0.4520\n","Epoch [1/5], Step [895/1072], Loss: 0.4176\n","Epoch [1/5], Step [896/1072], Loss: 0.4335\n","Epoch [1/5], Step [897/1072], Loss: 0.4160\n","Epoch [1/5], Step [898/1072], Loss: 0.4308\n","Epoch [1/5], Step [899/1072], Loss: 0.4374\n","Epoch [1/5], Step [900/1072], Loss: 0.4242\n","Epoch [1/5], Step [901/1072], Loss: 0.4247\n","Epoch [1/5], Step [902/1072], Loss: 0.4480\n","Epoch [1/5], Step [903/1072], Loss: 0.4442\n","Epoch [1/5], Step [904/1072], Loss: 0.4238\n","Epoch [1/5], Step [905/1072], Loss: 0.4115\n","Epoch [1/5], Step [906/1072], Loss: 0.4431\n","Epoch [1/5], Step [907/1072], Loss: 0.4341\n","Epoch [1/5], Step [908/1072], Loss: 0.4488\n","Epoch [1/5], Step [909/1072], Loss: 0.4386\n","Epoch [1/5], Step [910/1072], Loss: 0.4335\n","Epoch [1/5], Step [911/1072], Loss: 0.4424\n","Epoch [1/5], Step [912/1072], Loss: 0.4382\n","Epoch [1/5], Step [913/1072], Loss: 0.4305\n","Epoch [1/5], Step [914/1072], Loss: 0.4529\n","Epoch [1/5], Step [915/1072], Loss: 0.4452\n","Epoch [1/5], Step [916/1072], Loss: 0.4162\n","Epoch [1/5], Step [917/1072], Loss: 0.4439\n","Epoch [1/5], Step [918/1072], Loss: 0.4148\n","Epoch [1/5], Step [919/1072], Loss: 0.4307\n","Epoch [1/5], Step [920/1072], Loss: 0.4489\n","Epoch [1/5], Step [921/1072], Loss: 0.4461\n","Epoch [1/5], Step [922/1072], Loss: 0.4635\n","Epoch [1/5], Step [923/1072], Loss: 0.4055\n","Epoch [1/5], Step [924/1072], Loss: 0.4454\n","Epoch [1/5], Step [925/1072], Loss: 0.4095\n","Epoch [1/5], Step [926/1072], Loss: 0.4157\n","Epoch [1/5], Step [927/1072], Loss: 0.4720\n","Epoch [1/5], Step [928/1072], Loss: 0.4098\n","Epoch [1/5], Step [929/1072], Loss: 0.4320\n","Epoch [1/5], Step [930/1072], Loss: 0.4568\n","Epoch [1/5], Step [931/1072], Loss: 0.4437\n","Epoch [1/5], Step [932/1072], Loss: 0.4169\n","Epoch [1/5], Step [933/1072], Loss: 0.4534\n","Epoch [1/5], Step [934/1072], Loss: 0.4527\n","Epoch [1/5], Step [935/1072], Loss: 0.4408\n","Epoch [1/5], Step [936/1072], Loss: 0.4228\n","Epoch [1/5], Step [937/1072], Loss: 0.4343\n","Epoch [1/5], Step [938/1072], Loss: 0.4376\n","Epoch [1/5], Step [939/1072], Loss: 0.4642\n","Epoch [1/5], Step [940/1072], Loss: 0.4305\n","Epoch [1/5], Step [941/1072], Loss: 0.4169\n","Epoch [1/5], Step [942/1072], Loss: 0.4201\n","Epoch [1/5], Step [943/1072], Loss: 0.4326\n","Epoch [1/5], Step [944/1072], Loss: 0.4344\n","Epoch [1/5], Step [945/1072], Loss: 0.4071\n","Epoch [1/5], Step [946/1072], Loss: 0.4497\n","Epoch [1/5], Step [947/1072], Loss: 0.4307\n","Epoch [1/5], Step [948/1072], Loss: 0.4153\n","Epoch [1/5], Step [949/1072], Loss: 0.4586\n","Epoch [1/5], Step [950/1072], Loss: 0.4374\n","Epoch [1/5], Step [951/1072], Loss: 0.4540\n","Epoch [1/5], Step [952/1072], Loss: 0.4103\n","Epoch [1/5], Step [953/1072], Loss: 0.4558\n","Epoch [1/5], Step [954/1072], Loss: 0.4108\n","Epoch [1/5], Step [955/1072], Loss: 0.4697\n","Epoch [1/5], Step [956/1072], Loss: 0.4136\n","Epoch [1/5], Step [957/1072], Loss: 0.4237\n","Epoch [1/5], Step [958/1072], Loss: 0.4847\n","Epoch [1/5], Step [959/1072], Loss: 0.4527\n","Epoch [1/5], Step [960/1072], Loss: 0.4182\n","Epoch [1/5], Step [961/1072], Loss: 0.4286\n","Epoch [1/5], Step [962/1072], Loss: 0.4231\n","Epoch [1/5], Step [963/1072], Loss: 0.4315\n","Epoch [1/5], Step [964/1072], Loss: 0.4310\n","Epoch [1/5], Step [965/1072], Loss: 0.4708\n","Epoch [1/5], Step [966/1072], Loss: 0.4181\n","Epoch [1/5], Step [967/1072], Loss: 0.4403\n","Epoch [1/5], Step [968/1072], Loss: 0.4605\n","Epoch [1/5], Step [969/1072], Loss: 0.4387\n","Epoch [1/5], Step [970/1072], Loss: 0.4278\n","Epoch [1/5], Step [971/1072], Loss: 0.4396\n","Epoch [1/5], Step [972/1072], Loss: 0.4480\n","Epoch [1/5], Step [973/1072], Loss: 0.4377\n","Epoch [1/5], Step [974/1072], Loss: 0.4061\n","Epoch [1/5], Step [975/1072], Loss: 0.3993\n","Epoch [1/5], Step [976/1072], Loss: 0.4577\n","Epoch [1/5], Step [977/1072], Loss: 0.4407\n","Epoch [1/5], Step [978/1072], Loss: 0.4238\n","Epoch [1/5], Step [979/1072], Loss: 0.4334\n","Epoch [1/5], Step [980/1072], Loss: 0.4180\n","Epoch [1/5], Step [981/1072], Loss: 0.4286\n","Epoch [1/5], Step [982/1072], Loss: 0.4661\n","Epoch [1/5], Step [983/1072], Loss: 0.4327\n","Epoch [1/5], Step [984/1072], Loss: 0.4421\n","Epoch [1/5], Step [985/1072], Loss: 0.4367\n","Epoch [1/5], Step [986/1072], Loss: 0.4627\n","Epoch [1/5], Step [987/1072], Loss: 0.4291\n","Epoch [1/5], Step [988/1072], Loss: 0.4312\n","Epoch [1/5], Step [989/1072], Loss: 0.4330\n","Epoch [1/5], Step [990/1072], Loss: 0.4053\n","Epoch [1/5], Step [991/1072], Loss: 0.4383\n","Epoch [1/5], Step [992/1072], Loss: 0.3998\n","Epoch [1/5], Step [993/1072], Loss: 0.4356\n","Epoch [1/5], Step [994/1072], Loss: 0.4105\n","Epoch [1/5], Step [995/1072], Loss: 0.4258\n","Epoch [1/5], Step [996/1072], Loss: 0.4235\n","Epoch [1/5], Step [997/1072], Loss: 0.4432\n","Epoch [1/5], Step [998/1072], Loss: 0.4189\n","Epoch [1/5], Step [999/1072], Loss: 0.4023\n","Epoch [1/5], Step [1000/1072], Loss: 0.4447\n","Epoch [1/5], Step [1001/1072], Loss: 0.4409\n","Epoch [1/5], Step [1002/1072], Loss: 0.4081\n","Epoch [1/5], Step [1003/1072], Loss: 0.4233\n","Epoch [1/5], Step [1004/1072], Loss: 0.4405\n","Epoch [1/5], Step [1005/1072], Loss: 0.3956\n","Epoch [1/5], Step [1006/1072], Loss: 0.4296\n","Epoch [1/5], Step [1007/1072], Loss: 0.4198\n","Epoch [1/5], Step [1008/1072], Loss: 0.3960\n","Epoch [1/5], Step [1009/1072], Loss: 0.4514\n","Epoch [1/5], Step [1010/1072], Loss: 0.4449\n","Epoch [1/5], Step [1011/1072], Loss: 0.4230\n","Epoch [1/5], Step [1012/1072], Loss: 0.4342\n","Epoch [1/5], Step [1013/1072], Loss: 0.4428\n","Epoch [1/5], Step [1014/1072], Loss: 0.4293\n","Epoch [1/5], Step [1015/1072], Loss: 0.4423\n","Epoch [1/5], Step [1016/1072], Loss: 0.4081\n","Epoch [1/5], Step [1017/1072], Loss: 0.4084\n","Epoch [1/5], Step [1018/1072], Loss: 0.4269\n","Epoch [1/5], Step [1019/1072], Loss: 0.4153\n","Epoch [1/5], Step [1020/1072], Loss: 0.4054\n","Epoch [1/5], Step [1021/1072], Loss: 0.4114\n","Epoch [1/5], Step [1022/1072], Loss: 0.4059\n","Epoch [1/5], Step [1023/1072], Loss: 0.4237\n","Epoch [1/5], Step [1024/1072], Loss: 0.4679\n","Epoch [1/5], Step [1025/1072], Loss: 0.4344\n","Epoch [1/5], Step [1026/1072], Loss: 0.4266\n","Epoch [1/5], Step [1027/1072], Loss: 0.4470\n","Epoch [1/5], Step [1028/1072], Loss: 0.4439\n","Epoch [1/5], Step [1029/1072], Loss: 0.4203\n","Epoch [1/5], Step [1030/1072], Loss: 0.4321\n","Epoch [1/5], Step [1031/1072], Loss: 0.4467\n","Epoch [1/5], Step [1032/1072], Loss: 0.4399\n","Epoch [1/5], Step [1033/1072], Loss: 0.4424\n","Epoch [1/5], Step [1034/1072], Loss: 0.4589\n","Epoch [1/5], Step [1035/1072], Loss: 0.4125\n","Epoch [1/5], Step [1036/1072], Loss: 0.4378\n","Epoch [1/5], Step [1037/1072], Loss: 0.4176\n","Epoch [1/5], Step [1038/1072], Loss: 0.4236\n","Epoch [1/5], Step [1039/1072], Loss: 0.3612\n","Epoch [1/5], Step [1040/1072], Loss: 0.4437\n","Epoch [1/5], Step [1041/1072], Loss: 0.4256\n","Epoch [1/5], Step [1042/1072], Loss: 0.4022\n","Epoch [1/5], Step [1043/1072], Loss: 0.4371\n","Epoch [1/5], Step [1044/1072], Loss: 0.4275\n","Epoch [1/5], Step [1045/1072], Loss: 0.3892\n","Epoch [1/5], Step [1046/1072], Loss: 0.4267\n","Epoch [1/5], Step [1047/1072], Loss: 0.4554\n","Epoch [1/5], Step [1048/1072], Loss: 0.4211\n","Epoch [1/5], Step [1049/1072], Loss: 0.4225\n","Epoch [1/5], Step [1050/1072], Loss: 0.4164\n","Epoch [1/5], Step [1051/1072], Loss: 0.4251\n","Epoch [1/5], Step [1052/1072], Loss: 0.4379\n","Epoch [1/5], Step [1053/1072], Loss: 0.4342\n","Epoch [1/5], Step [1054/1072], Loss: 0.4081\n","Epoch [1/5], Step [1055/1072], Loss: 0.4115\n","Epoch [1/5], Step [1056/1072], Loss: 0.4107\n","Epoch [1/5], Step [1057/1072], Loss: 0.4578\n","Epoch [1/5], Step [1058/1072], Loss: 0.4138\n","Epoch [1/5], Step [1059/1072], Loss: 0.4333\n","Epoch [1/5], Step [1060/1072], Loss: 0.4149\n","Epoch [1/5], Step [1061/1072], Loss: 0.4674\n","Epoch [1/5], Step [1062/1072], Loss: 0.4080\n","Epoch [1/5], Step [1063/1072], Loss: 0.4427\n","Epoch [1/5], Step [1064/1072], Loss: 0.4472\n","Epoch [1/5], Step [1065/1072], Loss: 0.4208\n","Epoch [1/5], Step [1066/1072], Loss: 0.4231\n","Epoch [1/5], Step [1067/1072], Loss: 0.4563\n","Epoch [1/5], Step [1068/1072], Loss: 0.4289\n","Epoch [1/5], Step [1069/1072], Loss: 0.4513\n","Epoch [1/5], Step [1070/1072], Loss: 0.4418\n","Epoch [1/5], Step [1071/1072], Loss: 0.4541\n","Epoch [1/5], Step [1072/1072], Loss: 0.4046\n","Epoch [2/5], Step [1/1072], Loss: 0.3968\n","Epoch [2/5], Step [2/1072], Loss: 0.4090\n","Epoch [2/5], Step [3/1072], Loss: 0.4121\n","Epoch [2/5], Step [4/1072], Loss: 0.4030\n","Epoch [2/5], Step [5/1072], Loss: 0.4059\n","Epoch [2/5], Step [6/1072], Loss: 0.4253\n","Epoch [2/5], Step [7/1072], Loss: 0.4143\n","Epoch [2/5], Step [8/1072], Loss: 0.3877\n","Epoch [2/5], Step [9/1072], Loss: 0.4012\n","Epoch [2/5], Step [10/1072], Loss: 0.3747\n","Epoch [2/5], Step [11/1072], Loss: 0.4202\n","Epoch [2/5], Step [12/1072], Loss: 0.3870\n","Epoch [2/5], Step [13/1072], Loss: 0.4105\n","Epoch [2/5], Step [14/1072], Loss: 0.3920\n","Epoch [2/5], Step [15/1072], Loss: 0.3982\n","Epoch [2/5], Step [16/1072], Loss: 0.3947\n","Epoch [2/5], Step [17/1072], Loss: 0.4203\n","Epoch [2/5], Step [18/1072], Loss: 0.3829\n","Epoch [2/5], Step [19/1072], Loss: 0.4072\n","Epoch [2/5], Step [20/1072], Loss: 0.3904\n","Epoch [2/5], Step [21/1072], Loss: 0.4212\n","Epoch [2/5], Step [22/1072], Loss: 0.4395\n","Epoch [2/5], Step [23/1072], Loss: 0.4101\n","Epoch [2/5], Step [24/1072], Loss: 0.4002\n","Epoch [2/5], Step [25/1072], Loss: 0.3997\n","Epoch [2/5], Step [26/1072], Loss: 0.4191\n","Epoch [2/5], Step [27/1072], Loss: 0.4176\n","Epoch [2/5], Step [28/1072], Loss: 0.3771\n","Epoch [2/5], Step [29/1072], Loss: 0.4380\n","Epoch [2/5], Step [30/1072], Loss: 0.4343\n","Epoch [2/5], Step [31/1072], Loss: 0.3958\n","Epoch [2/5], Step [32/1072], Loss: 0.4093\n","Epoch [2/5], Step [33/1072], Loss: 0.4223\n","Epoch [2/5], Step [34/1072], Loss: 0.4079\n","Epoch [2/5], Step [35/1072], Loss: 0.4171\n","Epoch [2/5], Step [36/1072], Loss: 0.4129\n","Epoch [2/5], Step [37/1072], Loss: 0.4266\n","Epoch [2/5], Step [38/1072], Loss: 0.4064\n","Epoch [2/5], Step [39/1072], Loss: 0.4236\n","Epoch [2/5], Step [40/1072], Loss: 0.4249\n","Epoch [2/5], Step [41/1072], Loss: 0.4227\n","Epoch [2/5], Step [42/1072], Loss: 0.3948\n","Epoch [2/5], Step [43/1072], Loss: 0.3894\n","Epoch [2/5], Step [44/1072], Loss: 0.4059\n","Epoch [2/5], Step [45/1072], Loss: 0.3942\n","Epoch [2/5], Step [46/1072], Loss: 0.3901\n","Epoch [2/5], Step [47/1072], Loss: 0.4143\n","Epoch [2/5], Step [48/1072], Loss: 0.4094\n","Epoch [2/5], Step [49/1072], Loss: 0.3870\n","Epoch [2/5], Step [50/1072], Loss: 0.4111\n","Epoch [2/5], Step [51/1072], Loss: 0.4083\n","Epoch [2/5], Step [52/1072], Loss: 0.4314\n","Epoch [2/5], Step [53/1072], Loss: 0.4069\n","Epoch [2/5], Step [54/1072], Loss: 0.3967\n","Epoch [2/5], Step [55/1072], Loss: 0.4133\n","Epoch [2/5], Step [56/1072], Loss: 0.3871\n","Epoch [2/5], Step [57/1072], Loss: 0.4322\n","Epoch [2/5], Step [58/1072], Loss: 0.3894\n","Epoch [2/5], Step [59/1072], Loss: 0.4413\n","Epoch [2/5], Step [60/1072], Loss: 0.4155\n","Epoch [2/5], Step [61/1072], Loss: 0.3813\n","Epoch [2/5], Step [62/1072], Loss: 0.3947\n","Epoch [2/5], Step [63/1072], Loss: 0.3957\n","Epoch [2/5], Step [64/1072], Loss: 0.3826\n","Epoch [2/5], Step [65/1072], Loss: 0.4033\n","Epoch [2/5], Step [66/1072], Loss: 0.4371\n","Epoch [2/5], Step [67/1072], Loss: 0.3884\n","Epoch [2/5], Step [68/1072], Loss: 0.4144\n","Epoch [2/5], Step [69/1072], Loss: 0.3873\n","Epoch [2/5], Step [70/1072], Loss: 0.3990\n","Epoch [2/5], Step [71/1072], Loss: 0.3984\n","Epoch [2/5], Step [72/1072], Loss: 0.4148\n","Epoch [2/5], Step [73/1072], Loss: 0.4388\n","Epoch [2/5], Step [74/1072], Loss: 0.4029\n","Epoch [2/5], Step [75/1072], Loss: 0.4220\n","Epoch [2/5], Step [76/1072], Loss: 0.3899\n","Epoch [2/5], Step [77/1072], Loss: 0.3949\n","Epoch [2/5], Step [78/1072], Loss: 0.4197\n","Epoch [2/5], Step [79/1072], Loss: 0.4218\n","Epoch [2/5], Step [80/1072], Loss: 0.4270\n","Epoch [2/5], Step [81/1072], Loss: 0.4175\n","Epoch [2/5], Step [82/1072], Loss: 0.4130\n","Epoch [2/5], Step [83/1072], Loss: 0.3922\n","Epoch [2/5], Step [84/1072], Loss: 0.3796\n","Epoch [2/5], Step [85/1072], Loss: 0.4260\n","Epoch [2/5], Step [86/1072], Loss: 0.3779\n","Epoch [2/5], Step [87/1072], Loss: 0.4104\n","Epoch [2/5], Step [88/1072], Loss: 0.4111\n","Epoch [2/5], Step [89/1072], Loss: 0.4131\n","Epoch [2/5], Step [90/1072], Loss: 0.4183\n","Epoch [2/5], Step [91/1072], Loss: 0.4516\n","Epoch [2/5], Step [92/1072], Loss: 0.4210\n","Epoch [2/5], Step [93/1072], Loss: 0.4090\n","Epoch [2/5], Step [94/1072], Loss: 0.3920\n","Epoch [2/5], Step [95/1072], Loss: 0.3918\n","Epoch [2/5], Step [96/1072], Loss: 0.4292\n","Epoch [2/5], Step [97/1072], Loss: 0.3932\n","Epoch [2/5], Step [98/1072], Loss: 0.3947\n","Epoch [2/5], Step [99/1072], Loss: 0.3907\n","Epoch [2/5], Step [100/1072], Loss: 0.4255\n","Epoch [2/5], Step [101/1072], Loss: 0.4173\n","Epoch [2/5], Step [102/1072], Loss: 0.3958\n","Epoch [2/5], Step [103/1072], Loss: 0.4174\n","Epoch [2/5], Step [104/1072], Loss: 0.3973\n","Epoch [2/5], Step [105/1072], Loss: 0.4123\n","Epoch [2/5], Step [106/1072], Loss: 0.4043\n","Epoch [2/5], Step [107/1072], Loss: 0.3921\n","Epoch [2/5], Step [108/1072], Loss: 0.4295\n","Epoch [2/5], Step [109/1072], Loss: 0.3878\n","Epoch [2/5], Step [110/1072], Loss: 0.4260\n","Epoch [2/5], Step [111/1072], Loss: 0.4232\n","Epoch [2/5], Step [112/1072], Loss: 0.3943\n","Epoch [2/5], Step [113/1072], Loss: 0.4175\n","Epoch [2/5], Step [114/1072], Loss: 0.4195\n","Epoch [2/5], Step [115/1072], Loss: 0.3981\n","Epoch [2/5], Step [116/1072], Loss: 0.3963\n","Epoch [2/5], Step [117/1072], Loss: 0.3889\n","Epoch [2/5], Step [118/1072], Loss: 0.4206\n","Epoch [2/5], Step [119/1072], Loss: 0.4171\n","Epoch [2/5], Step [120/1072], Loss: 0.3726\n","Epoch [2/5], Step [121/1072], Loss: 0.3909\n","Epoch [2/5], Step [122/1072], Loss: 0.3955\n","Epoch [2/5], Step [123/1072], Loss: 0.4211\n","Epoch [2/5], Step [124/1072], Loss: 0.3882\n","Epoch [2/5], Step [125/1072], Loss: 0.3914\n","Epoch [2/5], Step [126/1072], Loss: 0.3795\n","Epoch [2/5], Step [127/1072], Loss: 0.3925\n","Epoch [2/5], Step [128/1072], Loss: 0.4074\n","Epoch [2/5], Step [129/1072], Loss: 0.3846\n","Epoch [2/5], Step [130/1072], Loss: 0.4128\n","Epoch [2/5], Step [131/1072], Loss: 0.4106\n","Epoch [2/5], Step [132/1072], Loss: 0.3894\n","Epoch [2/5], Step [133/1072], Loss: 0.4175\n","Epoch [2/5], Step [134/1072], Loss: 0.3887\n","Epoch [2/5], Step [135/1072], Loss: 0.3927\n","Epoch [2/5], Step [136/1072], Loss: 0.4144\n","Epoch [2/5], Step [137/1072], Loss: 0.4315\n","Epoch [2/5], Step [138/1072], Loss: 0.4245\n","Epoch [2/5], Step [139/1072], Loss: 0.4020\n","Epoch [2/5], Step [140/1072], Loss: 0.4205\n","Epoch [2/5], Step [141/1072], Loss: 0.4178\n","Epoch [2/5], Step [142/1072], Loss: 0.4134\n","Epoch [2/5], Step [143/1072], Loss: 0.3842\n","Epoch [2/5], Step [144/1072], Loss: 0.4220\n","Epoch [2/5], Step [145/1072], Loss: 0.4023\n","Epoch [2/5], Step [146/1072], Loss: 0.4034\n","Epoch [2/5], Step [147/1072], Loss: 0.4202\n","Epoch [2/5], Step [148/1072], Loss: 0.4023\n","Epoch [2/5], Step [149/1072], Loss: 0.4331\n","Epoch [2/5], Step [150/1072], Loss: 0.3918\n","Epoch [2/5], Step [151/1072], Loss: 0.4076\n","Epoch [2/5], Step [152/1072], Loss: 0.4200\n","Epoch [2/5], Step [153/1072], Loss: 0.4041\n","Epoch [2/5], Step [154/1072], Loss: 0.4178\n","Epoch [2/5], Step [155/1072], Loss: 0.4323\n","Epoch [2/5], Step [156/1072], Loss: 0.3772\n","Epoch [2/5], Step [157/1072], Loss: 0.4097\n","Epoch [2/5], Step [158/1072], Loss: 0.3849\n","Epoch [2/5], Step [159/1072], Loss: 0.3831\n","Epoch [2/5], Step [160/1072], Loss: 0.4227\n","Epoch [2/5], Step [161/1072], Loss: 0.3874\n","Epoch [2/5], Step [162/1072], Loss: 0.4159\n","Epoch [2/5], Step [163/1072], Loss: 0.3953\n","Epoch [2/5], Step [164/1072], Loss: 0.4053\n","Epoch [2/5], Step [165/1072], Loss: 0.3968\n","Epoch [2/5], Step [166/1072], Loss: 0.3942\n","Epoch [2/5], Step [167/1072], Loss: 0.3994\n","Epoch [2/5], Step [168/1072], Loss: 0.3695\n","Epoch [2/5], Step [169/1072], Loss: 0.4101\n","Epoch [2/5], Step [170/1072], Loss: 0.4142\n","Epoch [2/5], Step [171/1072], Loss: 0.4160\n","Epoch [2/5], Step [172/1072], Loss: 0.3916\n","Epoch [2/5], Step [173/1072], Loss: 0.4066\n","Epoch [2/5], Step [174/1072], Loss: 0.3956\n","Epoch [2/5], Step [175/1072], Loss: 0.4391\n","Epoch [2/5], Step [176/1072], Loss: 0.4143\n","Epoch [2/5], Step [177/1072], Loss: 0.3958\n","Epoch [2/5], Step [178/1072], Loss: 0.4031\n","Epoch [2/5], Step [179/1072], Loss: 0.4120\n","Epoch [2/5], Step [180/1072], Loss: 0.3957\n","Epoch [2/5], Step [181/1072], Loss: 0.3912\n","Epoch [2/5], Step [182/1072], Loss: 0.4124\n","Epoch [2/5], Step [183/1072], Loss: 0.3816\n","Epoch [2/5], Step [184/1072], Loss: 0.3748\n","Epoch [2/5], Step [185/1072], Loss: 0.4001\n","Epoch [2/5], Step [186/1072], Loss: 0.4018\n","Epoch [2/5], Step [187/1072], Loss: 0.4161\n","Epoch [2/5], Step [188/1072], Loss: 0.4147\n","Epoch [2/5], Step [189/1072], Loss: 0.4152\n","Epoch [2/5], Step [190/1072], Loss: 0.3866\n","Epoch [2/5], Step [191/1072], Loss: 0.4016\n","Epoch [2/5], Step [192/1072], Loss: 0.4080\n","Epoch [2/5], Step [193/1072], Loss: 0.3933\n","Epoch [2/5], Step [194/1072], Loss: 0.4071\n","Epoch [2/5], Step [195/1072], Loss: 0.4168\n","Epoch [2/5], Step [196/1072], Loss: 0.4006\n","Epoch [2/5], Step [197/1072], Loss: 0.3914\n","Epoch [2/5], Step [198/1072], Loss: 0.4267\n","Epoch [2/5], Step [199/1072], Loss: 0.4160\n","Epoch [2/5], Step [200/1072], Loss: 0.4022\n","Epoch [2/5], Step [201/1072], Loss: 0.4186\n","Epoch [2/5], Step [202/1072], Loss: 0.4116\n","Epoch [2/5], Step [203/1072], Loss: 0.4196\n","Epoch [2/5], Step [204/1072], Loss: 0.3865\n","Epoch [2/5], Step [205/1072], Loss: 0.4002\n","Epoch [2/5], Step [206/1072], Loss: 0.3877\n","Epoch [2/5], Step [207/1072], Loss: 0.4246\n","Epoch [2/5], Step [208/1072], Loss: 0.3920\n","Epoch [2/5], Step [209/1072], Loss: 0.4297\n","Epoch [2/5], Step [210/1072], Loss: 0.3893\n","Epoch [2/5], Step [211/1072], Loss: 0.4245\n","Epoch [2/5], Step [212/1072], Loss: 0.4045\n","Epoch [2/5], Step [213/1072], Loss: 0.3989\n","Epoch [2/5], Step [214/1072], Loss: 0.4221\n","Epoch [2/5], Step [215/1072], Loss: 0.4057\n","Epoch [2/5], Step [216/1072], Loss: 0.3885\n","Epoch [2/5], Step [217/1072], Loss: 0.4030\n","Epoch [2/5], Step [218/1072], Loss: 0.4290\n","Epoch [2/5], Step [219/1072], Loss: 0.4194\n","Epoch [2/5], Step [220/1072], Loss: 0.4091\n","Epoch [2/5], Step [221/1072], Loss: 0.4276\n","Epoch [2/5], Step [222/1072], Loss: 0.4194\n","Epoch [2/5], Step [223/1072], Loss: 0.4222\n","Epoch [2/5], Step [224/1072], Loss: 0.4118\n","Epoch [2/5], Step [225/1072], Loss: 0.3695\n","Epoch [2/5], Step [226/1072], Loss: 0.4265\n","Epoch [2/5], Step [227/1072], Loss: 0.4095\n","Epoch [2/5], Step [228/1072], Loss: 0.3751\n","Epoch [2/5], Step [229/1072], Loss: 0.4027\n","Epoch [2/5], Step [230/1072], Loss: 0.3999\n","Epoch [2/5], Step [231/1072], Loss: 0.3890\n","Epoch [2/5], Step [232/1072], Loss: 0.3849\n","Epoch [2/5], Step [233/1072], Loss: 0.3834\n","Epoch [2/5], Step [234/1072], Loss: 0.4562\n","Epoch [2/5], Step [235/1072], Loss: 0.3724\n","Epoch [2/5], Step [236/1072], Loss: 0.4124\n","Epoch [2/5], Step [237/1072], Loss: 0.4023\n","Epoch [2/5], Step [238/1072], Loss: 0.4007\n","Epoch [2/5], Step [239/1072], Loss: 0.3974\n","Epoch [2/5], Step [240/1072], Loss: 0.4270\n","Epoch [2/5], Step [241/1072], Loss: 0.4018\n","Epoch [2/5], Step [242/1072], Loss: 0.3774\n","Epoch [2/5], Step [243/1072], Loss: 0.3716\n","Epoch [2/5], Step [244/1072], Loss: 0.3847\n","Epoch [2/5], Step [245/1072], Loss: 0.3879\n","Epoch [2/5], Step [246/1072], Loss: 0.4042\n","Epoch [2/5], Step [247/1072], Loss: 0.3572\n","Epoch [2/5], Step [248/1072], Loss: 0.4403\n","Epoch [2/5], Step [249/1072], Loss: 0.3959\n","Epoch [2/5], Step [250/1072], Loss: 0.3796\n","Epoch [2/5], Step [251/1072], Loss: 0.4126\n","Epoch [2/5], Step [252/1072], Loss: 0.3881\n","Epoch [2/5], Step [253/1072], Loss: 0.4143\n","Epoch [2/5], Step [254/1072], Loss: 0.4300\n","Epoch [2/5], Step [255/1072], Loss: 0.3875\n","Epoch [2/5], Step [256/1072], Loss: 0.3786\n","Epoch [2/5], Step [257/1072], Loss: 0.3941\n","Epoch [2/5], Step [258/1072], Loss: 0.3893\n","Epoch [2/5], Step [259/1072], Loss: 0.3987\n","Epoch [2/5], Step [260/1072], Loss: 0.4003\n","Epoch [2/5], Step [261/1072], Loss: 0.4215\n","Epoch [2/5], Step [262/1072], Loss: 0.4084\n","Epoch [2/5], Step [263/1072], Loss: 0.3876\n","Epoch [2/5], Step [264/1072], Loss: 0.4185\n","Epoch [2/5], Step [265/1072], Loss: 0.3939\n","Epoch [2/5], Step [266/1072], Loss: 0.3882\n","Epoch [2/5], Step [267/1072], Loss: 0.4052\n","Epoch [2/5], Step [268/1072], Loss: 0.3971\n","Epoch [2/5], Step [269/1072], Loss: 0.3957\n","Epoch [2/5], Step [270/1072], Loss: 0.3713\n","Epoch [2/5], Step [271/1072], Loss: 0.3945\n","Epoch [2/5], Step [272/1072], Loss: 0.4152\n","Epoch [2/5], Step [273/1072], Loss: 0.4235\n","Epoch [2/5], Step [274/1072], Loss: 0.4098\n","Epoch [2/5], Step [275/1072], Loss: 0.3817\n","Epoch [2/5], Step [276/1072], Loss: 0.4192\n","Epoch [2/5], Step [277/1072], Loss: 0.4122\n","Epoch [2/5], Step [278/1072], Loss: 0.4180\n","Epoch [2/5], Step [279/1072], Loss: 0.3709\n","Epoch [2/5], Step [280/1072], Loss: 0.3893\n","Epoch [2/5], Step [281/1072], Loss: 0.3751\n","Epoch [2/5], Step [282/1072], Loss: 0.3873\n","Epoch [2/5], Step [283/1072], Loss: 0.3935\n","Epoch [2/5], Step [284/1072], Loss: 0.3987\n","Epoch [2/5], Step [285/1072], Loss: 0.4131\n","Epoch [2/5], Step [286/1072], Loss: 0.3983\n","Epoch [2/5], Step [287/1072], Loss: 0.3895\n","Epoch [2/5], Step [288/1072], Loss: 0.4003\n","Epoch [2/5], Step [289/1072], Loss: 0.4243\n","Epoch [2/5], Step [290/1072], Loss: 0.3915\n","Epoch [2/5], Step [291/1072], Loss: 0.3934\n","Epoch [2/5], Step [292/1072], Loss: 0.4144\n","Epoch [2/5], Step [293/1072], Loss: 0.4086\n","Epoch [2/5], Step [294/1072], Loss: 0.3880\n","Epoch [2/5], Step [295/1072], Loss: 0.3946\n","Epoch [2/5], Step [296/1072], Loss: 0.4026\n","Epoch [2/5], Step [297/1072], Loss: 0.4074\n","Epoch [2/5], Step [298/1072], Loss: 0.3887\n","Epoch [2/5], Step [299/1072], Loss: 0.4359\n","Epoch [2/5], Step [300/1072], Loss: 0.4200\n","Epoch [2/5], Step [301/1072], Loss: 0.4040\n","Epoch [2/5], Step [302/1072], Loss: 0.3987\n","Epoch [2/5], Step [303/1072], Loss: 0.4321\n","Epoch [2/5], Step [304/1072], Loss: 0.3793\n","Epoch [2/5], Step [305/1072], Loss: 0.3882\n","Epoch [2/5], Step [306/1072], Loss: 0.3984\n","Epoch [2/5], Step [307/1072], Loss: 0.4165\n","Epoch [2/5], Step [308/1072], Loss: 0.4350\n","Epoch [2/5], Step [309/1072], Loss: 0.3797\n","Epoch [2/5], Step [310/1072], Loss: 0.3608\n","Epoch [2/5], Step [311/1072], Loss: 0.4263\n","Epoch [2/5], Step [312/1072], Loss: 0.3960\n","Epoch [2/5], Step [313/1072], Loss: 0.3829\n","Epoch [2/5], Step [314/1072], Loss: 0.3963\n","Epoch [2/5], Step [315/1072], Loss: 0.4096\n","Epoch [2/5], Step [316/1072], Loss: 0.3736\n","Epoch [2/5], Step [317/1072], Loss: 0.3903\n","Epoch [2/5], Step [318/1072], Loss: 0.4043\n","Epoch [2/5], Step [319/1072], Loss: 0.4152\n","Epoch [2/5], Step [320/1072], Loss: 0.3796\n","Epoch [2/5], Step [321/1072], Loss: 0.3918\n","Epoch [2/5], Step [322/1072], Loss: 0.4041\n","Epoch [2/5], Step [323/1072], Loss: 0.4007\n","Epoch [2/5], Step [324/1072], Loss: 0.3786\n","Epoch [2/5], Step [325/1072], Loss: 0.4056\n","Epoch [2/5], Step [326/1072], Loss: 0.3927\n","Epoch [2/5], Step [327/1072], Loss: 0.3879\n","Epoch [2/5], Step [328/1072], Loss: 0.4370\n","Epoch [2/5], Step [329/1072], Loss: 0.3960\n","Epoch [2/5], Step [330/1072], Loss: 0.4065\n","Epoch [2/5], Step [331/1072], Loss: 0.3945\n","Epoch [2/5], Step [332/1072], Loss: 0.4004\n","Epoch [2/5], Step [333/1072], Loss: 0.4135\n","Epoch [2/5], Step [334/1072], Loss: 0.4473\n","Epoch [2/5], Step [335/1072], Loss: 0.4077\n","Epoch [2/5], Step [336/1072], Loss: 0.3979\n","Epoch [2/5], Step [337/1072], Loss: 0.3875\n","Epoch [2/5], Step [338/1072], Loss: 0.4046\n","Epoch [2/5], Step [339/1072], Loss: 0.3942\n","Epoch [2/5], Step [340/1072], Loss: 0.4127\n","Epoch [2/5], Step [341/1072], Loss: 0.4001\n","Epoch [2/5], Step [342/1072], Loss: 0.4279\n","Epoch [2/5], Step [343/1072], Loss: 0.4169\n","Epoch [2/5], Step [344/1072], Loss: 0.3922\n","Epoch [2/5], Step [345/1072], Loss: 0.3804\n","Epoch [2/5], Step [346/1072], Loss: 0.4197\n","Epoch [2/5], Step [347/1072], Loss: 0.3886\n","Epoch [2/5], Step [348/1072], Loss: 0.4247\n","Epoch [2/5], Step [349/1072], Loss: 0.3894\n","Epoch [2/5], Step [350/1072], Loss: 0.4087\n","Epoch [2/5], Step [351/1072], Loss: 0.3745\n","Epoch [2/5], Step [352/1072], Loss: 0.4194\n","Epoch [2/5], Step [353/1072], Loss: 0.3809\n","Epoch [2/5], Step [354/1072], Loss: 0.3993\n","Epoch [2/5], Step [355/1072], Loss: 0.4019\n","Epoch [2/5], Step [356/1072], Loss: 0.3815\n","Epoch [2/5], Step [357/1072], Loss: 0.4194\n","Epoch [2/5], Step [358/1072], Loss: 0.3852\n","Epoch [2/5], Step [359/1072], Loss: 0.4053\n","Epoch [2/5], Step [360/1072], Loss: 0.3961\n","Epoch [2/5], Step [361/1072], Loss: 0.3760\n","Epoch [2/5], Step [362/1072], Loss: 0.3795\n","Epoch [2/5], Step [363/1072], Loss: 0.4084\n","Epoch [2/5], Step [364/1072], Loss: 0.4097\n","Epoch [2/5], Step [365/1072], Loss: 0.4012\n","Epoch [2/5], Step [366/1072], Loss: 0.4066\n","Epoch [2/5], Step [367/1072], Loss: 0.4072\n","Epoch [2/5], Step [368/1072], Loss: 0.4308\n","Epoch [2/5], Step [369/1072], Loss: 0.4215\n","Epoch [2/5], Step [370/1072], Loss: 0.4030\n","Epoch [2/5], Step [371/1072], Loss: 0.4202\n","Epoch [2/5], Step [372/1072], Loss: 0.3843\n","Epoch [2/5], Step [373/1072], Loss: 0.3985\n","Epoch [2/5], Step [374/1072], Loss: 0.4371\n","Epoch [2/5], Step [375/1072], Loss: 0.4204\n","Epoch [2/5], Step [376/1072], Loss: 0.3958\n","Epoch [2/5], Step [377/1072], Loss: 0.4256\n","Epoch [2/5], Step [378/1072], Loss: 0.4081\n","Epoch [2/5], Step [379/1072], Loss: 0.4057\n","Epoch [2/5], Step [380/1072], Loss: 0.3854\n","Epoch [2/5], Step [381/1072], Loss: 0.3906\n","Epoch [2/5], Step [382/1072], Loss: 0.4069\n","Epoch [2/5], Step [383/1072], Loss: 0.4089\n","Epoch [2/5], Step [384/1072], Loss: 0.4095\n","Epoch [2/5], Step [385/1072], Loss: 0.3841\n","Epoch [2/5], Step [386/1072], Loss: 0.3862\n","Epoch [2/5], Step [387/1072], Loss: 0.3965\n","Epoch [2/5], Step [388/1072], Loss: 0.4141\n","Epoch [2/5], Step [389/1072], Loss: 0.3986\n","Epoch [2/5], Step [390/1072], Loss: 0.4185\n","Epoch [2/5], Step [391/1072], Loss: 0.4317\n","Epoch [2/5], Step [392/1072], Loss: 0.3997\n","Epoch [2/5], Step [393/1072], Loss: 0.3913\n","Epoch [2/5], Step [394/1072], Loss: 0.4115\n","Epoch [2/5], Step [395/1072], Loss: 0.4250\n","Epoch [2/5], Step [396/1072], Loss: 0.3972\n","Epoch [2/5], Step [397/1072], Loss: 0.3896\n","Epoch [2/5], Step [398/1072], Loss: 0.4099\n","Epoch [2/5], Step [399/1072], Loss: 0.3911\n","Epoch [2/5], Step [400/1072], Loss: 0.4079\n","Epoch [2/5], Step [401/1072], Loss: 0.3853\n","Epoch [2/5], Step [402/1072], Loss: 0.3946\n","Epoch [2/5], Step [403/1072], Loss: 0.3981\n","Epoch [2/5], Step [404/1072], Loss: 0.3933\n","Epoch [2/5], Step [405/1072], Loss: 0.4010\n","Epoch [2/5], Step [406/1072], Loss: 0.3859\n","Epoch [2/5], Step [407/1072], Loss: 0.3977\n","Epoch [2/5], Step [408/1072], Loss: 0.4085\n","Epoch [2/5], Step [409/1072], Loss: 0.4042\n","Epoch [2/5], Step [410/1072], Loss: 0.3877\n","Epoch [2/5], Step [411/1072], Loss: 0.3787\n","Epoch [2/5], Step [412/1072], Loss: 0.3834\n","Epoch [2/5], Step [413/1072], Loss: 0.3861\n","Epoch [2/5], Step [414/1072], Loss: 0.4092\n","Epoch [2/5], Step [415/1072], Loss: 0.3854\n","Epoch [2/5], Step [416/1072], Loss: 0.3984\n","Epoch [2/5], Step [417/1072], Loss: 0.3964\n","Epoch [2/5], Step [418/1072], Loss: 0.4257\n","Epoch [2/5], Step [419/1072], Loss: 0.4272\n","Epoch [2/5], Step [420/1072], Loss: 0.4140\n","Epoch [2/5], Step [421/1072], Loss: 0.4072\n","Epoch [2/5], Step [422/1072], Loss: 0.3986\n","Epoch [2/5], Step [423/1072], Loss: 0.4040\n","Epoch [2/5], Step [424/1072], Loss: 0.3921\n","Epoch [2/5], Step [425/1072], Loss: 0.3895\n","Epoch [2/5], Step [426/1072], Loss: 0.4180\n","Epoch [2/5], Step [427/1072], Loss: 0.3991\n","Epoch [2/5], Step [428/1072], Loss: 0.4134\n","Epoch [2/5], Step [429/1072], Loss: 0.3867\n","Epoch [2/5], Step [430/1072], Loss: 0.3961\n","Epoch [2/5], Step [431/1072], Loss: 0.3846\n","Epoch [2/5], Step [432/1072], Loss: 0.4067\n","Epoch [2/5], Step [433/1072], Loss: 0.4116\n","Epoch [2/5], Step [434/1072], Loss: 0.4119\n","Epoch [2/5], Step [435/1072], Loss: 0.3799\n","Epoch [2/5], Step [436/1072], Loss: 0.4101\n","Epoch [2/5], Step [437/1072], Loss: 0.3842\n","Epoch [2/5], Step [438/1072], Loss: 0.4048\n","Epoch [2/5], Step [439/1072], Loss: 0.4110\n","Epoch [2/5], Step [440/1072], Loss: 0.4107\n","Epoch [2/5], Step [441/1072], Loss: 0.4185\n","Epoch [2/5], Step [442/1072], Loss: 0.3920\n","Epoch [2/5], Step [443/1072], Loss: 0.3926\n","Epoch [2/5], Step [444/1072], Loss: 0.3681\n","Epoch [2/5], Step [445/1072], Loss: 0.3929\n","Epoch [2/5], Step [446/1072], Loss: 0.4051\n","Epoch [2/5], Step [447/1072], Loss: 0.3455\n","Epoch [2/5], Step [448/1072], Loss: 0.4034\n","Epoch [2/5], Step [449/1072], Loss: 0.3968\n","Epoch [2/5], Step [450/1072], Loss: 0.3957\n","Epoch [2/5], Step [451/1072], Loss: 0.3807\n","Epoch [2/5], Step [452/1072], Loss: 0.4142\n","Epoch [2/5], Step [453/1072], Loss: 0.3992\n","Epoch [2/5], Step [454/1072], Loss: 0.3834\n","Epoch [2/5], Step [455/1072], Loss: 0.4026\n","Epoch [2/5], Step [456/1072], Loss: 0.3721\n","Epoch [2/5], Step [457/1072], Loss: 0.4160\n","Epoch [2/5], Step [458/1072], Loss: 0.3715\n","Epoch [2/5], Step [459/1072], Loss: 0.3949\n","Epoch [2/5], Step [460/1072], Loss: 0.4230\n","Epoch [2/5], Step [461/1072], Loss: 0.3965\n","Epoch [2/5], Step [462/1072], Loss: 0.4222\n","Epoch [2/5], Step [463/1072], Loss: 0.3755\n","Epoch [2/5], Step [464/1072], Loss: 0.3990\n","Epoch [2/5], Step [465/1072], Loss: 0.3801\n","Epoch [2/5], Step [466/1072], Loss: 0.3800\n","Epoch [2/5], Step [467/1072], Loss: 0.3919\n","Epoch [2/5], Step [468/1072], Loss: 0.4061\n","Epoch [2/5], Step [469/1072], Loss: 0.4073\n","Epoch [2/5], Step [470/1072], Loss: 0.3864\n","Epoch [2/5], Step [471/1072], Loss: 0.3623\n","Epoch [2/5], Step [472/1072], Loss: 0.4007\n","Epoch [2/5], Step [473/1072], Loss: 0.3795\n","Epoch [2/5], Step [474/1072], Loss: 0.3697\n","Epoch [2/5], Step [475/1072], Loss: 0.4043\n","Epoch [2/5], Step [476/1072], Loss: 0.3886\n","Epoch [2/5], Step [477/1072], Loss: 0.3976\n","Epoch [2/5], Step [478/1072], Loss: 0.4032\n","Epoch [2/5], Step [479/1072], Loss: 0.3960\n","Epoch [2/5], Step [480/1072], Loss: 0.3776\n","Epoch [2/5], Step [481/1072], Loss: 0.3787\n","Epoch [2/5], Step [482/1072], Loss: 0.3889\n","Epoch [2/5], Step [483/1072], Loss: 0.3702\n","Epoch [2/5], Step [484/1072], Loss: 0.4334\n","Epoch [2/5], Step [485/1072], Loss: 0.4057\n","Epoch [2/5], Step [486/1072], Loss: 0.3826\n","Epoch [2/5], Step [487/1072], Loss: 0.3975\n","Epoch [2/5], Step [488/1072], Loss: 0.4206\n","Epoch [2/5], Step [489/1072], Loss: 0.3778\n","Epoch [2/5], Step [490/1072], Loss: 0.4148\n","Epoch [2/5], Step [491/1072], Loss: 0.3869\n","Epoch [2/5], Step [492/1072], Loss: 0.4183\n","Epoch [2/5], Step [493/1072], Loss: 0.4363\n","Epoch [2/5], Step [494/1072], Loss: 0.3883\n","Epoch [2/5], Step [495/1072], Loss: 0.3809\n","Epoch [2/5], Step [496/1072], Loss: 0.4044\n","Epoch [2/5], Step [497/1072], Loss: 0.3640\n","Epoch [2/5], Step [498/1072], Loss: 0.3786\n","Epoch [2/5], Step [499/1072], Loss: 0.4161\n","Epoch [2/5], Step [500/1072], Loss: 0.3848\n","Epoch [2/5], Step [501/1072], Loss: 0.3740\n","Epoch [2/5], Step [502/1072], Loss: 0.3751\n","Epoch [2/5], Step [503/1072], Loss: 0.4002\n","Epoch [2/5], Step [504/1072], Loss: 0.3837\n","Epoch [2/5], Step [505/1072], Loss: 0.4164\n","Epoch [2/5], Step [506/1072], Loss: 0.3930\n","Epoch [2/5], Step [507/1072], Loss: 0.4319\n","Epoch [2/5], Step [508/1072], Loss: 0.3812\n","Epoch [2/5], Step [509/1072], Loss: 0.3758\n","Epoch [2/5], Step [510/1072], Loss: 0.3851\n","Epoch [2/5], Step [511/1072], Loss: 0.4105\n","Epoch [2/5], Step [512/1072], Loss: 0.4175\n","Epoch [2/5], Step [513/1072], Loss: 0.3869\n","Epoch [2/5], Step [514/1072], Loss: 0.3781\n","Epoch [2/5], Step [515/1072], Loss: 0.4085\n","Epoch [2/5], Step [516/1072], Loss: 0.3836\n","Epoch [2/5], Step [517/1072], Loss: 0.3935\n","Epoch [2/5], Step [518/1072], Loss: 0.3905\n","Epoch [2/5], Step [519/1072], Loss: 0.3917\n","Epoch [2/5], Step [520/1072], Loss: 0.4037\n","Epoch [2/5], Step [521/1072], Loss: 0.3741\n","Epoch [2/5], Step [522/1072], Loss: 0.4028\n","Epoch [2/5], Step [523/1072], Loss: 0.4158\n","Epoch [2/5], Step [524/1072], Loss: 0.3870\n","Epoch [2/5], Step [525/1072], Loss: 0.3990\n","Epoch [2/5], Step [526/1072], Loss: 0.3620\n","Epoch [2/5], Step [527/1072], Loss: 0.3883\n","Epoch [2/5], Step [528/1072], Loss: 0.4075\n","Epoch [2/5], Step [529/1072], Loss: 0.3989\n","Epoch [2/5], Step [530/1072], Loss: 0.3966\n","Epoch [2/5], Step [531/1072], Loss: 0.4424\n","Epoch [2/5], Step [532/1072], Loss: 0.4122\n","Epoch [2/5], Step [533/1072], Loss: 0.4196\n","Epoch [2/5], Step [534/1072], Loss: 0.4052\n","Epoch [2/5], Step [535/1072], Loss: 0.3947\n","Epoch [2/5], Step [536/1072], Loss: 0.3917\n","Epoch [2/5], Step [537/1072], Loss: 0.3783\n","Epoch [2/5], Step [538/1072], Loss: 0.4155\n","Epoch [2/5], Step [539/1072], Loss: 0.3860\n","Epoch [2/5], Step [540/1072], Loss: 0.3867\n","Epoch [2/5], Step [541/1072], Loss: 0.3947\n","Epoch [2/5], Step [542/1072], Loss: 0.3832\n","Epoch [2/5], Step [543/1072], Loss: 0.4058\n","Epoch [2/5], Step [544/1072], Loss: 0.4242\n","Epoch [2/5], Step [545/1072], Loss: 0.3815\n","Epoch [2/5], Step [546/1072], Loss: 0.4216\n","Epoch [2/5], Step [547/1072], Loss: 0.4346\n","Epoch [2/5], Step [548/1072], Loss: 0.3806\n","Epoch [2/5], Step [549/1072], Loss: 0.3809\n","Epoch [2/5], Step [550/1072], Loss: 0.4283\n","Epoch [2/5], Step [551/1072], Loss: 0.4295\n","Epoch [2/5], Step [552/1072], Loss: 0.4023\n","Epoch [2/5], Step [553/1072], Loss: 0.3877\n","Epoch [2/5], Step [554/1072], Loss: 0.3788\n","Epoch [2/5], Step [555/1072], Loss: 0.3932\n","Epoch [2/5], Step [556/1072], Loss: 0.3615\n","Epoch [2/5], Step [557/1072], Loss: 0.3805\n","Epoch [2/5], Step [558/1072], Loss: 0.3772\n","Epoch [2/5], Step [559/1072], Loss: 0.3877\n","Epoch [2/5], Step [560/1072], Loss: 0.4017\n","Epoch [2/5], Step [561/1072], Loss: 0.4084\n","Epoch [2/5], Step [562/1072], Loss: 0.3831\n","Epoch [2/5], Step [563/1072], Loss: 0.3853\n","Epoch [2/5], Step [564/1072], Loss: 0.3828\n","Epoch [2/5], Step [565/1072], Loss: 0.3809\n","Epoch [2/5], Step [566/1072], Loss: 0.3872\n","Epoch [2/5], Step [567/1072], Loss: 0.3745\n","Epoch [2/5], Step [568/1072], Loss: 0.4030\n","Epoch [2/5], Step [569/1072], Loss: 0.3710\n","Epoch [2/5], Step [570/1072], Loss: 0.3724\n","Epoch [2/5], Step [571/1072], Loss: 0.4232\n","Epoch [2/5], Step [572/1072], Loss: 0.3884\n","Epoch [2/5], Step [573/1072], Loss: 0.3913\n","Epoch [2/5], Step [574/1072], Loss: 0.3951\n","Epoch [2/5], Step [575/1072], Loss: 0.4140\n","Epoch [2/5], Step [576/1072], Loss: 0.3731\n","Epoch [2/5], Step [577/1072], Loss: 0.3992\n","Epoch [2/5], Step [578/1072], Loss: 0.3862\n","Epoch [2/5], Step [579/1072], Loss: 0.3773\n","Epoch [2/5], Step [580/1072], Loss: 0.4318\n","Epoch [2/5], Step [581/1072], Loss: 0.4314\n","Epoch [2/5], Step [582/1072], Loss: 0.4086\n","Epoch [2/5], Step [583/1072], Loss: 0.4149\n","Epoch [2/5], Step [584/1072], Loss: 0.4167\n","Epoch [2/5], Step [585/1072], Loss: 0.4176\n","Epoch [2/5], Step [586/1072], Loss: 0.3746\n","Epoch [2/5], Step [587/1072], Loss: 0.4029\n","Epoch [2/5], Step [588/1072], Loss: 0.3873\n","Epoch [2/5], Step [589/1072], Loss: 0.4171\n","Epoch [2/5], Step [590/1072], Loss: 0.3966\n","Epoch [2/5], Step [591/1072], Loss: 0.3763\n","Epoch [2/5], Step [592/1072], Loss: 0.3879\n","Epoch [2/5], Step [593/1072], Loss: 0.4205\n","Epoch [2/5], Step [594/1072], Loss: 0.4178\n","Epoch [2/5], Step [595/1072], Loss: 0.4244\n","Epoch [2/5], Step [596/1072], Loss: 0.4080\n","Epoch [2/5], Step [597/1072], Loss: 0.4164\n","Epoch [2/5], Step [598/1072], Loss: 0.3930\n","Epoch [2/5], Step [599/1072], Loss: 0.3995\n","Epoch [2/5], Step [600/1072], Loss: 0.3977\n","Epoch [2/5], Step [601/1072], Loss: 0.4307\n","Epoch [2/5], Step [602/1072], Loss: 0.3602\n","Epoch [2/5], Step [603/1072], Loss: 0.3979\n","Epoch [2/5], Step [604/1072], Loss: 0.3605\n","Epoch [2/5], Step [605/1072], Loss: 0.3903\n","Epoch [2/5], Step [606/1072], Loss: 0.3959\n","Epoch [2/5], Step [607/1072], Loss: 0.3859\n","Epoch [2/5], Step [608/1072], Loss: 0.3908\n","Epoch [2/5], Step [609/1072], Loss: 0.4061\n","Epoch [2/5], Step [610/1072], Loss: 0.3721\n","Epoch [2/5], Step [611/1072], Loss: 0.3888\n","Epoch [2/5], Step [612/1072], Loss: 0.4133\n","Epoch [2/5], Step [613/1072], Loss: 0.4244\n","Epoch [2/5], Step [614/1072], Loss: 0.3797\n","Epoch [2/5], Step [615/1072], Loss: 0.3795\n","Epoch [2/5], Step [616/1072], Loss: 0.4080\n","Epoch [2/5], Step [617/1072], Loss: 0.3870\n","Epoch [2/5], Step [618/1072], Loss: 0.3881\n","Epoch [2/5], Step [619/1072], Loss: 0.3779\n","Epoch [2/5], Step [620/1072], Loss: 0.3651\n","Epoch [2/5], Step [621/1072], Loss: 0.3873\n","Epoch [2/5], Step [622/1072], Loss: 0.4080\n","Epoch [2/5], Step [623/1072], Loss: 0.3990\n","Epoch [2/5], Step [624/1072], Loss: 0.4149\n","Epoch [2/5], Step [625/1072], Loss: 0.3992\n","Epoch [2/5], Step [626/1072], Loss: 0.3695\n","Epoch [2/5], Step [627/1072], Loss: 0.3932\n","Epoch [2/5], Step [628/1072], Loss: 0.4174\n","Epoch [2/5], Step [629/1072], Loss: 0.4053\n","Epoch [2/5], Step [630/1072], Loss: 0.3984\n","Epoch [2/5], Step [631/1072], Loss: 0.3720\n","Epoch [2/5], Step [632/1072], Loss: 0.4005\n","Epoch [2/5], Step [633/1072], Loss: 0.4303\n","Epoch [2/5], Step [634/1072], Loss: 0.3801\n","Epoch [2/5], Step [635/1072], Loss: 0.3713\n","Epoch [2/5], Step [636/1072], Loss: 0.3816\n","Epoch [2/5], Step [637/1072], Loss: 0.3751\n","Epoch [2/5], Step [638/1072], Loss: 0.4141\n","Epoch [2/5], Step [639/1072], Loss: 0.3993\n","Epoch [2/5], Step [640/1072], Loss: 0.4383\n","Epoch [2/5], Step [641/1072], Loss: 0.4005\n","Epoch [2/5], Step [642/1072], Loss: 0.4252\n","Epoch [2/5], Step [643/1072], Loss: 0.3976\n","Epoch [2/5], Step [644/1072], Loss: 0.4021\n","Epoch [2/5], Step [645/1072], Loss: 0.4081\n","Epoch [2/5], Step [646/1072], Loss: 0.4247\n","Epoch [2/5], Step [647/1072], Loss: 0.4036\n","Epoch [2/5], Step [648/1072], Loss: 0.3590\n","Epoch [2/5], Step [649/1072], Loss: 0.4001\n","Epoch [2/5], Step [650/1072], Loss: 0.3870\n","Epoch [2/5], Step [651/1072], Loss: 0.3947\n","Epoch [2/5], Step [652/1072], Loss: 0.3974\n","Epoch [2/5], Step [653/1072], Loss: 0.4093\n","Epoch [2/5], Step [654/1072], Loss: 0.4221\n","Epoch [2/5], Step [655/1072], Loss: 0.3790\n","Epoch [2/5], Step [656/1072], Loss: 0.4212\n","Epoch [2/5], Step [657/1072], Loss: 0.3978\n","Epoch [2/5], Step [658/1072], Loss: 0.3871\n","Epoch [2/5], Step [659/1072], Loss: 0.3861\n","Epoch [2/5], Step [660/1072], Loss: 0.3766\n","Epoch [2/5], Step [661/1072], Loss: 0.3790\n","Epoch [2/5], Step [662/1072], Loss: 0.3802\n","Epoch [2/5], Step [663/1072], Loss: 0.3840\n","Epoch [2/5], Step [664/1072], Loss: 0.3758\n","Epoch [2/5], Step [665/1072], Loss: 0.3874\n","Epoch [2/5], Step [666/1072], Loss: 0.4091\n","Epoch [2/5], Step [667/1072], Loss: 0.4026\n","Epoch [2/5], Step [668/1072], Loss: 0.3944\n","Epoch [2/5], Step [669/1072], Loss: 0.4172\n","Epoch [2/5], Step [670/1072], Loss: 0.4164\n","Epoch [2/5], Step [671/1072], Loss: 0.3788\n","Epoch [2/5], Step [672/1072], Loss: 0.4069\n","Epoch [2/5], Step [673/1072], Loss: 0.4159\n","Epoch [2/5], Step [674/1072], Loss: 0.4000\n","Epoch [2/5], Step [675/1072], Loss: 0.3989\n","Epoch [2/5], Step [676/1072], Loss: 0.4243\n","Epoch [2/5], Step [677/1072], Loss: 0.3884\n","Epoch [2/5], Step [678/1072], Loss: 0.4118\n","Epoch [2/5], Step [679/1072], Loss: 0.3719\n","Epoch [2/5], Step [680/1072], Loss: 0.3769\n","Epoch [2/5], Step [681/1072], Loss: 0.3795\n","Epoch [2/5], Step [682/1072], Loss: 0.4013\n","Epoch [2/5], Step [683/1072], Loss: 0.4174\n","Epoch [2/5], Step [684/1072], Loss: 0.3810\n","Epoch [2/5], Step [685/1072], Loss: 0.3768\n","Epoch [2/5], Step [686/1072], Loss: 0.3888\n","Epoch [2/5], Step [687/1072], Loss: 0.3903\n","Epoch [2/5], Step [688/1072], Loss: 0.3937\n","Epoch [2/5], Step [689/1072], Loss: 0.4203\n","Epoch [2/5], Step [690/1072], Loss: 0.3736\n","Epoch [2/5], Step [691/1072], Loss: 0.3830\n","Epoch [2/5], Step [692/1072], Loss: 0.3872\n","Epoch [2/5], Step [693/1072], Loss: 0.3911\n","Epoch [2/5], Step [694/1072], Loss: 0.3785\n","Epoch [2/5], Step [695/1072], Loss: 0.4362\n","Epoch [2/5], Step [696/1072], Loss: 0.4035\n","Epoch [2/5], Step [697/1072], Loss: 0.4241\n","Epoch [2/5], Step [698/1072], Loss: 0.4387\n","Epoch [2/5], Step [699/1072], Loss: 0.3790\n","Epoch [2/5], Step [700/1072], Loss: 0.3984\n","Epoch [2/5], Step [701/1072], Loss: 0.3963\n","Epoch [2/5], Step [702/1072], Loss: 0.3764\n","Epoch [2/5], Step [703/1072], Loss: 0.4044\n","Epoch [2/5], Step [704/1072], Loss: 0.3528\n","Epoch [2/5], Step [705/1072], Loss: 0.3931\n","Epoch [2/5], Step [706/1072], Loss: 0.3538\n","Epoch [2/5], Step [707/1072], Loss: 0.3800\n","Epoch [2/5], Step [708/1072], Loss: 0.3641\n","Epoch [2/5], Step [709/1072], Loss: 0.3927\n","Epoch [2/5], Step [710/1072], Loss: 0.3723\n","Epoch [2/5], Step [711/1072], Loss: 0.3802\n","Epoch [2/5], Step [712/1072], Loss: 0.3692\n","Epoch [2/5], Step [713/1072], Loss: 0.4030\n","Epoch [2/5], Step [714/1072], Loss: 0.3898\n","Epoch [2/5], Step [715/1072], Loss: 0.4114\n","Epoch [2/5], Step [716/1072], Loss: 0.4124\n","Epoch [2/5], Step [717/1072], Loss: 0.4070\n","Epoch [2/5], Step [718/1072], Loss: 0.3877\n","Epoch [2/5], Step [719/1072], Loss: 0.3847\n","Epoch [2/5], Step [720/1072], Loss: 0.4027\n","Epoch [2/5], Step [721/1072], Loss: 0.3539\n","Epoch [2/5], Step [722/1072], Loss: 0.3760\n","Epoch [2/5], Step [723/1072], Loss: 0.3746\n","Epoch [2/5], Step [724/1072], Loss: 0.3716\n","Epoch [2/5], Step [725/1072], Loss: 0.3644\n","Epoch [2/5], Step [726/1072], Loss: 0.4125\n","Epoch [2/5], Step [727/1072], Loss: 0.3795\n","Epoch [2/5], Step [728/1072], Loss: 0.3686\n","Epoch [2/5], Step [729/1072], Loss: 0.3874\n","Epoch [2/5], Step [730/1072], Loss: 0.3771\n","Epoch [2/5], Step [731/1072], Loss: 0.4293\n","Epoch [2/5], Step [732/1072], Loss: 0.4185\n","Epoch [2/5], Step [733/1072], Loss: 0.4102\n","Epoch [2/5], Step [734/1072], Loss: 0.3851\n","Epoch [2/5], Step [735/1072], Loss: 0.4081\n","Epoch [2/5], Step [736/1072], Loss: 0.4063\n","Epoch [2/5], Step [737/1072], Loss: 0.3987\n","Epoch [2/5], Step [738/1072], Loss: 0.3829\n","Epoch [2/5], Step [739/1072], Loss: 0.3936\n","Epoch [2/5], Step [740/1072], Loss: 0.3828\n","Epoch [2/5], Step [741/1072], Loss: 0.4210\n","Epoch [2/5], Step [742/1072], Loss: 0.3857\n","Epoch [2/5], Step [743/1072], Loss: 0.3757\n","Epoch [2/5], Step [744/1072], Loss: 0.4045\n","Epoch [2/5], Step [745/1072], Loss: 0.3785\n","Epoch [2/5], Step [746/1072], Loss: 0.4350\n","Epoch [2/5], Step [747/1072], Loss: 0.4106\n","Epoch [2/5], Step [748/1072], Loss: 0.4194\n","Epoch [2/5], Step [749/1072], Loss: 0.4035\n","Epoch [2/5], Step [750/1072], Loss: 0.3965\n","Epoch [2/5], Step [751/1072], Loss: 0.4169\n","Epoch [2/5], Step [752/1072], Loss: 0.3914\n","Epoch [2/5], Step [753/1072], Loss: 0.4361\n","Epoch [2/5], Step [754/1072], Loss: 0.3760\n","Epoch [2/5], Step [755/1072], Loss: 0.3831\n","Epoch [2/5], Step [756/1072], Loss: 0.3850\n","Epoch [2/5], Step [757/1072], Loss: 0.3942\n","Epoch [2/5], Step [758/1072], Loss: 0.3875\n","Epoch [2/5], Step [759/1072], Loss: 0.3954\n","Epoch [2/5], Step [760/1072], Loss: 0.3947\n","Epoch [2/5], Step [761/1072], Loss: 0.4035\n","Epoch [2/5], Step [762/1072], Loss: 0.4078\n","Epoch [2/5], Step [763/1072], Loss: 0.3691\n","Epoch [2/5], Step [764/1072], Loss: 0.4120\n","Epoch [2/5], Step [765/1072], Loss: 0.3845\n","Epoch [2/5], Step [766/1072], Loss: 0.3926\n","Epoch [2/5], Step [767/1072], Loss: 0.4356\n","Epoch [2/5], Step [768/1072], Loss: 0.3872\n","Epoch [2/5], Step [769/1072], Loss: 0.3948\n","Epoch [2/5], Step [770/1072], Loss: 0.3793\n","Epoch [2/5], Step [771/1072], Loss: 0.3782\n","Epoch [2/5], Step [772/1072], Loss: 0.4137\n","Epoch [2/5], Step [773/1072], Loss: 0.4120\n","Epoch [2/5], Step [774/1072], Loss: 0.4071\n","Epoch [2/5], Step [775/1072], Loss: 0.3986\n","Epoch [2/5], Step [776/1072], Loss: 0.4132\n","Epoch [2/5], Step [777/1072], Loss: 0.3882\n","Epoch [2/5], Step [778/1072], Loss: 0.3749\n","Epoch [2/5], Step [779/1072], Loss: 0.3971\n","Epoch [2/5], Step [780/1072], Loss: 0.3922\n","Epoch [2/5], Step [781/1072], Loss: 0.3576\n","Epoch [2/5], Step [782/1072], Loss: 0.4240\n","Epoch [2/5], Step [783/1072], Loss: 0.3772\n","Epoch [2/5], Step [784/1072], Loss: 0.3873\n","Epoch [2/5], Step [785/1072], Loss: 0.4340\n","Epoch [2/5], Step [786/1072], Loss: 0.4123\n","Epoch [2/5], Step [787/1072], Loss: 0.4240\n","Epoch [2/5], Step [788/1072], Loss: 0.4137\n","Epoch [2/5], Step [789/1072], Loss: 0.3561\n","Epoch [2/5], Step [790/1072], Loss: 0.3996\n","Epoch [2/5], Step [791/1072], Loss: 0.3933\n","Epoch [2/5], Step [792/1072], Loss: 0.4057\n","Epoch [2/5], Step [793/1072], Loss: 0.3923\n","Epoch [2/5], Step [794/1072], Loss: 0.4174\n","Epoch [2/5], Step [795/1072], Loss: 0.3701\n","Epoch [2/5], Step [796/1072], Loss: 0.3917\n","Epoch [2/5], Step [797/1072], Loss: 0.3845\n","Epoch [2/5], Step [798/1072], Loss: 0.3723\n","Epoch [2/5], Step [799/1072], Loss: 0.3717\n","Epoch [2/5], Step [800/1072], Loss: 0.3771\n","Epoch [2/5], Step [801/1072], Loss: 0.3819\n","Epoch [2/5], Step [802/1072], Loss: 0.4038\n","Epoch [2/5], Step [803/1072], Loss: 0.3757\n","Epoch [2/5], Step [804/1072], Loss: 0.3942\n","Epoch [2/5], Step [805/1072], Loss: 0.3541\n","Epoch [2/5], Step [806/1072], Loss: 0.4244\n","Epoch [2/5], Step [807/1072], Loss: 0.3939\n","Epoch [2/5], Step [808/1072], Loss: 0.4165\n","Epoch [2/5], Step [809/1072], Loss: 0.4011\n","Epoch [2/5], Step [810/1072], Loss: 0.4170\n","Epoch [2/5], Step [811/1072], Loss: 0.3845\n","Epoch [2/5], Step [812/1072], Loss: 0.4030\n","Epoch [2/5], Step [813/1072], Loss: 0.3885\n","Epoch [2/5], Step [814/1072], Loss: 0.3739\n","Epoch [2/5], Step [815/1072], Loss: 0.3663\n","Epoch [2/5], Step [816/1072], Loss: 0.4066\n","Epoch [2/5], Step [817/1072], Loss: 0.3439\n","Epoch [2/5], Step [818/1072], Loss: 0.3830\n","Epoch [2/5], Step [819/1072], Loss: 0.3827\n","Epoch [2/5], Step [820/1072], Loss: 0.3816\n","Epoch [2/5], Step [821/1072], Loss: 0.3991\n","Epoch [2/5], Step [822/1072], Loss: 0.4100\n","Epoch [2/5], Step [823/1072], Loss: 0.4058\n","Epoch [2/5], Step [824/1072], Loss: 0.3992\n","Epoch [2/5], Step [825/1072], Loss: 0.3864\n","Epoch [2/5], Step [826/1072], Loss: 0.3880\n","Epoch [2/5], Step [827/1072], Loss: 0.3825\n","Epoch [2/5], Step [828/1072], Loss: 0.3948\n","Epoch [2/5], Step [829/1072], Loss: 0.3806\n","Epoch [2/5], Step [830/1072], Loss: 0.3811\n","Epoch [2/5], Step [831/1072], Loss: 0.3961\n","Epoch [2/5], Step [832/1072], Loss: 0.4139\n","Epoch [2/5], Step [833/1072], Loss: 0.4162\n","Epoch [2/5], Step [834/1072], Loss: 0.4207\n","Epoch [2/5], Step [835/1072], Loss: 0.3918\n","Epoch [2/5], Step [836/1072], Loss: 0.3930\n","Epoch [2/5], Step [837/1072], Loss: 0.4147\n","Epoch [2/5], Step [838/1072], Loss: 0.3505\n","Epoch [2/5], Step [839/1072], Loss: 0.4104\n","Epoch [2/5], Step [840/1072], Loss: 0.3934\n","Epoch [2/5], Step [841/1072], Loss: 0.4126\n","Epoch [2/5], Step [842/1072], Loss: 0.3777\n","Epoch [2/5], Step [843/1072], Loss: 0.3634\n","Epoch [2/5], Step [844/1072], Loss: 0.3981\n","Epoch [2/5], Step [845/1072], Loss: 0.4099\n","Epoch [2/5], Step [846/1072], Loss: 0.4191\n","Epoch [2/5], Step [847/1072], Loss: 0.3745\n","Epoch [2/5], Step [848/1072], Loss: 0.3817\n","Epoch [2/5], Step [849/1072], Loss: 0.3903\n","Epoch [2/5], Step [850/1072], Loss: 0.4064\n","Epoch [2/5], Step [851/1072], Loss: 0.3939\n","Epoch [2/5], Step [852/1072], Loss: 0.3810\n","Epoch [2/5], Step [853/1072], Loss: 0.3737\n","Epoch [2/5], Step [854/1072], Loss: 0.3898\n","Epoch [2/5], Step [855/1072], Loss: 0.3871\n","Epoch [2/5], Step [856/1072], Loss: 0.3851\n","Epoch [2/5], Step [857/1072], Loss: 0.3595\n","Epoch [2/5], Step [858/1072], Loss: 0.3982\n","Epoch [2/5], Step [859/1072], Loss: 0.3929\n","Epoch [2/5], Step [860/1072], Loss: 0.3798\n","Epoch [2/5], Step [861/1072], Loss: 0.3855\n","Epoch [2/5], Step [862/1072], Loss: 0.4007\n","Epoch [2/5], Step [863/1072], Loss: 0.3986\n","Epoch [2/5], Step [864/1072], Loss: 0.3899\n","Epoch [2/5], Step [865/1072], Loss: 0.3816\n","Epoch [2/5], Step [866/1072], Loss: 0.3694\n","Epoch [2/5], Step [867/1072], Loss: 0.4028\n","Epoch [2/5], Step [868/1072], Loss: 0.4028\n","Epoch [2/5], Step [869/1072], Loss: 0.4094\n","Epoch [2/5], Step [870/1072], Loss: 0.3601\n","Epoch [2/5], Step [871/1072], Loss: 0.3922\n","Epoch [2/5], Step [872/1072], Loss: 0.3742\n","Epoch [2/5], Step [873/1072], Loss: 0.3986\n","Epoch [2/5], Step [874/1072], Loss: 0.4154\n","Epoch [2/5], Step [875/1072], Loss: 0.3840\n","Epoch [2/5], Step [876/1072], Loss: 0.3591\n","Epoch [2/5], Step [877/1072], Loss: 0.3832\n","Epoch [2/5], Step [878/1072], Loss: 0.3832\n","Epoch [2/5], Step [879/1072], Loss: 0.4070\n","Epoch [2/5], Step [880/1072], Loss: 0.3670\n","Epoch [2/5], Step [881/1072], Loss: 0.3754\n","Epoch [2/5], Step [882/1072], Loss: 0.3803\n","Epoch [2/5], Step [883/1072], Loss: 0.4023\n","Epoch [2/5], Step [884/1072], Loss: 0.4055\n","Epoch [2/5], Step [885/1072], Loss: 0.4283\n","Epoch [2/5], Step [886/1072], Loss: 0.3673\n","Epoch [2/5], Step [887/1072], Loss: 0.3849\n","Epoch [2/5], Step [888/1072], Loss: 0.3716\n","Epoch [2/5], Step [889/1072], Loss: 0.3911\n","Epoch [2/5], Step [890/1072], Loss: 0.4190\n","Epoch [2/5], Step [891/1072], Loss: 0.4071\n","Epoch [2/5], Step [892/1072], Loss: 0.3850\n","Epoch [2/5], Step [893/1072], Loss: 0.4308\n","Epoch [2/5], Step [894/1072], Loss: 0.4146\n","Epoch [2/5], Step [895/1072], Loss: 0.3924\n","Epoch [2/5], Step [896/1072], Loss: 0.3826\n","Epoch [2/5], Step [897/1072], Loss: 0.4290\n","Epoch [2/5], Step [898/1072], Loss: 0.3535\n","Epoch [2/5], Step [899/1072], Loss: 0.3891\n","Epoch [2/5], Step [900/1072], Loss: 0.4033\n","Epoch [2/5], Step [901/1072], Loss: 0.3898\n","Epoch [2/5], Step [902/1072], Loss: 0.3833\n","Epoch [2/5], Step [903/1072], Loss: 0.3698\n","Epoch [2/5], Step [904/1072], Loss: 0.4162\n","Epoch [2/5], Step [905/1072], Loss: 0.3681\n","Epoch [2/5], Step [906/1072], Loss: 0.4019\n","Epoch [2/5], Step [907/1072], Loss: 0.4000\n","Epoch [2/5], Step [908/1072], Loss: 0.3700\n","Epoch [2/5], Step [909/1072], Loss: 0.3834\n","Epoch [2/5], Step [910/1072], Loss: 0.4158\n","Epoch [2/5], Step [911/1072], Loss: 0.3604\n","Epoch [2/5], Step [912/1072], Loss: 0.3971\n","Epoch [2/5], Step [913/1072], Loss: 0.3849\n","Epoch [2/5], Step [914/1072], Loss: 0.3697\n","Epoch [2/5], Step [915/1072], Loss: 0.3729\n","Epoch [2/5], Step [916/1072], Loss: 0.3894\n","Epoch [2/5], Step [917/1072], Loss: 0.3784\n","Epoch [2/5], Step [918/1072], Loss: 0.3706\n","Epoch [2/5], Step [919/1072], Loss: 0.3892\n","Epoch [2/5], Step [920/1072], Loss: 0.3994\n","Epoch [2/5], Step [921/1072], Loss: 0.3510\n","Epoch [2/5], Step [922/1072], Loss: 0.3899\n","Epoch [2/5], Step [923/1072], Loss: 0.3629\n","Epoch [2/5], Step [924/1072], Loss: 0.4250\n","Epoch [2/5], Step [925/1072], Loss: 0.4173\n","Epoch [2/5], Step [926/1072], Loss: 0.3892\n","Epoch [2/5], Step [927/1072], Loss: 0.3816\n","Epoch [2/5], Step [928/1072], Loss: 0.4054\n","Epoch [2/5], Step [929/1072], Loss: 0.4106\n","Epoch [2/5], Step [930/1072], Loss: 0.3895\n","Epoch [2/5], Step [931/1072], Loss: 0.4131\n","Epoch [2/5], Step [932/1072], Loss: 0.3853\n","Epoch [2/5], Step [933/1072], Loss: 0.4050\n","Epoch [2/5], Step [934/1072], Loss: 0.3825\n","Epoch [2/5], Step [935/1072], Loss: 0.3794\n","Epoch [2/5], Step [936/1072], Loss: 0.3713\n","Epoch [2/5], Step [937/1072], Loss: 0.3959\n","Epoch [2/5], Step [938/1072], Loss: 0.3909\n","Epoch [2/5], Step [939/1072], Loss: 0.3948\n","Epoch [2/5], Step [940/1072], Loss: 0.3559\n","Epoch [2/5], Step [941/1072], Loss: 0.3812\n","Epoch [2/5], Step [942/1072], Loss: 0.4131\n","Epoch [2/5], Step [943/1072], Loss: 0.3974\n","Epoch [2/5], Step [944/1072], Loss: 0.4204\n","Epoch [2/5], Step [945/1072], Loss: 0.3713\n","Epoch [2/5], Step [946/1072], Loss: 0.3911\n","Epoch [2/5], Step [947/1072], Loss: 0.3677\n","Epoch [2/5], Step [948/1072], Loss: 0.3547\n","Epoch [2/5], Step [949/1072], Loss: 0.3820\n","Epoch [2/5], Step [950/1072], Loss: 0.4010\n","Epoch [2/5], Step [951/1072], Loss: 0.3875\n","Epoch [2/5], Step [952/1072], Loss: 0.4217\n","Epoch [2/5], Step [953/1072], Loss: 0.3805\n","Epoch [2/5], Step [954/1072], Loss: 0.3829\n","Epoch [2/5], Step [955/1072], Loss: 0.3631\n","Epoch [2/5], Step [956/1072], Loss: 0.3929\n","Epoch [2/5], Step [957/1072], Loss: 0.3786\n","Epoch [2/5], Step [958/1072], Loss: 0.3783\n","Epoch [2/5], Step [959/1072], Loss: 0.3610\n","Epoch [2/5], Step [960/1072], Loss: 0.3924\n","Epoch [2/5], Step [961/1072], Loss: 0.4083\n","Epoch [2/5], Step [962/1072], Loss: 0.4006\n","Epoch [2/5], Step [963/1072], Loss: 0.3746\n","Epoch [2/5], Step [964/1072], Loss: 0.3898\n","Epoch [2/5], Step [965/1072], Loss: 0.3655\n","Epoch [2/5], Step [966/1072], Loss: 0.4138\n","Epoch [2/5], Step [967/1072], Loss: 0.4120\n","Epoch [2/5], Step [968/1072], Loss: 0.4196\n","Epoch [2/5], Step [969/1072], Loss: 0.4101\n","Epoch [2/5], Step [970/1072], Loss: 0.3854\n","Epoch [2/5], Step [971/1072], Loss: 0.4405\n","Epoch [2/5], Step [972/1072], Loss: 0.3905\n","Epoch [2/5], Step [973/1072], Loss: 0.4080\n","Epoch [2/5], Step [974/1072], Loss: 0.4199\n","Epoch [2/5], Step [975/1072], Loss: 0.3929\n","Epoch [2/5], Step [976/1072], Loss: 0.3861\n","Epoch [2/5], Step [977/1072], Loss: 0.4101\n","Epoch [2/5], Step [978/1072], Loss: 0.3846\n","Epoch [2/5], Step [979/1072], Loss: 0.3984\n","Epoch [2/5], Step [980/1072], Loss: 0.3479\n","Epoch [2/5], Step [981/1072], Loss: 0.3873\n","Epoch [2/5], Step [982/1072], Loss: 0.3813\n","Epoch [2/5], Step [983/1072], Loss: 0.3940\n","Epoch [2/5], Step [984/1072], Loss: 0.3826\n","Epoch [2/5], Step [985/1072], Loss: 0.4229\n","Epoch [2/5], Step [986/1072], Loss: 0.3798\n","Epoch [2/5], Step [987/1072], Loss: 0.3784\n","Epoch [2/5], Step [988/1072], Loss: 0.3982\n","Epoch [2/5], Step [989/1072], Loss: 0.3861\n","Epoch [2/5], Step [990/1072], Loss: 0.3997\n","Epoch [2/5], Step [991/1072], Loss: 0.3871\n","Epoch [2/5], Step [992/1072], Loss: 0.3728\n","Epoch [2/5], Step [993/1072], Loss: 0.4123\n","Epoch [2/5], Step [994/1072], Loss: 0.3918\n","Epoch [2/5], Step [995/1072], Loss: 0.3600\n","Epoch [2/5], Step [996/1072], Loss: 0.3584\n","Epoch [2/5], Step [997/1072], Loss: 0.3886\n","Epoch [2/5], Step [998/1072], Loss: 0.3864\n","Epoch [2/5], Step [999/1072], Loss: 0.3947\n","Epoch [2/5], Step [1000/1072], Loss: 0.3981\n","Epoch [2/5], Step [1001/1072], Loss: 0.3852\n","Epoch [2/5], Step [1002/1072], Loss: 0.3736\n","Epoch [2/5], Step [1003/1072], Loss: 0.3932\n","Epoch [2/5], Step [1004/1072], Loss: 0.3981\n","Epoch [2/5], Step [1005/1072], Loss: 0.3778\n","Epoch [2/5], Step [1006/1072], Loss: 0.3746\n","Epoch [2/5], Step [1007/1072], Loss: 0.3676\n","Epoch [2/5], Step [1008/1072], Loss: 0.4005\n","Epoch [2/5], Step [1009/1072], Loss: 0.3849\n","Epoch [2/5], Step [1010/1072], Loss: 0.4183\n","Epoch [2/5], Step [1011/1072], Loss: 0.4221\n","Epoch [2/5], Step [1012/1072], Loss: 0.4043\n","Epoch [2/5], Step [1013/1072], Loss: 0.3981\n","Epoch [2/5], Step [1014/1072], Loss: 0.4188\n","Epoch [2/5], Step [1015/1072], Loss: 0.3882\n","Epoch [2/5], Step [1016/1072], Loss: 0.3672\n","Epoch [2/5], Step [1017/1072], Loss: 0.4002\n","Epoch [2/5], Step [1018/1072], Loss: 0.3875\n","Epoch [2/5], Step [1019/1072], Loss: 0.3931\n","Epoch [2/5], Step [1020/1072], Loss: 0.4268\n","Epoch [2/5], Step [1021/1072], Loss: 0.3827\n","Epoch [2/5], Step [1022/1072], Loss: 0.4341\n","Epoch [2/5], Step [1023/1072], Loss: 0.3970\n","Epoch [2/5], Step [1024/1072], Loss: 0.4036\n","Epoch [2/5], Step [1025/1072], Loss: 0.3778\n","Epoch [2/5], Step [1026/1072], Loss: 0.4178\n","Epoch [2/5], Step [1027/1072], Loss: 0.3809\n","Epoch [2/5], Step [1028/1072], Loss: 0.4022\n","Epoch [2/5], Step [1029/1072], Loss: 0.3975\n","Epoch [2/5], Step [1030/1072], Loss: 0.4083\n","Epoch [2/5], Step [1031/1072], Loss: 0.3748\n","Epoch [2/5], Step [1032/1072], Loss: 0.3803\n","Epoch [2/5], Step [1033/1072], Loss: 0.4255\n","Epoch [2/5], Step [1034/1072], Loss: 0.3871\n","Epoch [2/5], Step [1035/1072], Loss: 0.4212\n","Epoch [2/5], Step [1036/1072], Loss: 0.3741\n","Epoch [2/5], Step [1037/1072], Loss: 0.3897\n","Epoch [2/5], Step [1038/1072], Loss: 0.3856\n","Epoch [2/5], Step [1039/1072], Loss: 0.4131\n","Epoch [2/5], Step [1040/1072], Loss: 0.3816\n","Epoch [2/5], Step [1041/1072], Loss: 0.3736\n","Epoch [2/5], Step [1042/1072], Loss: 0.3589\n","Epoch [2/5], Step [1043/1072], Loss: 0.3671\n","Epoch [2/5], Step [1044/1072], Loss: 0.3989\n","Epoch [2/5], Step [1045/1072], Loss: 0.3853\n","Epoch [2/5], Step [1046/1072], Loss: 0.4260\n","Epoch [2/5], Step [1047/1072], Loss: 0.4009\n","Epoch [2/5], Step [1048/1072], Loss: 0.3963\n","Epoch [2/5], Step [1049/1072], Loss: 0.3730\n","Epoch [2/5], Step [1050/1072], Loss: 0.3712\n","Epoch [2/5], Step [1051/1072], Loss: 0.3943\n","Epoch [2/5], Step [1052/1072], Loss: 0.3794\n","Epoch [2/5], Step [1053/1072], Loss: 0.3623\n","Epoch [2/5], Step [1054/1072], Loss: 0.3856\n","Epoch [2/5], Step [1055/1072], Loss: 0.3953\n","Epoch [2/5], Step [1056/1072], Loss: 0.3988\n","Epoch [2/5], Step [1057/1072], Loss: 0.4075\n","Epoch [2/5], Step [1058/1072], Loss: 0.3671\n","Epoch [2/5], Step [1059/1072], Loss: 0.3727\n","Epoch [2/5], Step [1060/1072], Loss: 0.3905\n","Epoch [2/5], Step [1061/1072], Loss: 0.4084\n","Epoch [2/5], Step [1062/1072], Loss: 0.3819\n","Epoch [2/5], Step [1063/1072], Loss: 0.3903\n","Epoch [2/5], Step [1064/1072], Loss: 0.3870\n","Epoch [2/5], Step [1065/1072], Loss: 0.3671\n","Epoch [2/5], Step [1066/1072], Loss: 0.3967\n","Epoch [2/5], Step [1067/1072], Loss: 0.4286\n","Epoch [2/5], Step [1068/1072], Loss: 0.3684\n","Epoch [2/5], Step [1069/1072], Loss: 0.3972\n","Epoch [2/5], Step [1070/1072], Loss: 0.4028\n","Epoch [2/5], Step [1071/1072], Loss: 0.4075\n","Epoch [2/5], Step [1072/1072], Loss: 0.3730\n","Epoch [3/5], Step [1/1072], Loss: 0.3698\n","Epoch [3/5], Step [2/1072], Loss: 0.3647\n","Epoch [3/5], Step [3/1072], Loss: 0.3935\n","Epoch [3/5], Step [4/1072], Loss: 0.3762\n","Epoch [3/5], Step [5/1072], Loss: 0.3462\n","Epoch [3/5], Step [6/1072], Loss: 0.3769\n","Epoch [3/5], Step [7/1072], Loss: 0.3674\n","Epoch [3/5], Step [8/1072], Loss: 0.3538\n","Epoch [3/5], Step [9/1072], Loss: 0.3360\n","Epoch [3/5], Step [10/1072], Loss: 0.3528\n","Epoch [3/5], Step [11/1072], Loss: 0.3675\n","Epoch [3/5], Step [12/1072], Loss: 0.3824\n","Epoch [3/5], Step [13/1072], Loss: 0.3737\n","Epoch [3/5], Step [14/1072], Loss: 0.3398\n","Epoch [3/5], Step [15/1072], Loss: 0.3818\n","Epoch [3/5], Step [16/1072], Loss: 0.3660\n","Epoch [3/5], Step [17/1072], Loss: 0.3590\n","Epoch [3/5], Step [18/1072], Loss: 0.3475\n","Epoch [3/5], Step [19/1072], Loss: 0.3579\n","Epoch [3/5], Step [20/1072], Loss: 0.3232\n","Epoch [3/5], Step [21/1072], Loss: 0.3748\n","Epoch [3/5], Step [22/1072], Loss: 0.3412\n","Epoch [3/5], Step [23/1072], Loss: 0.3971\n","Epoch [3/5], Step [24/1072], Loss: 0.3648\n","Epoch [3/5], Step [25/1072], Loss: 0.3640\n","Epoch [3/5], Step [26/1072], Loss: 0.3464\n","Epoch [3/5], Step [27/1072], Loss: 0.3876\n","Epoch [3/5], Step [28/1072], Loss: 0.3827\n","Epoch [3/5], Step [29/1072], Loss: 0.3545\n","Epoch [3/5], Step [30/1072], Loss: 0.3464\n","Epoch [3/5], Step [31/1072], Loss: 0.3552\n","Epoch [3/5], Step [32/1072], Loss: 0.3582\n","Epoch [3/5], Step [33/1072], Loss: 0.3391\n","Epoch [3/5], Step [34/1072], Loss: 0.3937\n","Epoch [3/5], Step [35/1072], Loss: 0.3569\n","Epoch [3/5], Step [36/1072], Loss: 0.3372\n","Epoch [3/5], Step [37/1072], Loss: 0.3552\n","Epoch [3/5], Step [38/1072], Loss: 0.3657\n","Epoch [3/5], Step [39/1072], Loss: 0.3481\n","Epoch [3/5], Step [40/1072], Loss: 0.3440\n","Epoch [3/5], Step [41/1072], Loss: 0.3591\n","Epoch [3/5], Step [42/1072], Loss: 0.3808\n","Epoch [3/5], Step [43/1072], Loss: 0.3399\n","Epoch [3/5], Step [44/1072], Loss: 0.3461\n","Epoch [3/5], Step [45/1072], Loss: 0.3893\n","Epoch [3/5], Step [46/1072], Loss: 0.3576\n","Epoch [3/5], Step [47/1072], Loss: 0.3981\n","Epoch [3/5], Step [48/1072], Loss: 0.3702\n","Epoch [3/5], Step [49/1072], Loss: 0.3335\n","Epoch [3/5], Step [50/1072], Loss: 0.3691\n","Epoch [3/5], Step [51/1072], Loss: 0.3476\n","Epoch [3/5], Step [52/1072], Loss: 0.3695\n","Epoch [3/5], Step [53/1072], Loss: 0.4163\n","Epoch [3/5], Step [54/1072], Loss: 0.3524\n","Epoch [3/5], Step [55/1072], Loss: 0.3924\n","Epoch [3/5], Step [56/1072], Loss: 0.3605\n","Epoch [3/5], Step [57/1072], Loss: 0.3758\n","Epoch [3/5], Step [58/1072], Loss: 0.3492\n","Epoch [3/5], Step [59/1072], Loss: 0.3468\n","Epoch [3/5], Step [60/1072], Loss: 0.3773\n","Epoch [3/5], Step [61/1072], Loss: 0.3595\n","Epoch [3/5], Step [62/1072], Loss: 0.3238\n","Epoch [3/5], Step [63/1072], Loss: 0.3689\n","Epoch [3/5], Step [64/1072], Loss: 0.3581\n","Epoch [3/5], Step [65/1072], Loss: 0.3863\n","Epoch [3/5], Step [66/1072], Loss: 0.3561\n","Epoch [3/5], Step [67/1072], Loss: 0.3715\n","Epoch [3/5], Step [68/1072], Loss: 0.3523\n","Epoch [3/5], Step [69/1072], Loss: 0.3474\n","Epoch [3/5], Step [70/1072], Loss: 0.3685\n","Epoch [3/5], Step [71/1072], Loss: 0.3674\n","Epoch [3/5], Step [72/1072], Loss: 0.3721\n","Epoch [3/5], Step [73/1072], Loss: 0.3466\n","Epoch [3/5], Step [74/1072], Loss: 0.3460\n","Epoch [3/5], Step [75/1072], Loss: 0.3717\n","Epoch [3/5], Step [76/1072], Loss: 0.3419\n","Epoch [3/5], Step [77/1072], Loss: 0.3683\n","Epoch [3/5], Step [78/1072], Loss: 0.3542\n","Epoch [3/5], Step [79/1072], Loss: 0.3546\n","Epoch [3/5], Step [80/1072], Loss: 0.3426\n","Epoch [3/5], Step [81/1072], Loss: 0.3712\n","Epoch [3/5], Step [82/1072], Loss: 0.3693\n","Epoch [3/5], Step [83/1072], Loss: 0.3839\n","Epoch [3/5], Step [84/1072], Loss: 0.3800\n","Epoch [3/5], Step [85/1072], Loss: 0.3571\n","Epoch [3/5], Step [86/1072], Loss: 0.3860\n","Epoch [3/5], Step [87/1072], Loss: 0.3497\n","Epoch [3/5], Step [88/1072], Loss: 0.3633\n","Epoch [3/5], Step [89/1072], Loss: 0.3481\n","Epoch [3/5], Step [90/1072], Loss: 0.3854\n","Epoch [3/5], Step [91/1072], Loss: 0.3511\n","Epoch [3/5], Step [92/1072], Loss: 0.3770\n","Epoch [3/5], Step [93/1072], Loss: 0.4090\n","Epoch [3/5], Step [94/1072], Loss: 0.3462\n","Epoch [3/5], Step [95/1072], Loss: 0.3787\n","Epoch [3/5], Step [96/1072], Loss: 0.3832\n","Epoch [3/5], Step [97/1072], Loss: 0.3261\n","Epoch [3/5], Step [98/1072], Loss: 0.4131\n","Epoch [3/5], Step [99/1072], Loss: 0.3747\n","Epoch [3/5], Step [100/1072], Loss: 0.3712\n","Epoch [3/5], Step [101/1072], Loss: 0.3385\n","Epoch [3/5], Step [102/1072], Loss: 0.3505\n","Epoch [3/5], Step [103/1072], Loss: 0.3911\n","Epoch [3/5], Step [104/1072], Loss: 0.3383\n","Epoch [3/5], Step [105/1072], Loss: 0.3523\n","Epoch [3/5], Step [106/1072], Loss: 0.4065\n","Epoch [3/5], Step [107/1072], Loss: 0.3730\n","Epoch [3/5], Step [108/1072], Loss: 0.3283\n","Epoch [3/5], Step [109/1072], Loss: 0.3790\n","Epoch [3/5], Step [110/1072], Loss: 0.3556\n","Epoch [3/5], Step [111/1072], Loss: 0.3566\n","Epoch [3/5], Step [112/1072], Loss: 0.3939\n","Epoch [3/5], Step [113/1072], Loss: 0.3323\n","Epoch [3/5], Step [114/1072], Loss: 0.3790\n","Epoch [3/5], Step [115/1072], Loss: 0.3608\n","Epoch [3/5], Step [116/1072], Loss: 0.3842\n","Epoch [3/5], Step [117/1072], Loss: 0.3592\n","Epoch [3/5], Step [118/1072], Loss: 0.3514\n","Epoch [3/5], Step [119/1072], Loss: 0.3738\n","Epoch [3/5], Step [120/1072], Loss: 0.3802\n","Epoch [3/5], Step [121/1072], Loss: 0.3698\n","Epoch [3/5], Step [122/1072], Loss: 0.3830\n","Epoch [3/5], Step [123/1072], Loss: 0.3603\n","Epoch [3/5], Step [124/1072], Loss: 0.3557\n","Epoch [3/5], Step [125/1072], Loss: 0.3914\n","Epoch [3/5], Step [126/1072], Loss: 0.3563\n","Epoch [3/5], Step [127/1072], Loss: 0.3707\n","Epoch [3/5], Step [128/1072], Loss: 0.3922\n","Epoch [3/5], Step [129/1072], Loss: 0.3725\n","Epoch [3/5], Step [130/1072], Loss: 0.3503\n","Epoch [3/5], Step [131/1072], Loss: 0.3863\n","Epoch [3/5], Step [132/1072], Loss: 0.3677\n","Epoch [3/5], Step [133/1072], Loss: 0.3435\n","Epoch [3/5], Step [134/1072], Loss: 0.3714\n","Epoch [3/5], Step [135/1072], Loss: 0.3744\n","Epoch [3/5], Step [136/1072], Loss: 0.3819\n","Epoch [3/5], Step [137/1072], Loss: 0.3677\n","Epoch [3/5], Step [138/1072], Loss: 0.3748\n","Epoch [3/5], Step [139/1072], Loss: 0.3902\n","Epoch [3/5], Step [140/1072], Loss: 0.3749\n","Epoch [3/5], Step [141/1072], Loss: 0.3597\n","Epoch [3/5], Step [142/1072], Loss: 0.3889\n","Epoch [3/5], Step [143/1072], Loss: 0.3422\n","Epoch [3/5], Step [144/1072], Loss: 0.3801\n","Epoch [3/5], Step [145/1072], Loss: 0.3437\n","Epoch [3/5], Step [146/1072], Loss: 0.3725\n","Epoch [3/5], Step [147/1072], Loss: 0.3553\n","Epoch [3/5], Step [148/1072], Loss: 0.3647\n","Epoch [3/5], Step [149/1072], Loss: 0.3334\n","Epoch [3/5], Step [150/1072], Loss: 0.3536\n","Epoch [3/5], Step [151/1072], Loss: 0.3930\n","Epoch [3/5], Step [152/1072], Loss: 0.3527\n","Epoch [3/5], Step [153/1072], Loss: 0.3517\n","Epoch [3/5], Step [154/1072], Loss: 0.3511\n","Epoch [3/5], Step [155/1072], Loss: 0.3680\n","Epoch [3/5], Step [156/1072], Loss: 0.3615\n","Epoch [3/5], Step [157/1072], Loss: 0.3637\n","Epoch [3/5], Step [158/1072], Loss: 0.3647\n","Epoch [3/5], Step [159/1072], Loss: 0.3713\n","Epoch [3/5], Step [160/1072], Loss: 0.3869\n","Epoch [3/5], Step [161/1072], Loss: 0.3775\n","Epoch [3/5], Step [162/1072], Loss: 0.3810\n","Epoch [3/5], Step [163/1072], Loss: 0.3494\n","Epoch [3/5], Step [164/1072], Loss: 0.3584\n","Epoch [3/5], Step [165/1072], Loss: 0.3791\n","Epoch [3/5], Step [166/1072], Loss: 0.3955\n","Epoch [3/5], Step [167/1072], Loss: 0.3814\n","Epoch [3/5], Step [168/1072], Loss: 0.3793\n","Epoch [3/5], Step [169/1072], Loss: 0.3674\n","Epoch [3/5], Step [170/1072], Loss: 0.3665\n","Epoch [3/5], Step [171/1072], Loss: 0.3449\n","Epoch [3/5], Step [172/1072], Loss: 0.3871\n","Epoch [3/5], Step [173/1072], Loss: 0.3723\n","Epoch [3/5], Step [174/1072], Loss: 0.3776\n","Epoch [3/5], Step [175/1072], Loss: 0.3740\n","Epoch [3/5], Step [176/1072], Loss: 0.3588\n","Epoch [3/5], Step [177/1072], Loss: 0.3797\n","Epoch [3/5], Step [178/1072], Loss: 0.3418\n","Epoch [3/5], Step [179/1072], Loss: 0.3635\n","Epoch [3/5], Step [180/1072], Loss: 0.4010\n","Epoch [3/5], Step [181/1072], Loss: 0.3543\n","Epoch [3/5], Step [182/1072], Loss: 0.3623\n","Epoch [3/5], Step [183/1072], Loss: 0.3769\n","Epoch [3/5], Step [184/1072], Loss: 0.3619\n","Epoch [3/5], Step [185/1072], Loss: 0.3308\n","Epoch [3/5], Step [186/1072], Loss: 0.3562\n","Epoch [3/5], Step [187/1072], Loss: 0.3499\n","Epoch [3/5], Step [188/1072], Loss: 0.3714\n","Epoch [3/5], Step [189/1072], Loss: 0.3695\n","Epoch [3/5], Step [190/1072], Loss: 0.3831\n","Epoch [3/5], Step [191/1072], Loss: 0.3652\n","Epoch [3/5], Step [192/1072], Loss: 0.3455\n","Epoch [3/5], Step [193/1072], Loss: 0.3667\n","Epoch [3/5], Step [194/1072], Loss: 0.3850\n","Epoch [3/5], Step [195/1072], Loss: 0.3977\n","Epoch [3/5], Step [196/1072], Loss: 0.3793\n","Epoch [3/5], Step [197/1072], Loss: 0.3595\n","Epoch [3/5], Step [198/1072], Loss: 0.3731\n","Epoch [3/5], Step [199/1072], Loss: 0.3666\n","Epoch [3/5], Step [200/1072], Loss: 0.3805\n","Epoch [3/5], Step [201/1072], Loss: 0.3517\n","Epoch [3/5], Step [202/1072], Loss: 0.3769\n","Epoch [3/5], Step [203/1072], Loss: 0.3551\n","Epoch [3/5], Step [204/1072], Loss: 0.3593\n","Epoch [3/5], Step [205/1072], Loss: 0.3722\n","Epoch [3/5], Step [206/1072], Loss: 0.3519\n","Epoch [3/5], Step [207/1072], Loss: 0.3644\n","Epoch [3/5], Step [208/1072], Loss: 0.3333\n","Epoch [3/5], Step [209/1072], Loss: 0.3675\n","Epoch [3/5], Step [210/1072], Loss: 0.3519\n","Epoch [3/5], Step [211/1072], Loss: 0.3881\n","Epoch [3/5], Step [212/1072], Loss: 0.4120\n","Epoch [3/5], Step [213/1072], Loss: 0.3642\n","Epoch [3/5], Step [214/1072], Loss: 0.3840\n","Epoch [3/5], Step [215/1072], Loss: 0.3934\n","Epoch [3/5], Step [216/1072], Loss: 0.3514\n","Epoch [3/5], Step [217/1072], Loss: 0.3671\n","Epoch [3/5], Step [218/1072], Loss: 0.3593\n","Epoch [3/5], Step [219/1072], Loss: 0.3692\n","Epoch [3/5], Step [220/1072], Loss: 0.3785\n","Epoch [3/5], Step [221/1072], Loss: 0.3619\n","Epoch [3/5], Step [222/1072], Loss: 0.3737\n","Epoch [3/5], Step [223/1072], Loss: 0.3438\n","Epoch [3/5], Step [224/1072], Loss: 0.3890\n","Epoch [3/5], Step [225/1072], Loss: 0.3294\n","Epoch [3/5], Step [226/1072], Loss: 0.3601\n","Epoch [3/5], Step [227/1072], Loss: 0.3369\n","Epoch [3/5], Step [228/1072], Loss: 0.3626\n","Epoch [3/5], Step [229/1072], Loss: 0.3505\n","Epoch [3/5], Step [230/1072], Loss: 0.3500\n","Epoch [3/5], Step [231/1072], Loss: 0.3479\n","Epoch [3/5], Step [232/1072], Loss: 0.3615\n","Epoch [3/5], Step [233/1072], Loss: 0.3388\n","Epoch [3/5], Step [234/1072], Loss: 0.3634\n","Epoch [3/5], Step [235/1072], Loss: 0.3592\n","Epoch [3/5], Step [236/1072], Loss: 0.3921\n","Epoch [3/5], Step [237/1072], Loss: 0.3894\n","Epoch [3/5], Step [238/1072], Loss: 0.3667\n","Epoch [3/5], Step [239/1072], Loss: 0.3718\n","Epoch [3/5], Step [240/1072], Loss: 0.3362\n","Epoch [3/5], Step [241/1072], Loss: 0.3782\n","Epoch [3/5], Step [242/1072], Loss: 0.3481\n","Epoch [3/5], Step [243/1072], Loss: 0.3534\n","Epoch [3/5], Step [244/1072], Loss: 0.3790\n","Epoch [3/5], Step [245/1072], Loss: 0.3537\n","Epoch [3/5], Step [246/1072], Loss: 0.3413\n","Epoch [3/5], Step [247/1072], Loss: 0.3685\n","Epoch [3/5], Step [248/1072], Loss: 0.3483\n","Epoch [3/5], Step [249/1072], Loss: 0.3634\n","Epoch [3/5], Step [250/1072], Loss: 0.3552\n","Epoch [3/5], Step [251/1072], Loss: 0.3798\n","Epoch [3/5], Step [252/1072], Loss: 0.3529\n","Epoch [3/5], Step [253/1072], Loss: 0.3309\n","Epoch [3/5], Step [254/1072], Loss: 0.3752\n","Epoch [3/5], Step [255/1072], Loss: 0.3725\n","Epoch [3/5], Step [256/1072], Loss: 0.3767\n","Epoch [3/5], Step [257/1072], Loss: 0.3669\n","Epoch [3/5], Step [258/1072], Loss: 0.3735\n","Epoch [3/5], Step [259/1072], Loss: 0.3666\n","Epoch [3/5], Step [260/1072], Loss: 0.3684\n","Epoch [3/5], Step [261/1072], Loss: 0.3833\n","Epoch [3/5], Step [262/1072], Loss: 0.3709\n","Epoch [3/5], Step [263/1072], Loss: 0.3517\n","Epoch [3/5], Step [264/1072], Loss: 0.3472\n","Epoch [3/5], Step [265/1072], Loss: 0.3588\n","Epoch [3/5], Step [266/1072], Loss: 0.3213\n","Epoch [3/5], Step [267/1072], Loss: 0.3716\n","Epoch [3/5], Step [268/1072], Loss: 0.3517\n","Epoch [3/5], Step [269/1072], Loss: 0.3563\n","Epoch [3/5], Step [270/1072], Loss: 0.3678\n","Epoch [3/5], Step [271/1072], Loss: 0.3665\n","Epoch [3/5], Step [272/1072], Loss: 0.3774\n","Epoch [3/5], Step [273/1072], Loss: 0.3558\n","Epoch [3/5], Step [274/1072], Loss: 0.3709\n","Epoch [3/5], Step [275/1072], Loss: 0.3503\n","Epoch [3/5], Step [276/1072], Loss: 0.3793\n","Epoch [3/5], Step [277/1072], Loss: 0.3840\n","Epoch [3/5], Step [278/1072], Loss: 0.3796\n","Epoch [3/5], Step [279/1072], Loss: 0.3672\n","Epoch [3/5], Step [280/1072], Loss: 0.3422\n","Epoch [3/5], Step [281/1072], Loss: 0.3645\n","Epoch [3/5], Step [282/1072], Loss: 0.3900\n","Epoch [3/5], Step [283/1072], Loss: 0.3537\n","Epoch [3/5], Step [284/1072], Loss: 0.3976\n","Epoch [3/5], Step [285/1072], Loss: 0.3821\n","Epoch [3/5], Step [286/1072], Loss: 0.3909\n","Epoch [3/5], Step [287/1072], Loss: 0.3700\n","Epoch [3/5], Step [288/1072], Loss: 0.3578\n","Epoch [3/5], Step [289/1072], Loss: 0.3732\n","Epoch [3/5], Step [290/1072], Loss: 0.3723\n","Epoch [3/5], Step [291/1072], Loss: 0.3657\n","Epoch [3/5], Step [292/1072], Loss: 0.4011\n","Epoch [3/5], Step [293/1072], Loss: 0.3602\n","Epoch [3/5], Step [294/1072], Loss: 0.3689\n","Epoch [3/5], Step [295/1072], Loss: 0.3633\n","Epoch [3/5], Step [296/1072], Loss: 0.3514\n","Epoch [3/5], Step [297/1072], Loss: 0.3640\n","Epoch [3/5], Step [298/1072], Loss: 0.3574\n","Epoch [3/5], Step [299/1072], Loss: 0.3691\n","Epoch [3/5], Step [300/1072], Loss: 0.3424\n","Epoch [3/5], Step [301/1072], Loss: 0.4023\n","Epoch [3/5], Step [302/1072], Loss: 0.3678\n","Epoch [3/5], Step [303/1072], Loss: 0.3781\n","Epoch [3/5], Step [304/1072], Loss: 0.3628\n","Epoch [3/5], Step [305/1072], Loss: 0.3726\n","Epoch [3/5], Step [306/1072], Loss: 0.3890\n","Epoch [3/5], Step [307/1072], Loss: 0.3723\n","Epoch [3/5], Step [308/1072], Loss: 0.3826\n","Epoch [3/5], Step [309/1072], Loss: 0.3285\n","Epoch [3/5], Step [310/1072], Loss: 0.3624\n","Epoch [3/5], Step [311/1072], Loss: 0.3738\n","Epoch [3/5], Step [312/1072], Loss: 0.3805\n","Epoch [3/5], Step [313/1072], Loss: 0.3789\n","Epoch [3/5], Step [314/1072], Loss: 0.3449\n","Epoch [3/5], Step [315/1072], Loss: 0.3535\n","Epoch [3/5], Step [316/1072], Loss: 0.3533\n","Epoch [3/5], Step [317/1072], Loss: 0.3325\n","Epoch [3/5], Step [318/1072], Loss: 0.3511\n","Epoch [3/5], Step [319/1072], Loss: 0.3468\n","Epoch [3/5], Step [320/1072], Loss: 0.3766\n","Epoch [3/5], Step [321/1072], Loss: 0.3378\n","Epoch [3/5], Step [322/1072], Loss: 0.3564\n","Epoch [3/5], Step [323/1072], Loss: 0.3674\n","Epoch [3/5], Step [324/1072], Loss: 0.3321\n","Epoch [3/5], Step [325/1072], Loss: 0.3489\n","Epoch [3/5], Step [326/1072], Loss: 0.3430\n","Epoch [3/5], Step [327/1072], Loss: 0.3842\n","Epoch [3/5], Step [328/1072], Loss: 0.3555\n","Epoch [3/5], Step [329/1072], Loss: 0.3766\n","Epoch [3/5], Step [330/1072], Loss: 0.4106\n","Epoch [3/5], Step [331/1072], Loss: 0.3694\n","Epoch [3/5], Step [332/1072], Loss: 0.3598\n","Epoch [3/5], Step [333/1072], Loss: 0.3550\n","Epoch [3/5], Step [334/1072], Loss: 0.3642\n","Epoch [3/5], Step [335/1072], Loss: 0.3490\n","Epoch [3/5], Step [336/1072], Loss: 0.3583\n","Epoch [3/5], Step [337/1072], Loss: 0.3525\n","Epoch [3/5], Step [338/1072], Loss: 0.3699\n","Epoch [3/5], Step [339/1072], Loss: 0.3680\n","Epoch [3/5], Step [340/1072], Loss: 0.3606\n","Epoch [3/5], Step [341/1072], Loss: 0.3660\n","Epoch [3/5], Step [342/1072], Loss: 0.3459\n","Epoch [3/5], Step [343/1072], Loss: 0.3744\n","Epoch [3/5], Step [344/1072], Loss: 0.3461\n","Epoch [3/5], Step [345/1072], Loss: 0.3790\n","Epoch [3/5], Step [346/1072], Loss: 0.3558\n","Epoch [3/5], Step [347/1072], Loss: 0.3621\n","Epoch [3/5], Step [348/1072], Loss: 0.3677\n","Epoch [3/5], Step [349/1072], Loss: 0.3783\n","Epoch [3/5], Step [350/1072], Loss: 0.3638\n","Epoch [3/5], Step [351/1072], Loss: 0.3475\n","Epoch [3/5], Step [352/1072], Loss: 0.3470\n","Epoch [3/5], Step [353/1072], Loss: 0.3441\n","Epoch [3/5], Step [354/1072], Loss: 0.3778\n","Epoch [3/5], Step [355/1072], Loss: 0.3649\n","Epoch [3/5], Step [356/1072], Loss: 0.3708\n","Epoch [3/5], Step [357/1072], Loss: 0.3792\n","Epoch [3/5], Step [358/1072], Loss: 0.3620\n","Epoch [3/5], Step [359/1072], Loss: 0.3704\n","Epoch [3/5], Step [360/1072], Loss: 0.3533\n","Epoch [3/5], Step [361/1072], Loss: 0.3654\n","Epoch [3/5], Step [362/1072], Loss: 0.3555\n","Epoch [3/5], Step [363/1072], Loss: 0.3703\n","Epoch [3/5], Step [364/1072], Loss: 0.3692\n","Epoch [3/5], Step [365/1072], Loss: 0.3713\n","Epoch [3/5], Step [366/1072], Loss: 0.3668\n","Epoch [3/5], Step [367/1072], Loss: 0.3708\n","Epoch [3/5], Step [368/1072], Loss: 0.3687\n","Epoch [3/5], Step [369/1072], Loss: 0.3911\n","Epoch [3/5], Step [370/1072], Loss: 0.3607\n","Epoch [3/5], Step [371/1072], Loss: 0.3610\n","Epoch [3/5], Step [372/1072], Loss: 0.3573\n","Epoch [3/5], Step [373/1072], Loss: 0.3654\n","Epoch [3/5], Step [374/1072], Loss: 0.3758\n","Epoch [3/5], Step [375/1072], Loss: 0.3423\n","Epoch [3/5], Step [376/1072], Loss: 0.3630\n","Epoch [3/5], Step [377/1072], Loss: 0.3634\n","Epoch [3/5], Step [378/1072], Loss: 0.3570\n","Epoch [3/5], Step [379/1072], Loss: 0.3763\n","Epoch [3/5], Step [380/1072], Loss: 0.3873\n","Epoch [3/5], Step [381/1072], Loss: 0.3583\n","Epoch [3/5], Step [382/1072], Loss: 0.3305\n","Epoch [3/5], Step [383/1072], Loss: 0.3376\n","Epoch [3/5], Step [384/1072], Loss: 0.3621\n","Epoch [3/5], Step [385/1072], Loss: 0.3500\n","Epoch [3/5], Step [386/1072], Loss: 0.3651\n","Epoch [3/5], Step [387/1072], Loss: 0.3450\n","Epoch [3/5], Step [388/1072], Loss: 0.3801\n","Epoch [3/5], Step [389/1072], Loss: 0.3878\n","Epoch [3/5], Step [390/1072], Loss: 0.3770\n","Epoch [3/5], Step [391/1072], Loss: 0.3602\n","Epoch [3/5], Step [392/1072], Loss: 0.3403\n","Epoch [3/5], Step [393/1072], Loss: 0.3701\n","Epoch [3/5], Step [394/1072], Loss: 0.3760\n","Epoch [3/5], Step [395/1072], Loss: 0.3495\n","Epoch [3/5], Step [396/1072], Loss: 0.3843\n","Epoch [3/5], Step [397/1072], Loss: 0.3814\n","Epoch [3/5], Step [398/1072], Loss: 0.3889\n","Epoch [3/5], Step [399/1072], Loss: 0.3735\n","Epoch [3/5], Step [400/1072], Loss: 0.3508\n","Epoch [3/5], Step [401/1072], Loss: 0.3463\n","Epoch [3/5], Step [402/1072], Loss: 0.3531\n","Epoch [3/5], Step [403/1072], Loss: 0.3915\n","Epoch [3/5], Step [404/1072], Loss: 0.3790\n","Epoch [3/5], Step [405/1072], Loss: 0.3511\n","Epoch [3/5], Step [406/1072], Loss: 0.3577\n","Epoch [3/5], Step [407/1072], Loss: 0.3642\n","Epoch [3/5], Step [408/1072], Loss: 0.3682\n","Epoch [3/5], Step [409/1072], Loss: 0.3484\n","Epoch [3/5], Step [410/1072], Loss: 0.3684\n","Epoch [3/5], Step [411/1072], Loss: 0.3645\n","Epoch [3/5], Step [412/1072], Loss: 0.3636\n","Epoch [3/5], Step [413/1072], Loss: 0.3635\n","Epoch [3/5], Step [414/1072], Loss: 0.4031\n","Epoch [3/5], Step [415/1072], Loss: 0.3794\n","Epoch [3/5], Step [416/1072], Loss: 0.4028\n","Epoch [3/5], Step [417/1072], Loss: 0.4024\n","Epoch [3/5], Step [418/1072], Loss: 0.3409\n","Epoch [3/5], Step [419/1072], Loss: 0.3752\n","Epoch [3/5], Step [420/1072], Loss: 0.3455\n","Epoch [3/5], Step [421/1072], Loss: 0.3748\n","Epoch [3/5], Step [422/1072], Loss: 0.3438\n","Epoch [3/5], Step [423/1072], Loss: 0.3550\n","Epoch [3/5], Step [424/1072], Loss: 0.3680\n","Epoch [3/5], Step [425/1072], Loss: 0.3198\n","Epoch [3/5], Step [426/1072], Loss: 0.3557\n","Epoch [3/5], Step [427/1072], Loss: 0.3548\n","Epoch [3/5], Step [428/1072], Loss: 0.3942\n","Epoch [3/5], Step [429/1072], Loss: 0.3555\n","Epoch [3/5], Step [430/1072], Loss: 0.3627\n","Epoch [3/5], Step [431/1072], Loss: 0.3530\n","Epoch [3/5], Step [432/1072], Loss: 0.3730\n","Epoch [3/5], Step [433/1072], Loss: 0.3562\n","Epoch [3/5], Step [434/1072], Loss: 0.3439\n","Epoch [3/5], Step [435/1072], Loss: 0.3619\n","Epoch [3/5], Step [436/1072], Loss: 0.3745\n","Epoch [3/5], Step [437/1072], Loss: 0.3622\n","Epoch [3/5], Step [438/1072], Loss: 0.3345\n","Epoch [3/5], Step [439/1072], Loss: 0.3765\n","Epoch [3/5], Step [440/1072], Loss: 0.3537\n","Epoch [3/5], Step [441/1072], Loss: 0.3520\n","Epoch [3/5], Step [442/1072], Loss: 0.3847\n","Epoch [3/5], Step [443/1072], Loss: 0.3493\n","Epoch [3/5], Step [444/1072], Loss: 0.3307\n","Epoch [3/5], Step [445/1072], Loss: 0.3672\n","Epoch [3/5], Step [446/1072], Loss: 0.3506\n","Epoch [3/5], Step [447/1072], Loss: 0.3809\n","Epoch [3/5], Step [448/1072], Loss: 0.3861\n","Epoch [3/5], Step [449/1072], Loss: 0.3612\n","Epoch [3/5], Step [450/1072], Loss: 0.3775\n","Epoch [3/5], Step [451/1072], Loss: 0.3775\n","Epoch [3/5], Step [452/1072], Loss: 0.3381\n","Epoch [3/5], Step [453/1072], Loss: 0.3676\n","Epoch [3/5], Step [454/1072], Loss: 0.3904\n","Epoch [3/5], Step [455/1072], Loss: 0.3525\n","Epoch [3/5], Step [456/1072], Loss: 0.3501\n","Epoch [3/5], Step [457/1072], Loss: 0.3711\n","Epoch [3/5], Step [458/1072], Loss: 0.3743\n","Epoch [3/5], Step [459/1072], Loss: 0.3577\n","Epoch [3/5], Step [460/1072], Loss: 0.3648\n","Epoch [3/5], Step [461/1072], Loss: 0.3570\n","Epoch [3/5], Step [462/1072], Loss: 0.3693\n","Epoch [3/5], Step [463/1072], Loss: 0.3629\n","Epoch [3/5], Step [464/1072], Loss: 0.3416\n","Epoch [3/5], Step [465/1072], Loss: 0.3505\n","Epoch [3/5], Step [466/1072], Loss: 0.3689\n","Epoch [3/5], Step [467/1072], Loss: 0.3557\n","Epoch [3/5], Step [468/1072], Loss: 0.3705\n","Epoch [3/5], Step [469/1072], Loss: 0.3679\n","Epoch [3/5], Step [470/1072], Loss: 0.3731\n","Epoch [3/5], Step [471/1072], Loss: 0.3565\n","Epoch [3/5], Step [472/1072], Loss: 0.3712\n","Epoch [3/5], Step [473/1072], Loss: 0.3653\n","Epoch [3/5], Step [474/1072], Loss: 0.3805\n","Epoch [3/5], Step [475/1072], Loss: 0.3724\n","Epoch [3/5], Step [476/1072], Loss: 0.3631\n","Epoch [3/5], Step [477/1072], Loss: 0.3597\n","Epoch [3/5], Step [478/1072], Loss: 0.4046\n","Epoch [3/5], Step [479/1072], Loss: 0.3724\n","Epoch [3/5], Step [480/1072], Loss: 0.3685\n","Epoch [3/5], Step [481/1072], Loss: 0.3850\n","Epoch [3/5], Step [482/1072], Loss: 0.3581\n","Epoch [3/5], Step [483/1072], Loss: 0.3728\n","Epoch [3/5], Step [484/1072], Loss: 0.3671\n","Epoch [3/5], Step [485/1072], Loss: 0.3765\n","Epoch [3/5], Step [486/1072], Loss: 0.3754\n","Epoch [3/5], Step [487/1072], Loss: 0.3994\n","Epoch [3/5], Step [488/1072], Loss: 0.3590\n","Epoch [3/5], Step [489/1072], Loss: 0.3969\n","Epoch [3/5], Step [490/1072], Loss: 0.3742\n","Epoch [3/5], Step [491/1072], Loss: 0.4029\n","Epoch [3/5], Step [492/1072], Loss: 0.3725\n","Epoch [3/5], Step [493/1072], Loss: 0.3744\n","Epoch [3/5], Step [494/1072], Loss: 0.3710\n","Epoch [3/5], Step [495/1072], Loss: 0.3342\n","Epoch [3/5], Step [496/1072], Loss: 0.3703\n","Epoch [3/5], Step [497/1072], Loss: 0.3690\n","Epoch [3/5], Step [498/1072], Loss: 0.3502\n","Epoch [3/5], Step [499/1072], Loss: 0.3864\n","Epoch [3/5], Step [500/1072], Loss: 0.3707\n","Epoch [3/5], Step [501/1072], Loss: 0.3826\n","Epoch [3/5], Step [502/1072], Loss: 0.3479\n","Epoch [3/5], Step [503/1072], Loss: 0.3437\n","Epoch [3/5], Step [504/1072], Loss: 0.3516\n","Epoch [3/5], Step [505/1072], Loss: 0.3802\n","Epoch [3/5], Step [506/1072], Loss: 0.3456\n","Epoch [3/5], Step [507/1072], Loss: 0.3515\n","Epoch [3/5], Step [508/1072], Loss: 0.3746\n","Epoch [3/5], Step [509/1072], Loss: 0.3767\n","Epoch [3/5], Step [510/1072], Loss: 0.3512\n","Epoch [3/5], Step [511/1072], Loss: 0.3613\n","Epoch [3/5], Step [512/1072], Loss: 0.3549\n","Epoch [3/5], Step [513/1072], Loss: 0.3629\n","Epoch [3/5], Step [514/1072], Loss: 0.3586\n","Epoch [3/5], Step [515/1072], Loss: 0.3668\n","Epoch [3/5], Step [516/1072], Loss: 0.3638\n","Epoch [3/5], Step [517/1072], Loss: 0.3691\n","Epoch [3/5], Step [518/1072], Loss: 0.3552\n","Epoch [3/5], Step [519/1072], Loss: 0.3646\n","Epoch [3/5], Step [520/1072], Loss: 0.3480\n","Epoch [3/5], Step [521/1072], Loss: 0.3543\n","Epoch [3/5], Step [522/1072], Loss: 0.3480\n","Epoch [3/5], Step [523/1072], Loss: 0.3713\n","Epoch [3/5], Step [524/1072], Loss: 0.3635\n","Epoch [3/5], Step [525/1072], Loss: 0.3731\n","Epoch [3/5], Step [526/1072], Loss: 0.3837\n","Epoch [3/5], Step [527/1072], Loss: 0.3576\n","Epoch [3/5], Step [528/1072], Loss: 0.3521\n","Epoch [3/5], Step [529/1072], Loss: 0.3757\n","Epoch [3/5], Step [530/1072], Loss: 0.3991\n","Epoch [3/5], Step [531/1072], Loss: 0.4121\n","Epoch [3/5], Step [532/1072], Loss: 0.3636\n","Epoch [3/5], Step [533/1072], Loss: 0.3886\n","Epoch [3/5], Step [534/1072], Loss: 0.3566\n","Epoch [3/5], Step [535/1072], Loss: 0.3690\n","Epoch [3/5], Step [536/1072], Loss: 0.3569\n","Epoch [3/5], Step [537/1072], Loss: 0.3612\n","Epoch [3/5], Step [538/1072], Loss: 0.3898\n","Epoch [3/5], Step [539/1072], Loss: 0.3640\n","Epoch [3/5], Step [540/1072], Loss: 0.3723\n","Epoch [3/5], Step [541/1072], Loss: 0.3289\n","Epoch [3/5], Step [542/1072], Loss: 0.3291\n","Epoch [3/5], Step [543/1072], Loss: 0.3318\n","Epoch [3/5], Step [544/1072], Loss: 0.3342\n","Epoch [3/5], Step [545/1072], Loss: 0.3624\n","Epoch [3/5], Step [546/1072], Loss: 0.3432\n","Epoch [3/5], Step [547/1072], Loss: 0.3644\n","Epoch [3/5], Step [548/1072], Loss: 0.3847\n","Epoch [3/5], Step [549/1072], Loss: 0.3679\n","Epoch [3/5], Step [550/1072], Loss: 0.3571\n","Epoch [3/5], Step [551/1072], Loss: 0.3704\n","Epoch [3/5], Step [552/1072], Loss: 0.3760\n","Epoch [3/5], Step [553/1072], Loss: 0.3814\n","Epoch [3/5], Step [554/1072], Loss: 0.3961\n","Epoch [3/5], Step [555/1072], Loss: 0.3426\n","Epoch [3/5], Step [556/1072], Loss: 0.3889\n","Epoch [3/5], Step [557/1072], Loss: 0.3716\n","Epoch [3/5], Step [558/1072], Loss: 0.3968\n","Epoch [3/5], Step [559/1072], Loss: 0.3660\n","Epoch [3/5], Step [560/1072], Loss: 0.3693\n","Epoch [3/5], Step [561/1072], Loss: 0.3686\n","Epoch [3/5], Step [562/1072], Loss: 0.3614\n","Epoch [3/5], Step [563/1072], Loss: 0.3565\n","Epoch [3/5], Step [564/1072], Loss: 0.3884\n","Epoch [3/5], Step [565/1072], Loss: 0.3665\n","Epoch [3/5], Step [566/1072], Loss: 0.3577\n","Epoch [3/5], Step [567/1072], Loss: 0.3618\n","Epoch [3/5], Step [568/1072], Loss: 0.3925\n","Epoch [3/5], Step [569/1072], Loss: 0.3715\n","Epoch [3/5], Step [570/1072], Loss: 0.3378\n","Epoch [3/5], Step [571/1072], Loss: 0.3861\n","Epoch [3/5], Step [572/1072], Loss: 0.3813\n","Epoch [3/5], Step [573/1072], Loss: 0.3687\n","Epoch [3/5], Step [574/1072], Loss: 0.3556\n","Epoch [3/5], Step [575/1072], Loss: 0.3991\n","Epoch [3/5], Step [576/1072], Loss: 0.3442\n","Epoch [3/5], Step [577/1072], Loss: 0.3696\n","Epoch [3/5], Step [578/1072], Loss: 0.3481\n","Epoch [3/5], Step [579/1072], Loss: 0.3803\n","Epoch [3/5], Step [580/1072], Loss: 0.3646\n","Epoch [3/5], Step [581/1072], Loss: 0.3697\n","Epoch [3/5], Step [582/1072], Loss: 0.3829\n","Epoch [3/5], Step [583/1072], Loss: 0.3655\n","Epoch [3/5], Step [584/1072], Loss: 0.3498\n","Epoch [3/5], Step [585/1072], Loss: 0.3697\n","Epoch [3/5], Step [586/1072], Loss: 0.3440\n","Epoch [3/5], Step [587/1072], Loss: 0.3276\n","Epoch [3/5], Step [588/1072], Loss: 0.3826\n","Epoch [3/5], Step [589/1072], Loss: 0.3940\n","Epoch [3/5], Step [590/1072], Loss: 0.3846\n","Epoch [3/5], Step [591/1072], Loss: 0.3537\n","Epoch [3/5], Step [592/1072], Loss: 0.3769\n","Epoch [3/5], Step [593/1072], Loss: 0.3537\n","Epoch [3/5], Step [594/1072], Loss: 0.3838\n","Epoch [3/5], Step [595/1072], Loss: 0.3475\n","Epoch [3/5], Step [596/1072], Loss: 0.3774\n","Epoch [3/5], Step [597/1072], Loss: 0.3837\n","Epoch [3/5], Step [598/1072], Loss: 0.3642\n","Epoch [3/5], Step [599/1072], Loss: 0.3496\n","Epoch [3/5], Step [600/1072], Loss: 0.3785\n","Epoch [3/5], Step [601/1072], Loss: 0.3548\n","Epoch [3/5], Step [602/1072], Loss: 0.3979\n","Epoch [3/5], Step [603/1072], Loss: 0.3629\n","Epoch [3/5], Step [604/1072], Loss: 0.3580\n","Epoch [3/5], Step [605/1072], Loss: 0.3738\n","Epoch [3/5], Step [606/1072], Loss: 0.3804\n","Epoch [3/5], Step [607/1072], Loss: 0.3683\n","Epoch [3/5], Step [608/1072], Loss: 0.3387\n","Epoch [3/5], Step [609/1072], Loss: 0.3754\n","Epoch [3/5], Step [610/1072], Loss: 0.3648\n","Epoch [3/5], Step [611/1072], Loss: 0.3387\n","Epoch [3/5], Step [612/1072], Loss: 0.3943\n","Epoch [3/5], Step [613/1072], Loss: 0.3738\n","Epoch [3/5], Step [614/1072], Loss: 0.3673\n","Epoch [3/5], Step [615/1072], Loss: 0.3664\n","Epoch [3/5], Step [616/1072], Loss: 0.3667\n","Epoch [3/5], Step [617/1072], Loss: 0.3686\n","Epoch [3/5], Step [618/1072], Loss: 0.4005\n","Epoch [3/5], Step [619/1072], Loss: 0.3678\n","Epoch [3/5], Step [620/1072], Loss: 0.3620\n","Epoch [3/5], Step [621/1072], Loss: 0.3558\n","Epoch [3/5], Step [622/1072], Loss: 0.3780\n","Epoch [3/5], Step [623/1072], Loss: 0.3466\n","Epoch [3/5], Step [624/1072], Loss: 0.3588\n","Epoch [3/5], Step [625/1072], Loss: 0.3570\n","Epoch [3/5], Step [626/1072], Loss: 0.3771\n","Epoch [3/5], Step [627/1072], Loss: 0.3539\n","Epoch [3/5], Step [628/1072], Loss: 0.3849\n","Epoch [3/5], Step [629/1072], Loss: 0.3703\n","Epoch [3/5], Step [630/1072], Loss: 0.3623\n","Epoch [3/5], Step [631/1072], Loss: 0.3696\n","Epoch [3/5], Step [632/1072], Loss: 0.3590\n","Epoch [3/5], Step [633/1072], Loss: 0.3368\n","Epoch [3/5], Step [634/1072], Loss: 0.3708\n","Epoch [3/5], Step [635/1072], Loss: 0.3941\n","Epoch [3/5], Step [636/1072], Loss: 0.3594\n","Epoch [3/5], Step [637/1072], Loss: 0.3530\n","Epoch [3/5], Step [638/1072], Loss: 0.3606\n","Epoch [3/5], Step [639/1072], Loss: 0.4008\n","Epoch [3/5], Step [640/1072], Loss: 0.3623\n","Epoch [3/5], Step [641/1072], Loss: 0.3705\n","Epoch [3/5], Step [642/1072], Loss: 0.3637\n","Epoch [3/5], Step [643/1072], Loss: 0.3873\n","Epoch [3/5], Step [644/1072], Loss: 0.3607\n","Epoch [3/5], Step [645/1072], Loss: 0.3898\n","Epoch [3/5], Step [646/1072], Loss: 0.3504\n","Epoch [3/5], Step [647/1072], Loss: 0.3789\n","Epoch [3/5], Step [648/1072], Loss: 0.3790\n","Epoch [3/5], Step [649/1072], Loss: 0.3627\n","Epoch [3/5], Step [650/1072], Loss: 0.3703\n","Epoch [3/5], Step [651/1072], Loss: 0.3533\n","Epoch [3/5], Step [652/1072], Loss: 0.3532\n","Epoch [3/5], Step [653/1072], Loss: 0.3419\n","Epoch [3/5], Step [654/1072], Loss: 0.3404\n","Epoch [3/5], Step [655/1072], Loss: 0.3720\n","Epoch [3/5], Step [656/1072], Loss: 0.3424\n","Epoch [3/5], Step [657/1072], Loss: 0.3560\n","Epoch [3/5], Step [658/1072], Loss: 0.3758\n","Epoch [3/5], Step [659/1072], Loss: 0.3585\n","Epoch [3/5], Step [660/1072], Loss: 0.3735\n","Epoch [3/5], Step [661/1072], Loss: 0.3490\n","Epoch [3/5], Step [662/1072], Loss: 0.3872\n","Epoch [3/5], Step [663/1072], Loss: 0.3562\n","Epoch [3/5], Step [664/1072], Loss: 0.3670\n","Epoch [3/5], Step [665/1072], Loss: 0.3468\n","Epoch [3/5], Step [666/1072], Loss: 0.3656\n","Epoch [3/5], Step [667/1072], Loss: 0.3552\n","Epoch [3/5], Step [668/1072], Loss: 0.3953\n","Epoch [3/5], Step [669/1072], Loss: 0.3386\n","Epoch [3/5], Step [670/1072], Loss: 0.3714\n","Epoch [3/5], Step [671/1072], Loss: 0.3875\n","Epoch [3/5], Step [672/1072], Loss: 0.3583\n","Epoch [3/5], Step [673/1072], Loss: 0.3714\n","Epoch [3/5], Step [674/1072], Loss: 0.3798\n","Epoch [3/5], Step [675/1072], Loss: 0.3728\n","Epoch [3/5], Step [676/1072], Loss: 0.3502\n","Epoch [3/5], Step [677/1072], Loss: 0.3912\n","Epoch [3/5], Step [678/1072], Loss: 0.3792\n","Epoch [3/5], Step [679/1072], Loss: 0.3653\n","Epoch [3/5], Step [680/1072], Loss: 0.3761\n","Epoch [3/5], Step [681/1072], Loss: 0.3764\n","Epoch [3/5], Step [682/1072], Loss: 0.3734\n","Epoch [3/5], Step [683/1072], Loss: 0.3684\n","Epoch [3/5], Step [684/1072], Loss: 0.3791\n","Epoch [3/5], Step [685/1072], Loss: 0.3616\n","Epoch [3/5], Step [686/1072], Loss: 0.3727\n","Epoch [3/5], Step [687/1072], Loss: 0.3730\n","Epoch [3/5], Step [688/1072], Loss: 0.3540\n","Epoch [3/5], Step [689/1072], Loss: 0.3683\n","Epoch [3/5], Step [690/1072], Loss: 0.3550\n","Epoch [3/5], Step [691/1072], Loss: 0.3612\n","Epoch [3/5], Step [692/1072], Loss: 0.3410\n","Epoch [3/5], Step [693/1072], Loss: 0.3824\n","Epoch [3/5], Step [694/1072], Loss: 0.3770\n","Epoch [3/5], Step [695/1072], Loss: 0.3474\n","Epoch [3/5], Step [696/1072], Loss: 0.3475\n","Epoch [3/5], Step [697/1072], Loss: 0.3569\n","Epoch [3/5], Step [698/1072], Loss: 0.3539\n","Epoch [3/5], Step [699/1072], Loss: 0.3786\n","Epoch [3/5], Step [700/1072], Loss: 0.3726\n","Epoch [3/5], Step [701/1072], Loss: 0.3636\n","Epoch [3/5], Step [702/1072], Loss: 0.3526\n","Epoch [3/5], Step [703/1072], Loss: 0.3731\n","Epoch [3/5], Step [704/1072], Loss: 0.3574\n","Epoch [3/5], Step [705/1072], Loss: 0.3657\n","Epoch [3/5], Step [706/1072], Loss: 0.3698\n","Epoch [3/5], Step [707/1072], Loss: 0.3908\n","Epoch [3/5], Step [708/1072], Loss: 0.3648\n","Epoch [3/5], Step [709/1072], Loss: 0.3539\n","Epoch [3/5], Step [710/1072], Loss: 0.3834\n","Epoch [3/5], Step [711/1072], Loss: 0.3692\n","Epoch [3/5], Step [712/1072], Loss: 0.3585\n","Epoch [3/5], Step [713/1072], Loss: 0.3610\n","Epoch [3/5], Step [714/1072], Loss: 0.3670\n","Epoch [3/5], Step [715/1072], Loss: 0.3832\n","Epoch [3/5], Step [716/1072], Loss: 0.3884\n","Epoch [3/5], Step [717/1072], Loss: 0.3700\n","Epoch [3/5], Step [718/1072], Loss: 0.3690\n","Epoch [3/5], Step [719/1072], Loss: 0.3470\n","Epoch [3/5], Step [720/1072], Loss: 0.3500\n","Epoch [3/5], Step [721/1072], Loss: 0.3537\n","Epoch [3/5], Step [722/1072], Loss: 0.3508\n","Epoch [3/5], Step [723/1072], Loss: 0.3973\n","Epoch [3/5], Step [724/1072], Loss: 0.3780\n","Epoch [3/5], Step [725/1072], Loss: 0.3731\n","Epoch [3/5], Step [726/1072], Loss: 0.3678\n","Epoch [3/5], Step [727/1072], Loss: 0.3446\n","Epoch [3/5], Step [728/1072], Loss: 0.3817\n","Epoch [3/5], Step [729/1072], Loss: 0.3707\n","Epoch [3/5], Step [730/1072], Loss: 0.3358\n","Epoch [3/5], Step [731/1072], Loss: 0.3741\n","Epoch [3/5], Step [732/1072], Loss: 0.3473\n","Epoch [3/5], Step [733/1072], Loss: 0.3501\n","Epoch [3/5], Step [734/1072], Loss: 0.3963\n","Epoch [3/5], Step [735/1072], Loss: 0.3763\n","Epoch [3/5], Step [736/1072], Loss: 0.3793\n","Epoch [3/5], Step [737/1072], Loss: 0.3604\n","Epoch [3/5], Step [738/1072], Loss: 0.3986\n","Epoch [3/5], Step [739/1072], Loss: 0.3535\n","Epoch [3/5], Step [740/1072], Loss: 0.3506\n","Epoch [3/5], Step [741/1072], Loss: 0.3863\n","Epoch [3/5], Step [742/1072], Loss: 0.3739\n","Epoch [3/5], Step [743/1072], Loss: 0.3628\n","Epoch [3/5], Step [744/1072], Loss: 0.3666\n","Epoch [3/5], Step [745/1072], Loss: 0.3556\n","Epoch [3/5], Step [746/1072], Loss: 0.3680\n","Epoch [3/5], Step [747/1072], Loss: 0.3716\n","Epoch [3/5], Step [748/1072], Loss: 0.3610\n","Epoch [3/5], Step [749/1072], Loss: 0.3531\n","Epoch [3/5], Step [750/1072], Loss: 0.3481\n","Epoch [3/5], Step [751/1072], Loss: 0.3787\n","Epoch [3/5], Step [752/1072], Loss: 0.3816\n","Epoch [3/5], Step [753/1072], Loss: 0.3823\n","Epoch [3/5], Step [754/1072], Loss: 0.3750\n","Epoch [3/5], Step [755/1072], Loss: 0.3531\n","Epoch [3/5], Step [756/1072], Loss: 0.3778\n","Epoch [3/5], Step [757/1072], Loss: 0.3605\n","Epoch [3/5], Step [758/1072], Loss: 0.3844\n","Epoch [3/5], Step [759/1072], Loss: 0.3782\n","Epoch [3/5], Step [760/1072], Loss: 0.3594\n","Epoch [3/5], Step [761/1072], Loss: 0.3757\n","Epoch [3/5], Step [762/1072], Loss: 0.3731\n","Epoch [3/5], Step [763/1072], Loss: 0.3921\n","Epoch [3/5], Step [764/1072], Loss: 0.4064\n","Epoch [3/5], Step [765/1072], Loss: 0.3661\n","Epoch [3/5], Step [766/1072], Loss: 0.3411\n","Epoch [3/5], Step [767/1072], Loss: 0.3701\n","Epoch [3/5], Step [768/1072], Loss: 0.3622\n","Epoch [3/5], Step [769/1072], Loss: 0.3361\n","Epoch [3/5], Step [770/1072], Loss: 0.3622\n","Epoch [3/5], Step [771/1072], Loss: 0.4069\n","Epoch [3/5], Step [772/1072], Loss: 0.3681\n","Epoch [3/5], Step [773/1072], Loss: 0.3552\n","Epoch [3/5], Step [774/1072], Loss: 0.3679\n","Epoch [3/5], Step [775/1072], Loss: 0.3784\n","Epoch [3/5], Step [776/1072], Loss: 0.3458\n","Epoch [3/5], Step [777/1072], Loss: 0.3653\n","Epoch [3/5], Step [778/1072], Loss: 0.3662\n","Epoch [3/5], Step [779/1072], Loss: 0.3799\n","Epoch [3/5], Step [780/1072], Loss: 0.3527\n","Epoch [3/5], Step [781/1072], Loss: 0.4148\n","Epoch [3/5], Step [782/1072], Loss: 0.3592\n","Epoch [3/5], Step [783/1072], Loss: 0.3879\n","Epoch [3/5], Step [784/1072], Loss: 0.3658\n","Epoch [3/5], Step [785/1072], Loss: 0.4151\n","Epoch [3/5], Step [786/1072], Loss: 0.3443\n","Epoch [3/5], Step [787/1072], Loss: 0.3762\n","Epoch [3/5], Step [788/1072], Loss: 0.3530\n","Epoch [3/5], Step [789/1072], Loss: 0.3686\n","Epoch [3/5], Step [790/1072], Loss: 0.3656\n","Epoch [3/5], Step [791/1072], Loss: 0.3929\n","Epoch [3/5], Step [792/1072], Loss: 0.3508\n","Epoch [3/5], Step [793/1072], Loss: 0.3633\n","Epoch [3/5], Step [794/1072], Loss: 0.3464\n","Epoch [3/5], Step [795/1072], Loss: 0.3737\n","Epoch [3/5], Step [796/1072], Loss: 0.3412\n","Epoch [3/5], Step [797/1072], Loss: 0.3739\n","Epoch [3/5], Step [798/1072], Loss: 0.3660\n","Epoch [3/5], Step [799/1072], Loss: 0.3533\n","Epoch [3/5], Step [800/1072], Loss: 0.3787\n","Epoch [3/5], Step [801/1072], Loss: 0.3747\n","Epoch [3/5], Step [802/1072], Loss: 0.3466\n","Epoch [3/5], Step [803/1072], Loss: 0.3499\n","Epoch [3/5], Step [804/1072], Loss: 0.3624\n","Epoch [3/5], Step [805/1072], Loss: 0.3917\n","Epoch [3/5], Step [806/1072], Loss: 0.3562\n","Epoch [3/5], Step [807/1072], Loss: 0.3612\n","Epoch [3/5], Step [808/1072], Loss: 0.3825\n","Epoch [3/5], Step [809/1072], Loss: 0.3631\n","Epoch [3/5], Step [810/1072], Loss: 0.3303\n","Epoch [3/5], Step [811/1072], Loss: 0.3614\n","Epoch [3/5], Step [812/1072], Loss: 0.3651\n","Epoch [3/5], Step [813/1072], Loss: 0.3583\n","Epoch [3/5], Step [814/1072], Loss: 0.3659\n","Epoch [3/5], Step [815/1072], Loss: 0.3494\n","Epoch [3/5], Step [816/1072], Loss: 0.3757\n","Epoch [3/5], Step [817/1072], Loss: 0.3705\n","Epoch [3/5], Step [818/1072], Loss: 0.3632\n","Epoch [3/5], Step [819/1072], Loss: 0.3621\n","Epoch [3/5], Step [820/1072], Loss: 0.3604\n","Epoch [3/5], Step [821/1072], Loss: 0.3488\n","Epoch [3/5], Step [822/1072], Loss: 0.3665\n","Epoch [3/5], Step [823/1072], Loss: 0.3718\n","Epoch [3/5], Step [824/1072], Loss: 0.3293\n","Epoch [3/5], Step [825/1072], Loss: 0.3726\n","Epoch [3/5], Step [826/1072], Loss: 0.3294\n","Epoch [3/5], Step [827/1072], Loss: 0.3725\n","Epoch [3/5], Step [828/1072], Loss: 0.3592\n","Epoch [3/5], Step [829/1072], Loss: 0.3540\n","Epoch [3/5], Step [830/1072], Loss: 0.3653\n","Epoch [3/5], Step [831/1072], Loss: 0.3474\n","Epoch [3/5], Step [832/1072], Loss: 0.3686\n","Epoch [3/5], Step [833/1072], Loss: 0.3546\n","Epoch [3/5], Step [834/1072], Loss: 0.3723\n","Epoch [3/5], Step [835/1072], Loss: 0.3527\n","Epoch [3/5], Step [836/1072], Loss: 0.3918\n","Epoch [3/5], Step [837/1072], Loss: 0.3656\n","Epoch [3/5], Step [838/1072], Loss: 0.3514\n","Epoch [3/5], Step [839/1072], Loss: 0.3593\n","Epoch [3/5], Step [840/1072], Loss: 0.3662\n","Epoch [3/5], Step [841/1072], Loss: 0.3446\n","Epoch [3/5], Step [842/1072], Loss: 0.3673\n","Epoch [3/5], Step [843/1072], Loss: 0.3453\n","Epoch [3/5], Step [844/1072], Loss: 0.3738\n","Epoch [3/5], Step [845/1072], Loss: 0.3297\n","Epoch [3/5], Step [846/1072], Loss: 0.3691\n","Epoch [3/5], Step [847/1072], Loss: 0.3419\n","Epoch [3/5], Step [848/1072], Loss: 0.3831\n","Epoch [3/5], Step [849/1072], Loss: 0.3835\n","Epoch [3/5], Step [850/1072], Loss: 0.3795\n","Epoch [3/5], Step [851/1072], Loss: 0.3637\n","Epoch [3/5], Step [852/1072], Loss: 0.3727\n","Epoch [3/5], Step [853/1072], Loss: 0.3789\n","Epoch [3/5], Step [854/1072], Loss: 0.3557\n","Epoch [3/5], Step [855/1072], Loss: 0.3872\n","Epoch [3/5], Step [856/1072], Loss: 0.3793\n","Epoch [3/5], Step [857/1072], Loss: 0.3592\n","Epoch [3/5], Step [858/1072], Loss: 0.3656\n","Epoch [3/5], Step [859/1072], Loss: 0.3648\n","Epoch [3/5], Step [860/1072], Loss: 0.3685\n","Epoch [3/5], Step [861/1072], Loss: 0.3713\n","Epoch [3/5], Step [862/1072], Loss: 0.3552\n","Epoch [3/5], Step [863/1072], Loss: 0.3624\n","Epoch [3/5], Step [864/1072], Loss: 0.3582\n","Epoch [3/5], Step [865/1072], Loss: 0.3463\n","Epoch [3/5], Step [866/1072], Loss: 0.3747\n","Epoch [3/5], Step [867/1072], Loss: 0.3661\n","Epoch [3/5], Step [868/1072], Loss: 0.4097\n","Epoch [3/5], Step [869/1072], Loss: 0.3713\n","Epoch [3/5], Step [870/1072], Loss: 0.3718\n","Epoch [3/5], Step [871/1072], Loss: 0.3701\n","Epoch [3/5], Step [872/1072], Loss: 0.3662\n","Epoch [3/5], Step [873/1072], Loss: 0.3391\n","Epoch [3/5], Step [874/1072], Loss: 0.3532\n","Epoch [3/5], Step [875/1072], Loss: 0.3477\n","Epoch [3/5], Step [876/1072], Loss: 0.3370\n","Epoch [3/5], Step [877/1072], Loss: 0.3665\n","Epoch [3/5], Step [878/1072], Loss: 0.3609\n","Epoch [3/5], Step [879/1072], Loss: 0.3384\n","Epoch [3/5], Step [880/1072], Loss: 0.3072\n","Epoch [3/5], Step [881/1072], Loss: 0.3596\n","Epoch [3/5], Step [882/1072], Loss: 0.3633\n","Epoch [3/5], Step [883/1072], Loss: 0.3634\n","Epoch [3/5], Step [884/1072], Loss: 0.3617\n","Epoch [3/5], Step [885/1072], Loss: 0.3318\n","Epoch [3/5], Step [886/1072], Loss: 0.3575\n","Epoch [3/5], Step [887/1072], Loss: 0.3682\n","Epoch [3/5], Step [888/1072], Loss: 0.3593\n","Epoch [3/5], Step [889/1072], Loss: 0.3772\n","Epoch [3/5], Step [890/1072], Loss: 0.3534\n","Epoch [3/5], Step [891/1072], Loss: 0.3506\n","Epoch [3/5], Step [892/1072], Loss: 0.3436\n","Epoch [3/5], Step [893/1072], Loss: 0.3694\n","Epoch [3/5], Step [894/1072], Loss: 0.3731\n","Epoch [3/5], Step [895/1072], Loss: 0.3527\n","Epoch [3/5], Step [896/1072], Loss: 0.3759\n","Epoch [3/5], Step [897/1072], Loss: 0.3725\n","Epoch [3/5], Step [898/1072], Loss: 0.3713\n","Epoch [3/5], Step [899/1072], Loss: 0.3472\n","Epoch [3/5], Step [900/1072], Loss: 0.3334\n","Epoch [3/5], Step [901/1072], Loss: 0.3694\n","Epoch [3/5], Step [902/1072], Loss: 0.3842\n","Epoch [3/5], Step [903/1072], Loss: 0.3853\n","Epoch [3/5], Step [904/1072], Loss: 0.3201\n","Epoch [3/5], Step [905/1072], Loss: 0.3960\n","Epoch [3/5], Step [906/1072], Loss: 0.3762\n","Epoch [3/5], Step [907/1072], Loss: 0.3535\n","Epoch [3/5], Step [908/1072], Loss: 0.3348\n","Epoch [3/5], Step [909/1072], Loss: 0.3470\n","Epoch [3/5], Step [910/1072], Loss: 0.3575\n","Epoch [3/5], Step [911/1072], Loss: 0.3724\n","Epoch [3/5], Step [912/1072], Loss: 0.3666\n","Epoch [3/5], Step [913/1072], Loss: 0.3782\n","Epoch [3/5], Step [914/1072], Loss: 0.3560\n","Epoch [3/5], Step [915/1072], Loss: 0.3528\n","Epoch [3/5], Step [916/1072], Loss: 0.3713\n","Epoch [3/5], Step [917/1072], Loss: 0.3516\n","Epoch [3/5], Step [918/1072], Loss: 0.3672\n","Epoch [3/5], Step [919/1072], Loss: 0.3531\n","Epoch [3/5], Step [920/1072], Loss: 0.3663\n","Epoch [3/5], Step [921/1072], Loss: 0.3720\n","Epoch [3/5], Step [922/1072], Loss: 0.3872\n","Epoch [3/5], Step [923/1072], Loss: 0.3580\n","Epoch [3/5], Step [924/1072], Loss: 0.3600\n","Epoch [3/5], Step [925/1072], Loss: 0.3459\n","Epoch [3/5], Step [926/1072], Loss: 0.3647\n","Epoch [3/5], Step [927/1072], Loss: 0.3851\n","Epoch [3/5], Step [928/1072], Loss: 0.3406\n","Epoch [3/5], Step [929/1072], Loss: 0.3607\n","Epoch [3/5], Step [930/1072], Loss: 0.3542\n","Epoch [3/5], Step [931/1072], Loss: 0.3497\n","Epoch [3/5], Step [932/1072], Loss: 0.3447\n","Epoch [3/5], Step [933/1072], Loss: 0.3548\n","Epoch [3/5], Step [934/1072], Loss: 0.3564\n","Epoch [3/5], Step [935/1072], Loss: 0.3672\n","Epoch [3/5], Step [936/1072], Loss: 0.3657\n","Epoch [3/5], Step [937/1072], Loss: 0.3585\n","Epoch [3/5], Step [938/1072], Loss: 0.3801\n","Epoch [3/5], Step [939/1072], Loss: 0.3391\n","Epoch [3/5], Step [940/1072], Loss: 0.3902\n","Epoch [3/5], Step [941/1072], Loss: 0.3519\n","Epoch [3/5], Step [942/1072], Loss: 0.3729\n","Epoch [3/5], Step [943/1072], Loss: 0.3756\n","Epoch [3/5], Step [944/1072], Loss: 0.3512\n","Epoch [3/5], Step [945/1072], Loss: 0.3477\n","Epoch [3/5], Step [946/1072], Loss: 0.3436\n","Epoch [3/5], Step [947/1072], Loss: 0.3856\n","Epoch [3/5], Step [948/1072], Loss: 0.3522\n","Epoch [3/5], Step [949/1072], Loss: 0.3824\n","Epoch [3/5], Step [950/1072], Loss: 0.3679\n","Epoch [3/5], Step [951/1072], Loss: 0.3805\n","Epoch [3/5], Step [952/1072], Loss: 0.3896\n","Epoch [3/5], Step [953/1072], Loss: 0.3819\n","Epoch [3/5], Step [954/1072], Loss: 0.3576\n","Epoch [3/5], Step [955/1072], Loss: 0.3620\n","Epoch [3/5], Step [956/1072], Loss: 0.3733\n","Epoch [3/5], Step [957/1072], Loss: 0.3648\n","Epoch [3/5], Step [958/1072], Loss: 0.3500\n","Epoch [3/5], Step [959/1072], Loss: 0.3536\n","Epoch [3/5], Step [960/1072], Loss: 0.3759\n","Epoch [3/5], Step [961/1072], Loss: 0.3333\n","Epoch [3/5], Step [962/1072], Loss: 0.3569\n","Epoch [3/5], Step [963/1072], Loss: 0.3547\n","Epoch [3/5], Step [964/1072], Loss: 0.3492\n","Epoch [3/5], Step [965/1072], Loss: 0.3621\n","Epoch [3/5], Step [966/1072], Loss: 0.3254\n","Epoch [3/5], Step [967/1072], Loss: 0.4043\n","Epoch [3/5], Step [968/1072], Loss: 0.3369\n","Epoch [3/5], Step [969/1072], Loss: 0.3525\n","Epoch [3/5], Step [970/1072], Loss: 0.3696\n","Epoch [3/5], Step [971/1072], Loss: 0.3241\n","Epoch [3/5], Step [972/1072], Loss: 0.3858\n","Epoch [3/5], Step [973/1072], Loss: 0.3675\n","Epoch [3/5], Step [974/1072], Loss: 0.3911\n","Epoch [3/5], Step [975/1072], Loss: 0.3782\n","Epoch [3/5], Step [976/1072], Loss: 0.3583\n","Epoch [3/5], Step [977/1072], Loss: 0.3767\n","Epoch [3/5], Step [978/1072], Loss: 0.3659\n","Epoch [3/5], Step [979/1072], Loss: 0.3742\n","Epoch [3/5], Step [980/1072], Loss: 0.3940\n","Epoch [3/5], Step [981/1072], Loss: 0.3226\n","Epoch [3/5], Step [982/1072], Loss: 0.3352\n","Epoch [3/5], Step [983/1072], Loss: 0.3732\n","Epoch [3/5], Step [984/1072], Loss: 0.3476\n","Epoch [3/5], Step [985/1072], Loss: 0.3645\n","Epoch [3/5], Step [986/1072], Loss: 0.3599\n","Epoch [3/5], Step [987/1072], Loss: 0.3935\n","Epoch [3/5], Step [988/1072], Loss: 0.3764\n","Epoch [3/5], Step [989/1072], Loss: 0.3633\n","Epoch [3/5], Step [990/1072], Loss: 0.3280\n","Epoch [3/5], Step [991/1072], Loss: 0.3556\n","Epoch [3/5], Step [992/1072], Loss: 0.3665\n","Epoch [3/5], Step [993/1072], Loss: 0.3667\n","Epoch [3/5], Step [994/1072], Loss: 0.3581\n","Epoch [3/5], Step [995/1072], Loss: 0.3842\n","Epoch [3/5], Step [996/1072], Loss: 0.3559\n","Epoch [3/5], Step [997/1072], Loss: 0.3315\n","Epoch [3/5], Step [998/1072], Loss: 0.3346\n","Epoch [3/5], Step [999/1072], Loss: 0.3504\n","Epoch [3/5], Step [1000/1072], Loss: 0.3651\n","Epoch [3/5], Step [1001/1072], Loss: 0.3723\n","Epoch [3/5], Step [1002/1072], Loss: 0.3539\n","Epoch [3/5], Step [1003/1072], Loss: 0.3489\n","Epoch [3/5], Step [1004/1072], Loss: 0.3747\n","Epoch [3/5], Step [1005/1072], Loss: 0.3536\n","Epoch [3/5], Step [1006/1072], Loss: 0.3624\n","Epoch [3/5], Step [1007/1072], Loss: 0.3905\n","Epoch [3/5], Step [1008/1072], Loss: 0.3705\n","Epoch [3/5], Step [1009/1072], Loss: 0.3615\n","Epoch [3/5], Step [1010/1072], Loss: 0.3552\n","Epoch [3/5], Step [1011/1072], Loss: 0.3821\n","Epoch [3/5], Step [1012/1072], Loss: 0.3677\n","Epoch [3/5], Step [1013/1072], Loss: 0.3486\n","Epoch [3/5], Step [1014/1072], Loss: 0.3734\n","Epoch [3/5], Step [1015/1072], Loss: 0.3368\n","Epoch [3/5], Step [1016/1072], Loss: 0.3668\n","Epoch [3/5], Step [1017/1072], Loss: 0.3851\n","Epoch [3/5], Step [1018/1072], Loss: 0.4071\n","Epoch [3/5], Step [1019/1072], Loss: 0.3653\n","Epoch [3/5], Step [1020/1072], Loss: 0.4030\n","Epoch [3/5], Step [1021/1072], Loss: 0.3362\n","Epoch [3/5], Step [1022/1072], Loss: 0.3746\n","Epoch [3/5], Step [1023/1072], Loss: 0.3554\n","Epoch [3/5], Step [1024/1072], Loss: 0.3559\n","Epoch [3/5], Step [1025/1072], Loss: 0.3843\n","Epoch [3/5], Step [1026/1072], Loss: 0.3713\n","Epoch [3/5], Step [1027/1072], Loss: 0.3693\n","Epoch [3/5], Step [1028/1072], Loss: 0.3706\n","Epoch [3/5], Step [1029/1072], Loss: 0.3844\n","Epoch [3/5], Step [1030/1072], Loss: 0.3622\n","Epoch [3/5], Step [1031/1072], Loss: 0.3892\n","Epoch [3/5], Step [1032/1072], Loss: 0.3694\n","Epoch [3/5], Step [1033/1072], Loss: 0.3831\n","Epoch [3/5], Step [1034/1072], Loss: 0.3504\n","Epoch [3/5], Step [1035/1072], Loss: 0.3665\n","Epoch [3/5], Step [1036/1072], Loss: 0.3780\n","Epoch [3/5], Step [1037/1072], Loss: 0.3458\n","Epoch [3/5], Step [1038/1072], Loss: 0.3544\n","Epoch [3/5], Step [1039/1072], Loss: 0.3629\n","Epoch [3/5], Step [1040/1072], Loss: 0.3557\n","Epoch [3/5], Step [1041/1072], Loss: 0.3713\n","Epoch [3/5], Step [1042/1072], Loss: 0.3682\n","Epoch [3/5], Step [1043/1072], Loss: 0.3556\n","Epoch [3/5], Step [1044/1072], Loss: 0.3669\n","Epoch [3/5], Step [1045/1072], Loss: 0.3514\n","Epoch [3/5], Step [1046/1072], Loss: 0.3627\n","Epoch [3/5], Step [1047/1072], Loss: 0.3342\n","Epoch [3/5], Step [1048/1072], Loss: 0.3798\n","Epoch [3/5], Step [1049/1072], Loss: 0.3286\n","Epoch [3/5], Step [1050/1072], Loss: 0.3750\n","Epoch [3/5], Step [1051/1072], Loss: 0.3400\n","Epoch [3/5], Step [1052/1072], Loss: 0.3434\n","Epoch [3/5], Step [1053/1072], Loss: 0.3619\n","Epoch [3/5], Step [1054/1072], Loss: 0.3796\n","Epoch [3/5], Step [1055/1072], Loss: 0.3605\n","Epoch [3/5], Step [1056/1072], Loss: 0.3327\n","Epoch [3/5], Step [1057/1072], Loss: 0.3642\n","Epoch [3/5], Step [1058/1072], Loss: 0.3742\n","Epoch [3/5], Step [1059/1072], Loss: 0.3499\n","Epoch [3/5], Step [1060/1072], Loss: 0.3735\n","Epoch [3/5], Step [1061/1072], Loss: 0.3664\n","Epoch [3/5], Step [1062/1072], Loss: 0.3639\n","Epoch [3/5], Step [1063/1072], Loss: 0.3429\n","Epoch [3/5], Step [1064/1072], Loss: 0.3498\n","Epoch [3/5], Step [1065/1072], Loss: 0.3436\n","Epoch [3/5], Step [1066/1072], Loss: 0.3718\n","Epoch [3/5], Step [1067/1072], Loss: 0.3641\n","Epoch [3/5], Step [1068/1072], Loss: 0.3572\n","Epoch [3/5], Step [1069/1072], Loss: 0.3615\n","Epoch [3/5], Step [1070/1072], Loss: 0.3925\n","Epoch [3/5], Step [1071/1072], Loss: 0.3732\n","Epoch [3/5], Step [1072/1072], Loss: 0.3609\n","Epoch [4/5], Step [1/1072], Loss: 0.3410\n","Epoch [4/5], Step [2/1072], Loss: 0.3390\n","Epoch [4/5], Step [3/1072], Loss: 0.3511\n","Epoch [4/5], Step [4/1072], Loss: 0.3785\n","Epoch [4/5], Step [5/1072], Loss: 0.3285\n","Epoch [4/5], Step [6/1072], Loss: 0.3325\n","Epoch [4/5], Step [7/1072], Loss: 0.3786\n","Epoch [4/5], Step [8/1072], Loss: 0.3487\n","Epoch [4/5], Step [9/1072], Loss: 0.3319\n","Epoch [4/5], Step [10/1072], Loss: 0.3521\n","Epoch [4/5], Step [11/1072], Loss: 0.3568\n","Epoch [4/5], Step [12/1072], Loss: 0.3497\n","Epoch [4/5], Step [13/1072], Loss: 0.3223\n","Epoch [4/5], Step [14/1072], Loss: 0.3621\n","Epoch [4/5], Step [15/1072], Loss: 0.3401\n","Epoch [4/5], Step [16/1072], Loss: 0.3491\n","Epoch [4/5], Step [17/1072], Loss: 0.3032\n","Epoch [4/5], Step [18/1072], Loss: 0.3574\n","Epoch [4/5], Step [19/1072], Loss: 0.3181\n","Epoch [4/5], Step [20/1072], Loss: 0.3565\n","Epoch [4/5], Step [21/1072], Loss: 0.3134\n","Epoch [4/5], Step [22/1072], Loss: 0.3111\n","Epoch [4/5], Step [23/1072], Loss: 0.3619\n","Epoch [4/5], Step [24/1072], Loss: 0.3278\n","Epoch [4/5], Step [25/1072], Loss: 0.3319\n","Epoch [4/5], Step [26/1072], Loss: 0.3469\n","Epoch [4/5], Step [27/1072], Loss: 0.3072\n","Epoch [4/5], Step [28/1072], Loss: 0.3241\n","Epoch [4/5], Step [29/1072], Loss: 0.3187\n","Epoch [4/5], Step [30/1072], Loss: 0.3459\n","Epoch [4/5], Step [31/1072], Loss: 0.3526\n","Epoch [4/5], Step [32/1072], Loss: 0.3461\n","Epoch [4/5], Step [33/1072], Loss: 0.3261\n","Epoch [4/5], Step [34/1072], Loss: 0.3367\n","Epoch [4/5], Step [35/1072], Loss: 0.3237\n","Epoch [4/5], Step [36/1072], Loss: 0.3595\n","Epoch [4/5], Step [37/1072], Loss: 0.3280\n","Epoch [4/5], Step [38/1072], Loss: 0.3506\n","Epoch [4/5], Step [39/1072], Loss: 0.3386\n","Epoch [4/5], Step [40/1072], Loss: 0.3291\n","Epoch [4/5], Step [41/1072], Loss: 0.3567\n","Epoch [4/5], Step [42/1072], Loss: 0.3125\n","Epoch [4/5], Step [43/1072], Loss: 0.3720\n","Epoch [4/5], Step [44/1072], Loss: 0.3230\n","Epoch [4/5], Step [45/1072], Loss: 0.3375\n","Epoch [4/5], Step [46/1072], Loss: 0.3210\n","Epoch [4/5], Step [47/1072], Loss: 0.3183\n","Epoch [4/5], Step [48/1072], Loss: 0.3275\n","Epoch [4/5], Step [49/1072], Loss: 0.3475\n","Epoch [4/5], Step [50/1072], Loss: 0.3264\n","Epoch [4/5], Step [51/1072], Loss: 0.3174\n","Epoch [4/5], Step [52/1072], Loss: 0.3361\n","Epoch [4/5], Step [53/1072], Loss: 0.3427\n","Epoch [4/5], Step [54/1072], Loss: 0.3343\n","Epoch [4/5], Step [55/1072], Loss: 0.3607\n","Epoch [4/5], Step [56/1072], Loss: 0.3349\n","Epoch [4/5], Step [57/1072], Loss: 0.3433\n","Epoch [4/5], Step [58/1072], Loss: 0.3149\n","Epoch [4/5], Step [59/1072], Loss: 0.3436\n","Epoch [4/5], Step [60/1072], Loss: 0.3210\n","Epoch [4/5], Step [61/1072], Loss: 0.3709\n","Epoch [4/5], Step [62/1072], Loss: 0.3436\n","Epoch [4/5], Step [63/1072], Loss: 0.3593\n","Epoch [4/5], Step [64/1072], Loss: 0.3475\n","Epoch [4/5], Step [65/1072], Loss: 0.3093\n","Epoch [4/5], Step [66/1072], Loss: 0.3295\n","Epoch [4/5], Step [67/1072], Loss: 0.3344\n","Epoch [4/5], Step [68/1072], Loss: 0.3295\n","Epoch [4/5], Step [69/1072], Loss: 0.3530\n","Epoch [4/5], Step [70/1072], Loss: 0.3410\n","Epoch [4/5], Step [71/1072], Loss: 0.3333\n","Epoch [4/5], Step [72/1072], Loss: 0.3378\n","Epoch [4/5], Step [73/1072], Loss: 0.3429\n","Epoch [4/5], Step [74/1072], Loss: 0.3317\n","Epoch [4/5], Step [75/1072], Loss: 0.3515\n","Epoch [4/5], Step [76/1072], Loss: 0.3349\n","Epoch [4/5], Step [77/1072], Loss: 0.3241\n","Epoch [4/5], Step [78/1072], Loss: 0.3277\n","Epoch [4/5], Step [79/1072], Loss: 0.3372\n","Epoch [4/5], Step [80/1072], Loss: 0.3267\n","Epoch [4/5], Step [81/1072], Loss: 0.3287\n","Epoch [4/5], Step [82/1072], Loss: 0.3440\n","Epoch [4/5], Step [83/1072], Loss: 0.3616\n","Epoch [4/5], Step [84/1072], Loss: 0.3545\n","Epoch [4/5], Step [85/1072], Loss: 0.3457\n","Epoch [4/5], Step [86/1072], Loss: 0.3623\n","Epoch [4/5], Step [87/1072], Loss: 0.3587\n","Epoch [4/5], Step [88/1072], Loss: 0.3238\n","Epoch [4/5], Step [89/1072], Loss: 0.3201\n","Epoch [4/5], Step [90/1072], Loss: 0.3391\n","Epoch [4/5], Step [91/1072], Loss: 0.3116\n","Epoch [4/5], Step [92/1072], Loss: 0.3322\n","Epoch [4/5], Step [93/1072], Loss: 0.3266\n","Epoch [4/5], Step [94/1072], Loss: 0.3472\n","Epoch [4/5], Step [95/1072], Loss: 0.3272\n","Epoch [4/5], Step [96/1072], Loss: 0.3254\n","Epoch [4/5], Step [97/1072], Loss: 0.3558\n","Epoch [4/5], Step [98/1072], Loss: 0.3171\n","Epoch [4/5], Step [99/1072], Loss: 0.3234\n","Epoch [4/5], Step [100/1072], Loss: 0.3584\n","Epoch [4/5], Step [101/1072], Loss: 0.3376\n","Epoch [4/5], Step [102/1072], Loss: 0.3426\n","Epoch [4/5], Step [103/1072], Loss: 0.3429\n","Epoch [4/5], Step [104/1072], Loss: 0.3437\n","Epoch [4/5], Step [105/1072], Loss: 0.3716\n","Epoch [4/5], Step [106/1072], Loss: 0.3261\n","Epoch [4/5], Step [107/1072], Loss: 0.3431\n","Epoch [4/5], Step [108/1072], Loss: 0.3678\n","Epoch [4/5], Step [109/1072], Loss: 0.3009\n","Epoch [4/5], Step [110/1072], Loss: 0.3071\n","Epoch [4/5], Step [111/1072], Loss: 0.3136\n","Epoch [4/5], Step [112/1072], Loss: 0.3362\n","Epoch [4/5], Step [113/1072], Loss: 0.3448\n","Epoch [4/5], Step [114/1072], Loss: 0.3405\n","Epoch [4/5], Step [115/1072], Loss: 0.3436\n","Epoch [4/5], Step [116/1072], Loss: 0.3501\n","Epoch [4/5], Step [117/1072], Loss: 0.3483\n","Epoch [4/5], Step [118/1072], Loss: 0.3319\n","Epoch [4/5], Step [119/1072], Loss: 0.3466\n","Epoch [4/5], Step [120/1072], Loss: 0.3523\n","Epoch [4/5], Step [121/1072], Loss: 0.3665\n","Epoch [4/5], Step [122/1072], Loss: 0.3136\n","Epoch [4/5], Step [123/1072], Loss: 0.3445\n","Epoch [4/5], Step [124/1072], Loss: 0.3359\n","Epoch [4/5], Step [125/1072], Loss: 0.3266\n","Epoch [4/5], Step [126/1072], Loss: 0.3446\n","Epoch [4/5], Step [127/1072], Loss: 0.3170\n","Epoch [4/5], Step [128/1072], Loss: 0.3455\n","Epoch [4/5], Step [129/1072], Loss: 0.3668\n","Epoch [4/5], Step [130/1072], Loss: 0.3673\n","Epoch [4/5], Step [131/1072], Loss: 0.3562\n","Epoch [4/5], Step [132/1072], Loss: 0.3197\n","Epoch [4/5], Step [133/1072], Loss: 0.3227\n","Epoch [4/5], Step [134/1072], Loss: 0.3195\n","Epoch [4/5], Step [135/1072], Loss: 0.3518\n","Epoch [4/5], Step [136/1072], Loss: 0.3363\n","Epoch [4/5], Step [137/1072], Loss: 0.3502\n","Epoch [4/5], Step [138/1072], Loss: 0.3346\n","Epoch [4/5], Step [139/1072], Loss: 0.3061\n","Epoch [4/5], Step [140/1072], Loss: 0.3088\n","Epoch [4/5], Step [141/1072], Loss: 0.3359\n","Epoch [4/5], Step [142/1072], Loss: 0.3467\n","Epoch [4/5], Step [143/1072], Loss: 0.3390\n","Epoch [4/5], Step [144/1072], Loss: 0.3376\n","Epoch [4/5], Step [145/1072], Loss: 0.3456\n","Epoch [4/5], Step [146/1072], Loss: 0.3223\n","Epoch [4/5], Step [147/1072], Loss: 0.3398\n","Epoch [4/5], Step [148/1072], Loss: 0.3291\n","Epoch [4/5], Step [149/1072], Loss: 0.3128\n","Epoch [4/5], Step [150/1072], Loss: 0.3350\n","Epoch [4/5], Step [151/1072], Loss: 0.3514\n","Epoch [4/5], Step [152/1072], Loss: 0.3220\n","Epoch [4/5], Step [153/1072], Loss: 0.3111\n","Epoch [4/5], Step [154/1072], Loss: 0.3718\n","Epoch [4/5], Step [155/1072], Loss: 0.3596\n","Epoch [4/5], Step [156/1072], Loss: 0.3586\n","Epoch [4/5], Step [157/1072], Loss: 0.3353\n","Epoch [4/5], Step [158/1072], Loss: 0.3442\n","Epoch [4/5], Step [159/1072], Loss: 0.3612\n","Epoch [4/5], Step [160/1072], Loss: 0.3443\n","Epoch [4/5], Step [161/1072], Loss: 0.3436\n","Epoch [4/5], Step [162/1072], Loss: 0.3373\n","Epoch [4/5], Step [163/1072], Loss: 0.3326\n","Epoch [4/5], Step [164/1072], Loss: 0.3341\n","Epoch [4/5], Step [165/1072], Loss: 0.3193\n","Epoch [4/5], Step [166/1072], Loss: 0.3419\n","Epoch [4/5], Step [167/1072], Loss: 0.3321\n","Epoch [4/5], Step [168/1072], Loss: 0.3550\n","Epoch [4/5], Step [169/1072], Loss: 0.3495\n","Epoch [4/5], Step [170/1072], Loss: 0.3428\n","Epoch [4/5], Step [171/1072], Loss: 0.3463\n","Epoch [4/5], Step [172/1072], Loss: 0.3770\n","Epoch [4/5], Step [173/1072], Loss: 0.3471\n","Epoch [4/5], Step [174/1072], Loss: 0.3296\n","Epoch [4/5], Step [175/1072], Loss: 0.3366\n","Epoch [4/5], Step [176/1072], Loss: 0.3252\n","Epoch [4/5], Step [177/1072], Loss: 0.3618\n","Epoch [4/5], Step [178/1072], Loss: 0.3474\n","Epoch [4/5], Step [179/1072], Loss: 0.3324\n","Epoch [4/5], Step [180/1072], Loss: 0.3608\n","Epoch [4/5], Step [181/1072], Loss: 0.3156\n","Epoch [4/5], Step [182/1072], Loss: 0.3525\n","Epoch [4/5], Step [183/1072], Loss: 0.3408\n","Epoch [4/5], Step [184/1072], Loss: 0.3412\n","Epoch [4/5], Step [185/1072], Loss: 0.3168\n","Epoch [4/5], Step [186/1072], Loss: 0.3237\n","Epoch [4/5], Step [187/1072], Loss: 0.3142\n","Epoch [4/5], Step [188/1072], Loss: 0.3218\n","Epoch [4/5], Step [189/1072], Loss: 0.3454\n","Epoch [4/5], Step [190/1072], Loss: 0.3286\n","Epoch [4/5], Step [191/1072], Loss: 0.3364\n","Epoch [4/5], Step [192/1072], Loss: 0.3636\n","Epoch [4/5], Step [193/1072], Loss: 0.3229\n","Epoch [4/5], Step [194/1072], Loss: 0.3507\n","Epoch [4/5], Step [195/1072], Loss: 0.3340\n","Epoch [4/5], Step [196/1072], Loss: 0.3484\n","Epoch [4/5], Step [197/1072], Loss: 0.3385\n","Epoch [4/5], Step [198/1072], Loss: 0.3237\n","Epoch [4/5], Step [199/1072], Loss: 0.3401\n","Epoch [4/5], Step [200/1072], Loss: 0.3638\n","Epoch [4/5], Step [201/1072], Loss: 0.3270\n","Epoch [4/5], Step [202/1072], Loss: 0.3395\n","Epoch [4/5], Step [203/1072], Loss: 0.3176\n","Epoch [4/5], Step [204/1072], Loss: 0.3329\n","Epoch [4/5], Step [205/1072], Loss: 0.2983\n","Epoch [4/5], Step [206/1072], Loss: 0.3185\n","Epoch [4/5], Step [207/1072], Loss: 0.3266\n","Epoch [4/5], Step [208/1072], Loss: 0.3633\n","Epoch [4/5], Step [209/1072], Loss: 0.3576\n","Epoch [4/5], Step [210/1072], Loss: 0.3412\n","Epoch [4/5], Step [211/1072], Loss: 0.3698\n","Epoch [4/5], Step [212/1072], Loss: 0.3584\n","Epoch [4/5], Step [213/1072], Loss: 0.3442\n","Epoch [4/5], Step [214/1072], Loss: 0.3146\n","Epoch [4/5], Step [215/1072], Loss: 0.3262\n","Epoch [4/5], Step [216/1072], Loss: 0.3675\n","Epoch [4/5], Step [217/1072], Loss: 0.3493\n","Epoch [4/5], Step [218/1072], Loss: 0.3231\n","Epoch [4/5], Step [219/1072], Loss: 0.3293\n","Epoch [4/5], Step [220/1072], Loss: 0.3349\n","Epoch [4/5], Step [221/1072], Loss: 0.3441\n","Epoch [4/5], Step [222/1072], Loss: 0.3445\n","Epoch [4/5], Step [223/1072], Loss: 0.3508\n","Epoch [4/5], Step [224/1072], Loss: 0.3277\n","Epoch [4/5], Step [225/1072], Loss: 0.3466\n","Epoch [4/5], Step [226/1072], Loss: 0.3491\n","Epoch [4/5], Step [227/1072], Loss: 0.3502\n","Epoch [4/5], Step [228/1072], Loss: 0.3455\n","Epoch [4/5], Step [229/1072], Loss: 0.3318\n","Epoch [4/5], Step [230/1072], Loss: 0.3368\n","Epoch [4/5], Step [231/1072], Loss: 0.3458\n","Epoch [4/5], Step [232/1072], Loss: 0.3326\n","Epoch [4/5], Step [233/1072], Loss: 0.3499\n","Epoch [4/5], Step [234/1072], Loss: 0.3550\n","Epoch [4/5], Step [235/1072], Loss: 0.3255\n","Epoch [4/5], Step [236/1072], Loss: 0.3283\n","Epoch [4/5], Step [237/1072], Loss: 0.3528\n","Epoch [4/5], Step [238/1072], Loss: 0.3495\n","Epoch [4/5], Step [239/1072], Loss: 0.3338\n","Epoch [4/5], Step [240/1072], Loss: 0.3689\n","Epoch [4/5], Step [241/1072], Loss: 0.3480\n","Epoch [4/5], Step [242/1072], Loss: 0.3448\n","Epoch [4/5], Step [243/1072], Loss: 0.3508\n","Epoch [4/5], Step [244/1072], Loss: 0.3567\n","Epoch [4/5], Step [245/1072], Loss: 0.3390\n","Epoch [4/5], Step [246/1072], Loss: 0.3575\n","Epoch [4/5], Step [247/1072], Loss: 0.3577\n","Epoch [4/5], Step [248/1072], Loss: 0.3552\n","Epoch [4/5], Step [249/1072], Loss: 0.3780\n","Epoch [4/5], Step [250/1072], Loss: 0.3374\n","Epoch [4/5], Step [251/1072], Loss: 0.3276\n","Epoch [4/5], Step [252/1072], Loss: 0.3300\n","Epoch [4/5], Step [253/1072], Loss: 0.3553\n","Epoch [4/5], Step [254/1072], Loss: 0.3219\n","Epoch [4/5], Step [255/1072], Loss: 0.3864\n","Epoch [4/5], Step [256/1072], Loss: 0.3322\n","Epoch [4/5], Step [257/1072], Loss: 0.3707\n","Epoch [4/5], Step [258/1072], Loss: 0.3179\n","Epoch [4/5], Step [259/1072], Loss: 0.3554\n","Epoch [4/5], Step [260/1072], Loss: 0.3144\n","Epoch [4/5], Step [261/1072], Loss: 0.3287\n","Epoch [4/5], Step [262/1072], Loss: 0.3482\n","Epoch [4/5], Step [263/1072], Loss: 0.3293\n","Epoch [4/5], Step [264/1072], Loss: 0.3288\n","Epoch [4/5], Step [265/1072], Loss: 0.3257\n","Epoch [4/5], Step [266/1072], Loss: 0.3309\n","Epoch [4/5], Step [267/1072], Loss: 0.3411\n","Epoch [4/5], Step [268/1072], Loss: 0.3524\n","Epoch [4/5], Step [269/1072], Loss: 0.3435\n","Epoch [4/5], Step [270/1072], Loss: 0.3340\n","Epoch [4/5], Step [271/1072], Loss: 0.3589\n","Epoch [4/5], Step [272/1072], Loss: 0.3306\n","Epoch [4/5], Step [273/1072], Loss: 0.3406\n","Epoch [4/5], Step [274/1072], Loss: 0.3354\n","Epoch [4/5], Step [275/1072], Loss: 0.3221\n","Epoch [4/5], Step [276/1072], Loss: 0.3455\n","Epoch [4/5], Step [277/1072], Loss: 0.3148\n","Epoch [4/5], Step [278/1072], Loss: 0.3331\n","Epoch [4/5], Step [279/1072], Loss: 0.3450\n","Epoch [4/5], Step [280/1072], Loss: 0.3332\n","Epoch [4/5], Step [281/1072], Loss: 0.3435\n","Epoch [4/5], Step [282/1072], Loss: 0.3429\n","Epoch [4/5], Step [283/1072], Loss: 0.3299\n","Epoch [4/5], Step [284/1072], Loss: 0.3727\n","Epoch [4/5], Step [285/1072], Loss: 0.3203\n","Epoch [4/5], Step [286/1072], Loss: 0.3385\n","Epoch [4/5], Step [287/1072], Loss: 0.3273\n","Epoch [4/5], Step [288/1072], Loss: 0.3424\n","Epoch [4/5], Step [289/1072], Loss: 0.3483\n","Epoch [4/5], Step [290/1072], Loss: 0.3717\n","Epoch [4/5], Step [291/1072], Loss: 0.3150\n","Epoch [4/5], Step [292/1072], Loss: 0.3364\n","Epoch [4/5], Step [293/1072], Loss: 0.3536\n","Epoch [4/5], Step [294/1072], Loss: 0.3329\n","Epoch [4/5], Step [295/1072], Loss: 0.3216\n","Epoch [4/5], Step [296/1072], Loss: 0.3498\n","Epoch [4/5], Step [297/1072], Loss: 0.3400\n","Epoch [4/5], Step [298/1072], Loss: 0.3518\n","Epoch [4/5], Step [299/1072], Loss: 0.3312\n","Epoch [4/5], Step [300/1072], Loss: 0.3238\n","Epoch [4/5], Step [301/1072], Loss: 0.3427\n","Epoch [4/5], Step [302/1072], Loss: 0.3244\n","Epoch [4/5], Step [303/1072], Loss: 0.3248\n","Epoch [4/5], Step [304/1072], Loss: 0.3281\n","Epoch [4/5], Step [305/1072], Loss: 0.3214\n","Epoch [4/5], Step [306/1072], Loss: 0.3329\n","Epoch [4/5], Step [307/1072], Loss: 0.3170\n","Epoch [4/5], Step [308/1072], Loss: 0.3282\n","Epoch [4/5], Step [309/1072], Loss: 0.3422\n","Epoch [4/5], Step [310/1072], Loss: 0.3486\n","Epoch [4/5], Step [311/1072], Loss: 0.3399\n","Epoch [4/5], Step [312/1072], Loss: 0.3449\n","Epoch [4/5], Step [313/1072], Loss: 0.3575\n","Epoch [4/5], Step [314/1072], Loss: 0.3329\n","Epoch [4/5], Step [315/1072], Loss: 0.3306\n","Epoch [4/5], Step [316/1072], Loss: 0.3579\n","Epoch [4/5], Step [317/1072], Loss: 0.3260\n","Epoch [4/5], Step [318/1072], Loss: 0.3410\n","Epoch [4/5], Step [319/1072], Loss: 0.3125\n","Epoch [4/5], Step [320/1072], Loss: 0.3358\n","Epoch [4/5], Step [321/1072], Loss: 0.3644\n","Epoch [4/5], Step [322/1072], Loss: 0.3300\n","Epoch [4/5], Step [323/1072], Loss: 0.3476\n","Epoch [4/5], Step [324/1072], Loss: 0.3207\n","Epoch [4/5], Step [325/1072], Loss: 0.3295\n","Epoch [4/5], Step [326/1072], Loss: 0.3265\n","Epoch [4/5], Step [327/1072], Loss: 0.3213\n","Epoch [4/5], Step [328/1072], Loss: 0.3219\n","Epoch [4/5], Step [329/1072], Loss: 0.3190\n","Epoch [4/5], Step [330/1072], Loss: 0.3104\n","Epoch [4/5], Step [331/1072], Loss: 0.3141\n","Epoch [4/5], Step [332/1072], Loss: 0.3488\n","Epoch [4/5], Step [333/1072], Loss: 0.3433\n","Epoch [4/5], Step [334/1072], Loss: 0.3247\n","Epoch [4/5], Step [335/1072], Loss: 0.3464\n","Epoch [4/5], Step [336/1072], Loss: 0.3280\n","Epoch [4/5], Step [337/1072], Loss: 0.3322\n","Epoch [4/5], Step [338/1072], Loss: 0.3350\n","Epoch [4/5], Step [339/1072], Loss: 0.3423\n","Epoch [4/5], Step [340/1072], Loss: 0.3334\n","Epoch [4/5], Step [341/1072], Loss: 0.3158\n","Epoch [4/5], Step [342/1072], Loss: 0.3565\n","Epoch [4/5], Step [343/1072], Loss: 0.3370\n","Epoch [4/5], Step [344/1072], Loss: 0.3528\n","Epoch [4/5], Step [345/1072], Loss: 0.3652\n","Epoch [4/5], Step [346/1072], Loss: 0.3248\n","Epoch [4/5], Step [347/1072], Loss: 0.3624\n","Epoch [4/5], Step [348/1072], Loss: 0.3373\n","Epoch [4/5], Step [349/1072], Loss: 0.3463\n","Epoch [4/5], Step [350/1072], Loss: 0.3395\n","Epoch [4/5], Step [351/1072], Loss: 0.3369\n","Epoch [4/5], Step [352/1072], Loss: 0.3425\n","Epoch [4/5], Step [353/1072], Loss: 0.3519\n","Epoch [4/5], Step [354/1072], Loss: 0.3037\n","Epoch [4/5], Step [355/1072], Loss: 0.3190\n","Epoch [4/5], Step [356/1072], Loss: 0.3368\n","Epoch [4/5], Step [357/1072], Loss: 0.3695\n","Epoch [4/5], Step [358/1072], Loss: 0.3446\n","Epoch [4/5], Step [359/1072], Loss: 0.3335\n","Epoch [4/5], Step [360/1072], Loss: 0.3118\n","Epoch [4/5], Step [361/1072], Loss: 0.3457\n","Epoch [4/5], Step [362/1072], Loss: 0.3002\n","Epoch [4/5], Step [363/1072], Loss: 0.3320\n","Epoch [4/5], Step [364/1072], Loss: 0.3633\n","Epoch [4/5], Step [365/1072], Loss: 0.3579\n","Epoch [4/5], Step [366/1072], Loss: 0.3291\n","Epoch [4/5], Step [367/1072], Loss: 0.3486\n","Epoch [4/5], Step [368/1072], Loss: 0.3588\n","Epoch [4/5], Step [369/1072], Loss: 0.3279\n","Epoch [4/5], Step [370/1072], Loss: 0.3192\n","Epoch [4/5], Step [371/1072], Loss: 0.2993\n","Epoch [4/5], Step [372/1072], Loss: 0.3242\n","Epoch [4/5], Step [373/1072], Loss: 0.3436\n","Epoch [4/5], Step [374/1072], Loss: 0.3621\n","Epoch [4/5], Step [375/1072], Loss: 0.3312\n","Epoch [4/5], Step [376/1072], Loss: 0.2877\n","Epoch [4/5], Step [377/1072], Loss: 0.3349\n","Epoch [4/5], Step [378/1072], Loss: 0.3440\n","Epoch [4/5], Step [379/1072], Loss: 0.3088\n","Epoch [4/5], Step [380/1072], Loss: 0.3401\n","Epoch [4/5], Step [381/1072], Loss: 0.3347\n","Epoch [4/5], Step [382/1072], Loss: 0.3460\n","Epoch [4/5], Step [383/1072], Loss: 0.3282\n","Epoch [4/5], Step [384/1072], Loss: 0.3492\n","Epoch [4/5], Step [385/1072], Loss: 0.3651\n","Epoch [4/5], Step [386/1072], Loss: 0.3634\n","Epoch [4/5], Step [387/1072], Loss: 0.3330\n","Epoch [4/5], Step [388/1072], Loss: 0.3495\n","Epoch [4/5], Step [389/1072], Loss: 0.3289\n","Epoch [4/5], Step [390/1072], Loss: 0.3406\n","Epoch [4/5], Step [391/1072], Loss: 0.3357\n","Epoch [4/5], Step [392/1072], Loss: 0.3326\n","Epoch [4/5], Step [393/1072], Loss: 0.3215\n","Epoch [4/5], Step [394/1072], Loss: 0.3067\n","Epoch [4/5], Step [395/1072], Loss: 0.3458\n","Epoch [4/5], Step [396/1072], Loss: 0.3444\n","Epoch [4/5], Step [397/1072], Loss: 0.3425\n","Epoch [4/5], Step [398/1072], Loss: 0.3366\n","Epoch [4/5], Step [399/1072], Loss: 0.3314\n","Epoch [4/5], Step [400/1072], Loss: 0.3226\n","Epoch [4/5], Step [401/1072], Loss: 0.3428\n","Epoch [4/5], Step [402/1072], Loss: 0.3492\n","Epoch [4/5], Step [403/1072], Loss: 0.3616\n","Epoch [4/5], Step [404/1072], Loss: 0.3410\n","Epoch [4/5], Step [405/1072], Loss: 0.3642\n","Epoch [4/5], Step [406/1072], Loss: 0.3723\n","Epoch [4/5], Step [407/1072], Loss: 0.3091\n","Epoch [4/5], Step [408/1072], Loss: 0.3593\n","Epoch [4/5], Step [409/1072], Loss: 0.3485\n","Epoch [4/5], Step [410/1072], Loss: 0.3183\n","Epoch [4/5], Step [411/1072], Loss: 0.3141\n","Epoch [4/5], Step [412/1072], Loss: 0.3585\n","Epoch [4/5], Step [413/1072], Loss: 0.3155\n","Epoch [4/5], Step [414/1072], Loss: 0.3236\n","Epoch [4/5], Step [415/1072], Loss: 0.3104\n","Epoch [4/5], Step [416/1072], Loss: 0.3465\n","Epoch [4/5], Step [417/1072], Loss: 0.3432\n","Epoch [4/5], Step [418/1072], Loss: 0.3327\n","Epoch [4/5], Step [419/1072], Loss: 0.3247\n","Epoch [4/5], Step [420/1072], Loss: 0.3554\n","Epoch [4/5], Step [421/1072], Loss: 0.3233\n","Epoch [4/5], Step [422/1072], Loss: 0.3701\n","Epoch [4/5], Step [423/1072], Loss: 0.3485\n","Epoch [4/5], Step [424/1072], Loss: 0.3600\n","Epoch [4/5], Step [425/1072], Loss: 0.3529\n","Epoch [4/5], Step [426/1072], Loss: 0.3646\n","Epoch [4/5], Step [427/1072], Loss: 0.3467\n","Epoch [4/5], Step [428/1072], Loss: 0.3629\n","Epoch [4/5], Step [429/1072], Loss: 0.3552\n","Epoch [4/5], Step [430/1072], Loss: 0.3496\n","Epoch [4/5], Step [431/1072], Loss: 0.3654\n","Epoch [4/5], Step [432/1072], Loss: 0.3857\n","Epoch [4/5], Step [433/1072], Loss: 0.3517\n","Epoch [4/5], Step [434/1072], Loss: 0.3262\n","Epoch [4/5], Step [435/1072], Loss: 0.3403\n","Epoch [4/5], Step [436/1072], Loss: 0.3089\n","Epoch [4/5], Step [437/1072], Loss: 0.3303\n","Epoch [4/5], Step [438/1072], Loss: 0.3273\n","Epoch [4/5], Step [439/1072], Loss: 0.3628\n","Epoch [4/5], Step [440/1072], Loss: 0.3472\n","Epoch [4/5], Step [441/1072], Loss: 0.3264\n","Epoch [4/5], Step [442/1072], Loss: 0.3391\n","Epoch [4/5], Step [443/1072], Loss: 0.3320\n","Epoch [4/5], Step [444/1072], Loss: 0.3456\n","Epoch [4/5], Step [445/1072], Loss: 0.3450\n","Epoch [4/5], Step [446/1072], Loss: 0.3399\n","Epoch [4/5], Step [447/1072], Loss: 0.3220\n","Epoch [4/5], Step [448/1072], Loss: 0.3031\n","Epoch [4/5], Step [449/1072], Loss: 0.3400\n","Epoch [4/5], Step [450/1072], Loss: 0.3374\n","Epoch [4/5], Step [451/1072], Loss: 0.3317\n","Epoch [4/5], Step [452/1072], Loss: 0.3190\n","Epoch [4/5], Step [453/1072], Loss: 0.3317\n","Epoch [4/5], Step [454/1072], Loss: 0.3183\n","Epoch [4/5], Step [455/1072], Loss: 0.3770\n","Epoch [4/5], Step [456/1072], Loss: 0.3487\n","Epoch [4/5], Step [457/1072], Loss: 0.3495\n","Epoch [4/5], Step [458/1072], Loss: 0.3173\n","Epoch [4/5], Step [459/1072], Loss: 0.3330\n","Epoch [4/5], Step [460/1072], Loss: 0.3505\n","Epoch [4/5], Step [461/1072], Loss: 0.3273\n","Epoch [4/5], Step [462/1072], Loss: 0.3583\n","Epoch [4/5], Step [463/1072], Loss: 0.3474\n","Epoch [4/5], Step [464/1072], Loss: 0.3834\n","Epoch [4/5], Step [465/1072], Loss: 0.3274\n","Epoch [4/5], Step [466/1072], Loss: 0.3072\n","Epoch [4/5], Step [467/1072], Loss: 0.3387\n","Epoch [4/5], Step [468/1072], Loss: 0.3752\n","Epoch [4/5], Step [469/1072], Loss: 0.3540\n","Epoch [4/5], Step [470/1072], Loss: 0.3539\n","Epoch [4/5], Step [471/1072], Loss: 0.3283\n","Epoch [4/5], Step [472/1072], Loss: 0.3606\n","Epoch [4/5], Step [473/1072], Loss: 0.3163\n","Epoch [4/5], Step [474/1072], Loss: 0.3460\n","Epoch [4/5], Step [475/1072], Loss: 0.3358\n","Epoch [4/5], Step [476/1072], Loss: 0.3284\n","Epoch [4/5], Step [477/1072], Loss: 0.3492\n","Epoch [4/5], Step [478/1072], Loss: 0.3572\n","Epoch [4/5], Step [479/1072], Loss: 0.3391\n","Epoch [4/5], Step [480/1072], Loss: 0.3492\n","Epoch [4/5], Step [481/1072], Loss: 0.3644\n","Epoch [4/5], Step [482/1072], Loss: 0.3503\n","Epoch [4/5], Step [483/1072], Loss: 0.3378\n","Epoch [4/5], Step [484/1072], Loss: 0.3688\n","Epoch [4/5], Step [485/1072], Loss: 0.3663\n","Epoch [4/5], Step [486/1072], Loss: 0.3566\n","Epoch [4/5], Step [487/1072], Loss: 0.3295\n","Epoch [4/5], Step [488/1072], Loss: 0.3357\n","Epoch [4/5], Step [489/1072], Loss: 0.3198\n","Epoch [4/5], Step [490/1072], Loss: 0.3232\n","Epoch [4/5], Step [491/1072], Loss: 0.3150\n","Epoch [4/5], Step [492/1072], Loss: 0.3337\n","Epoch [4/5], Step [493/1072], Loss: 0.3482\n","Epoch [4/5], Step [494/1072], Loss: 0.3406\n","Epoch [4/5], Step [495/1072], Loss: 0.3647\n","Epoch [4/5], Step [496/1072], Loss: 0.3237\n","Epoch [4/5], Step [497/1072], Loss: 0.3395\n","Epoch [4/5], Step [498/1072], Loss: 0.3813\n","Epoch [4/5], Step [499/1072], Loss: 0.3249\n","Epoch [4/5], Step [500/1072], Loss: 0.3366\n","Epoch [4/5], Step [501/1072], Loss: 0.3496\n","Epoch [4/5], Step [502/1072], Loss: 0.3454\n","Epoch [4/5], Step [503/1072], Loss: 0.3314\n","Epoch [4/5], Step [504/1072], Loss: 0.3376\n","Epoch [4/5], Step [505/1072], Loss: 0.3358\n","Epoch [4/5], Step [506/1072], Loss: 0.3364\n","Epoch [4/5], Step [507/1072], Loss: 0.3339\n","Epoch [4/5], Step [508/1072], Loss: 0.3216\n","Epoch [4/5], Step [509/1072], Loss: 0.3360\n","Epoch [4/5], Step [510/1072], Loss: 0.3579\n","Epoch [4/5], Step [511/1072], Loss: 0.3599\n","Epoch [4/5], Step [512/1072], Loss: 0.3501\n","Epoch [4/5], Step [513/1072], Loss: 0.3545\n","Epoch [4/5], Step [514/1072], Loss: 0.3179\n","Epoch [4/5], Step [515/1072], Loss: 0.3349\n","Epoch [4/5], Step [516/1072], Loss: 0.3335\n","Epoch [4/5], Step [517/1072], Loss: 0.3721\n","Epoch [4/5], Step [518/1072], Loss: 0.3364\n","Epoch [4/5], Step [519/1072], Loss: 0.3476\n","Epoch [4/5], Step [520/1072], Loss: 0.3361\n","Epoch [4/5], Step [521/1072], Loss: 0.3607\n","Epoch [4/5], Step [522/1072], Loss: 0.3217\n","Epoch [4/5], Step [523/1072], Loss: 0.3893\n","Epoch [4/5], Step [524/1072], Loss: 0.3493\n","Epoch [4/5], Step [525/1072], Loss: 0.3704\n","Epoch [4/5], Step [526/1072], Loss: 0.3338\n","Epoch [4/5], Step [527/1072], Loss: 0.3305\n","Epoch [4/5], Step [528/1072], Loss: 0.3262\n","Epoch [4/5], Step [529/1072], Loss: 0.3242\n","Epoch [4/5], Step [530/1072], Loss: 0.3400\n","Epoch [4/5], Step [531/1072], Loss: 0.3283\n","Epoch [4/5], Step [532/1072], Loss: 0.3298\n","Epoch [4/5], Step [533/1072], Loss: 0.3213\n","Epoch [4/5], Step [534/1072], Loss: 0.3277\n","Epoch [4/5], Step [535/1072], Loss: 0.3043\n","Epoch [4/5], Step [536/1072], Loss: 0.3630\n","Epoch [4/5], Step [537/1072], Loss: 0.3278\n","Epoch [4/5], Step [538/1072], Loss: 0.3528\n","Epoch [4/5], Step [539/1072], Loss: 0.3364\n","Epoch [4/5], Step [540/1072], Loss: 0.3327\n","Epoch [4/5], Step [541/1072], Loss: 0.3331\n","Epoch [4/5], Step [542/1072], Loss: 0.3414\n","Epoch [4/5], Step [543/1072], Loss: 0.3501\n","Epoch [4/5], Step [544/1072], Loss: 0.3718\n","Epoch [4/5], Step [545/1072], Loss: 0.3378\n","Epoch [4/5], Step [546/1072], Loss: 0.3447\n","Epoch [4/5], Step [547/1072], Loss: 0.3302\n","Epoch [4/5], Step [548/1072], Loss: 0.3410\n","Epoch [4/5], Step [549/1072], Loss: 0.3278\n","Epoch [4/5], Step [550/1072], Loss: 0.3658\n","Epoch [4/5], Step [551/1072], Loss: 0.3311\n","Epoch [4/5], Step [552/1072], Loss: 0.3455\n","Epoch [4/5], Step [553/1072], Loss: 0.3420\n","Epoch [4/5], Step [554/1072], Loss: 0.3358\n","Epoch [4/5], Step [555/1072], Loss: 0.3597\n","Epoch [4/5], Step [556/1072], Loss: 0.3376\n","Epoch [4/5], Step [557/1072], Loss: 0.3250\n","Epoch [4/5], Step [558/1072], Loss: 0.3639\n","Epoch [4/5], Step [559/1072], Loss: 0.3260\n","Epoch [4/5], Step [560/1072], Loss: 0.3422\n","Epoch [4/5], Step [561/1072], Loss: 0.3599\n","Epoch [4/5], Step [562/1072], Loss: 0.3577\n","Epoch [4/5], Step [563/1072], Loss: 0.3294\n","Epoch [4/5], Step [564/1072], Loss: 0.3412\n","Epoch [4/5], Step [565/1072], Loss: 0.3660\n","Epoch [4/5], Step [566/1072], Loss: 0.3439\n","Epoch [4/5], Step [567/1072], Loss: 0.3553\n","Epoch [4/5], Step [568/1072], Loss: 0.3238\n","Epoch [4/5], Step [569/1072], Loss: 0.2855\n","Epoch [4/5], Step [570/1072], Loss: 0.3483\n","Epoch [4/5], Step [571/1072], Loss: 0.3654\n","Epoch [4/5], Step [572/1072], Loss: 0.3205\n","Epoch [4/5], Step [573/1072], Loss: 0.3667\n","Epoch [4/5], Step [574/1072], Loss: 0.3348\n","Epoch [4/5], Step [575/1072], Loss: 0.3312\n","Epoch [4/5], Step [576/1072], Loss: 0.3455\n","Epoch [4/5], Step [577/1072], Loss: 0.3399\n","Epoch [4/5], Step [578/1072], Loss: 0.3489\n","Epoch [4/5], Step [579/1072], Loss: 0.3111\n","Epoch [4/5], Step [580/1072], Loss: 0.3412\n","Epoch [4/5], Step [581/1072], Loss: 0.3285\n","Epoch [4/5], Step [582/1072], Loss: 0.3417\n","Epoch [4/5], Step [583/1072], Loss: 0.3342\n","Epoch [4/5], Step [584/1072], Loss: 0.3412\n","Epoch [4/5], Step [585/1072], Loss: 0.3546\n","Epoch [4/5], Step [586/1072], Loss: 0.3341\n","Epoch [4/5], Step [587/1072], Loss: 0.3570\n","Epoch [4/5], Step [588/1072], Loss: 0.3382\n","Epoch [4/5], Step [589/1072], Loss: 0.3381\n","Epoch [4/5], Step [590/1072], Loss: 0.3194\n","Epoch [4/5], Step [591/1072], Loss: 0.3455\n","Epoch [4/5], Step [592/1072], Loss: 0.3439\n","Epoch [4/5], Step [593/1072], Loss: 0.3604\n","Epoch [4/5], Step [594/1072], Loss: 0.3466\n","Epoch [4/5], Step [595/1072], Loss: 0.3343\n","Epoch [4/5], Step [596/1072], Loss: 0.3542\n","Epoch [4/5], Step [597/1072], Loss: 0.3181\n","Epoch [4/5], Step [598/1072], Loss: 0.3353\n","Epoch [4/5], Step [599/1072], Loss: 0.3418\n","Epoch [4/5], Step [600/1072], Loss: 0.3384\n","Epoch [4/5], Step [601/1072], Loss: 0.3496\n","Epoch [4/5], Step [602/1072], Loss: 0.3192\n","Epoch [4/5], Step [603/1072], Loss: 0.3140\n","Epoch [4/5], Step [604/1072], Loss: 0.3399\n","Epoch [4/5], Step [605/1072], Loss: 0.3291\n","Epoch [4/5], Step [606/1072], Loss: 0.3430\n","Epoch [4/5], Step [607/1072], Loss: 0.3406\n","Epoch [4/5], Step [608/1072], Loss: 0.3287\n","Epoch [4/5], Step [609/1072], Loss: 0.3568\n","Epoch [4/5], Step [610/1072], Loss: 0.3567\n","Epoch [4/5], Step [611/1072], Loss: 0.3406\n","Epoch [4/5], Step [612/1072], Loss: 0.3387\n","Epoch [4/5], Step [613/1072], Loss: 0.3461\n","Epoch [4/5], Step [614/1072], Loss: 0.3104\n","Epoch [4/5], Step [615/1072], Loss: 0.3591\n","Epoch [4/5], Step [616/1072], Loss: 0.3214\n","Epoch [4/5], Step [617/1072], Loss: 0.3635\n","Epoch [4/5], Step [618/1072], Loss: 0.3353\n","Epoch [4/5], Step [619/1072], Loss: 0.3562\n","Epoch [4/5], Step [620/1072], Loss: 0.3419\n","Epoch [4/5], Step [621/1072], Loss: 0.3499\n","Epoch [4/5], Step [622/1072], Loss: 0.3335\n","Epoch [4/5], Step [623/1072], Loss: 0.3381\n","Epoch [4/5], Step [624/1072], Loss: 0.3316\n","Epoch [4/5], Step [625/1072], Loss: 0.3403\n","Epoch [4/5], Step [626/1072], Loss: 0.3276\n","Epoch [4/5], Step [627/1072], Loss: 0.3560\n","Epoch [4/5], Step [628/1072], Loss: 0.3535\n","Epoch [4/5], Step [629/1072], Loss: 0.3479\n","Epoch [4/5], Step [630/1072], Loss: 0.3454\n","Epoch [4/5], Step [631/1072], Loss: 0.3353\n","Epoch [4/5], Step [632/1072], Loss: 0.3193\n","Epoch [4/5], Step [633/1072], Loss: 0.3355\n","Epoch [4/5], Step [634/1072], Loss: 0.3547\n","Epoch [4/5], Step [635/1072], Loss: 0.3587\n","Epoch [4/5], Step [636/1072], Loss: 0.3416\n","Epoch [4/5], Step [637/1072], Loss: 0.3618\n","Epoch [4/5], Step [638/1072], Loss: 0.3527\n","Epoch [4/5], Step [639/1072], Loss: 0.3294\n","Epoch [4/5], Step [640/1072], Loss: 0.3233\n","Epoch [4/5], Step [641/1072], Loss: 0.3380\n","Epoch [4/5], Step [642/1072], Loss: 0.3187\n","Epoch [4/5], Step [643/1072], Loss: 0.3618\n","Epoch [4/5], Step [644/1072], Loss: 0.3589\n","Epoch [4/5], Step [645/1072], Loss: 0.3668\n","Epoch [4/5], Step [646/1072], Loss: 0.3232\n","Epoch [4/5], Step [647/1072], Loss: 0.3242\n","Epoch [4/5], Step [648/1072], Loss: 0.3764\n","Epoch [4/5], Step [649/1072], Loss: 0.3445\n","Epoch [4/5], Step [650/1072], Loss: 0.3390\n","Epoch [4/5], Step [651/1072], Loss: 0.3370\n","Epoch [4/5], Step [652/1072], Loss: 0.3262\n","Epoch [4/5], Step [653/1072], Loss: 0.3428\n","Epoch [4/5], Step [654/1072], Loss: 0.3323\n","Epoch [4/5], Step [655/1072], Loss: 0.3563\n","Epoch [4/5], Step [656/1072], Loss: 0.3246\n","Epoch [4/5], Step [657/1072], Loss: 0.3146\n","Epoch [4/5], Step [658/1072], Loss: 0.3152\n","Epoch [4/5], Step [659/1072], Loss: 0.3401\n","Epoch [4/5], Step [660/1072], Loss: 0.3514\n","Epoch [4/5], Step [661/1072], Loss: 0.3260\n","Epoch [4/5], Step [662/1072], Loss: 0.3342\n","Epoch [4/5], Step [663/1072], Loss: 0.3282\n","Epoch [4/5], Step [664/1072], Loss: 0.3479\n","Epoch [4/5], Step [665/1072], Loss: 0.3388\n","Epoch [4/5], Step [666/1072], Loss: 0.3480\n","Epoch [4/5], Step [667/1072], Loss: 0.3262\n","Epoch [4/5], Step [668/1072], Loss: 0.3516\n","Epoch [4/5], Step [669/1072], Loss: 0.3503\n","Epoch [4/5], Step [670/1072], Loss: 0.3221\n","Epoch [4/5], Step [671/1072], Loss: 0.3267\n","Epoch [4/5], Step [672/1072], Loss: 0.3815\n","Epoch [4/5], Step [673/1072], Loss: 0.3224\n","Epoch [4/5], Step [674/1072], Loss: 0.3667\n","Epoch [4/5], Step [675/1072], Loss: 0.3482\n","Epoch [4/5], Step [676/1072], Loss: 0.3008\n","Epoch [4/5], Step [677/1072], Loss: 0.3307\n","Epoch [4/5], Step [678/1072], Loss: 0.3211\n","Epoch [4/5], Step [679/1072], Loss: 0.3218\n","Epoch [4/5], Step [680/1072], Loss: 0.3415\n","Epoch [4/5], Step [681/1072], Loss: 0.3145\n","Epoch [4/5], Step [682/1072], Loss: 0.3523\n","Epoch [4/5], Step [683/1072], Loss: 0.3226\n","Epoch [4/5], Step [684/1072], Loss: 0.3277\n","Epoch [4/5], Step [685/1072], Loss: 0.3286\n","Epoch [4/5], Step [686/1072], Loss: 0.3274\n","Epoch [4/5], Step [687/1072], Loss: 0.3434\n","Epoch [4/5], Step [688/1072], Loss: 0.3335\n","Epoch [4/5], Step [689/1072], Loss: 0.3319\n","Epoch [4/5], Step [690/1072], Loss: 0.3455\n","Epoch [4/5], Step [691/1072], Loss: 0.3762\n","Epoch [4/5], Step [692/1072], Loss: 0.3311\n","Epoch [4/5], Step [693/1072], Loss: 0.3637\n","Epoch [4/5], Step [694/1072], Loss: 0.3486\n","Epoch [4/5], Step [695/1072], Loss: 0.3159\n","Epoch [4/5], Step [696/1072], Loss: 0.3439\n","Epoch [4/5], Step [697/1072], Loss: 0.3470\n","Epoch [4/5], Step [698/1072], Loss: 0.3520\n","Epoch [4/5], Step [699/1072], Loss: 0.3649\n","Epoch [4/5], Step [700/1072], Loss: 0.3361\n","Epoch [4/5], Step [701/1072], Loss: 0.3156\n","Epoch [4/5], Step [702/1072], Loss: 0.3315\n","Epoch [4/5], Step [703/1072], Loss: 0.3221\n","Epoch [4/5], Step [704/1072], Loss: 0.3354\n","Epoch [4/5], Step [705/1072], Loss: 0.3610\n","Epoch [4/5], Step [706/1072], Loss: 0.3276\n","Epoch [4/5], Step [707/1072], Loss: 0.3376\n","Epoch [4/5], Step [708/1072], Loss: 0.3258\n","Epoch [4/5], Step [709/1072], Loss: 0.3609\n","Epoch [4/5], Step [710/1072], Loss: 0.3632\n","Epoch [4/5], Step [711/1072], Loss: 0.3336\n","Epoch [4/5], Step [712/1072], Loss: 0.3274\n","Epoch [4/5], Step [713/1072], Loss: 0.3561\n","Epoch [4/5], Step [714/1072], Loss: 0.3177\n","Epoch [4/5], Step [715/1072], Loss: 0.3330\n","Epoch [4/5], Step [716/1072], Loss: 0.3543\n","Epoch [4/5], Step [717/1072], Loss: 0.3352\n","Epoch [4/5], Step [718/1072], Loss: 0.3916\n","Epoch [4/5], Step [719/1072], Loss: 0.3410\n","Epoch [4/5], Step [720/1072], Loss: 0.3370\n","Epoch [4/5], Step [721/1072], Loss: 0.3528\n","Epoch [4/5], Step [722/1072], Loss: 0.3432\n","Epoch [4/5], Step [723/1072], Loss: 0.3374\n","Epoch [4/5], Step [724/1072], Loss: 0.3558\n","Epoch [4/5], Step [725/1072], Loss: 0.3562\n","Epoch [4/5], Step [726/1072], Loss: 0.3343\n","Epoch [4/5], Step [727/1072], Loss: 0.3500\n","Epoch [4/5], Step [728/1072], Loss: 0.3367\n","Epoch [4/5], Step [729/1072], Loss: 0.3273\n","Epoch [4/5], Step [730/1072], Loss: 0.3664\n","Epoch [4/5], Step [731/1072], Loss: 0.3319\n","Epoch [4/5], Step [732/1072], Loss: 0.3432\n","Epoch [4/5], Step [733/1072], Loss: 0.3848\n","Epoch [4/5], Step [734/1072], Loss: 0.3361\n","Epoch [4/5], Step [735/1072], Loss: 0.3190\n","Epoch [4/5], Step [736/1072], Loss: 0.3305\n","Epoch [4/5], Step [737/1072], Loss: 0.3373\n","Epoch [4/5], Step [738/1072], Loss: 0.3455\n","Epoch [4/5], Step [739/1072], Loss: 0.3216\n","Epoch [4/5], Step [740/1072], Loss: 0.3700\n","Epoch [4/5], Step [741/1072], Loss: 0.3501\n","Epoch [4/5], Step [742/1072], Loss: 0.3376\n","Epoch [4/5], Step [743/1072], Loss: 0.3567\n","Epoch [4/5], Step [744/1072], Loss: 0.3452\n","Epoch [4/5], Step [745/1072], Loss: 0.3566\n","Epoch [4/5], Step [746/1072], Loss: 0.3435\n","Epoch [4/5], Step [747/1072], Loss: 0.3335\n","Epoch [4/5], Step [748/1072], Loss: 0.3426\n","Epoch [4/5], Step [749/1072], Loss: 0.3449\n","Epoch [4/5], Step [750/1072], Loss: 0.3348\n","Epoch [4/5], Step [751/1072], Loss: 0.3422\n","Epoch [4/5], Step [752/1072], Loss: 0.3384\n","Epoch [4/5], Step [753/1072], Loss: 0.3263\n","Epoch [4/5], Step [754/1072], Loss: 0.3361\n","Epoch [4/5], Step [755/1072], Loss: 0.3323\n","Epoch [4/5], Step [756/1072], Loss: 0.3195\n","Epoch [4/5], Step [757/1072], Loss: 0.3165\n","Epoch [4/5], Step [758/1072], Loss: 0.3255\n","Epoch [4/5], Step [759/1072], Loss: 0.3328\n","Epoch [4/5], Step [760/1072], Loss: 0.3435\n","Epoch [4/5], Step [761/1072], Loss: 0.3298\n","Epoch [4/5], Step [762/1072], Loss: 0.3503\n","Epoch [4/5], Step [763/1072], Loss: 0.3262\n","Epoch [4/5], Step [764/1072], Loss: 0.3328\n","Epoch [4/5], Step [765/1072], Loss: 0.3357\n","Epoch [4/5], Step [766/1072], Loss: 0.3356\n","Epoch [4/5], Step [767/1072], Loss: 0.3688\n","Epoch [4/5], Step [768/1072], Loss: 0.3855\n","Epoch [4/5], Step [769/1072], Loss: 0.3042\n","Epoch [4/5], Step [770/1072], Loss: 0.3348\n","Epoch [4/5], Step [771/1072], Loss: 0.3252\n","Epoch [4/5], Step [772/1072], Loss: 0.3657\n","Epoch [4/5], Step [773/1072], Loss: 0.3465\n","Epoch [4/5], Step [774/1072], Loss: 0.3519\n","Epoch [4/5], Step [775/1072], Loss: 0.3248\n","Epoch [4/5], Step [776/1072], Loss: 0.3365\n","Epoch [4/5], Step [777/1072], Loss: 0.3219\n","Epoch [4/5], Step [778/1072], Loss: 0.3467\n","Epoch [4/5], Step [779/1072], Loss: 0.3389\n","Epoch [4/5], Step [780/1072], Loss: 0.3617\n","Epoch [4/5], Step [781/1072], Loss: 0.3688\n","Epoch [4/5], Step [782/1072], Loss: 0.3422\n","Epoch [4/5], Step [783/1072], Loss: 0.3355\n","Epoch [4/5], Step [784/1072], Loss: 0.3489\n","Epoch [4/5], Step [785/1072], Loss: 0.3460\n","Epoch [4/5], Step [786/1072], Loss: 0.3488\n","Epoch [4/5], Step [787/1072], Loss: 0.3486\n","Epoch [4/5], Step [788/1072], Loss: 0.3269\n","Epoch [4/5], Step [789/1072], Loss: 0.3227\n","Epoch [4/5], Step [790/1072], Loss: 0.3421\n","Epoch [4/5], Step [791/1072], Loss: 0.3352\n","Epoch [4/5], Step [792/1072], Loss: 0.3344\n","Epoch [4/5], Step [793/1072], Loss: 0.3345\n","Epoch [4/5], Step [794/1072], Loss: 0.3628\n","Epoch [4/5], Step [795/1072], Loss: 0.3488\n","Epoch [4/5], Step [796/1072], Loss: 0.3526\n","Epoch [4/5], Step [797/1072], Loss: 0.3343\n","Epoch [4/5], Step [798/1072], Loss: 0.3561\n","Epoch [4/5], Step [799/1072], Loss: 0.3360\n","Epoch [4/5], Step [800/1072], Loss: 0.3442\n","Epoch [4/5], Step [801/1072], Loss: 0.3384\n","Epoch [4/5], Step [802/1072], Loss: 0.3580\n","Epoch [4/5], Step [803/1072], Loss: 0.3236\n","Epoch [4/5], Step [804/1072], Loss: 0.3630\n","Epoch [4/5], Step [805/1072], Loss: 0.3812\n","Epoch [4/5], Step [806/1072], Loss: 0.3370\n","Epoch [4/5], Step [807/1072], Loss: 0.3187\n","Epoch [4/5], Step [808/1072], Loss: 0.3098\n","Epoch [4/5], Step [809/1072], Loss: 0.3196\n","Epoch [4/5], Step [810/1072], Loss: 0.3290\n","Epoch [4/5], Step [811/1072], Loss: 0.3433\n","Epoch [4/5], Step [812/1072], Loss: 0.3390\n","Epoch [4/5], Step [813/1072], Loss: 0.3535\n","Epoch [4/5], Step [814/1072], Loss: 0.3258\n","Epoch [4/5], Step [815/1072], Loss: 0.3596\n","Epoch [4/5], Step [816/1072], Loss: 0.3193\n","Epoch [4/5], Step [817/1072], Loss: 0.3779\n","Epoch [4/5], Step [818/1072], Loss: 0.3225\n","Epoch [4/5], Step [819/1072], Loss: 0.3408\n","Epoch [4/5], Step [820/1072], Loss: 0.3357\n","Epoch [4/5], Step [821/1072], Loss: 0.3399\n","Epoch [4/5], Step [822/1072], Loss: 0.3233\n","Epoch [4/5], Step [823/1072], Loss: 0.3280\n","Epoch [4/5], Step [824/1072], Loss: 0.3201\n","Epoch [4/5], Step [825/1072], Loss: 0.3368\n","Epoch [4/5], Step [826/1072], Loss: 0.3416\n","Epoch [4/5], Step [827/1072], Loss: 0.3337\n","Epoch [4/5], Step [828/1072], Loss: 0.3121\n","Epoch [4/5], Step [829/1072], Loss: 0.3297\n","Epoch [4/5], Step [830/1072], Loss: 0.3520\n","Epoch [4/5], Step [831/1072], Loss: 0.3268\n","Epoch [4/5], Step [832/1072], Loss: 0.2861\n","Epoch [4/5], Step [833/1072], Loss: 0.3497\n","Epoch [4/5], Step [834/1072], Loss: 0.3601\n","Epoch [4/5], Step [835/1072], Loss: 0.3451\n","Epoch [4/5], Step [836/1072], Loss: 0.3667\n","Epoch [4/5], Step [837/1072], Loss: 0.3392\n","Epoch [4/5], Step [838/1072], Loss: 0.3339\n","Epoch [4/5], Step [839/1072], Loss: 0.3279\n","Epoch [4/5], Step [840/1072], Loss: 0.3301\n","Epoch [4/5], Step [841/1072], Loss: 0.3334\n","Epoch [4/5], Step [842/1072], Loss: 0.3656\n","Epoch [4/5], Step [843/1072], Loss: 0.3330\n","Epoch [4/5], Step [844/1072], Loss: 0.3662\n","Epoch [4/5], Step [845/1072], Loss: 0.3390\n","Epoch [4/5], Step [846/1072], Loss: 0.3501\n","Epoch [4/5], Step [847/1072], Loss: 0.3189\n","Epoch [4/5], Step [848/1072], Loss: 0.3338\n","Epoch [4/5], Step [849/1072], Loss: 0.3272\n","Epoch [4/5], Step [850/1072], Loss: 0.3469\n","Epoch [4/5], Step [851/1072], Loss: 0.3448\n","Epoch [4/5], Step [852/1072], Loss: 0.3335\n","Epoch [4/5], Step [853/1072], Loss: 0.3363\n","Epoch [4/5], Step [854/1072], Loss: 0.3259\n","Epoch [4/5], Step [855/1072], Loss: 0.3505\n","Epoch [4/5], Step [856/1072], Loss: 0.3530\n","Epoch [4/5], Step [857/1072], Loss: 0.3368\n","Epoch [4/5], Step [858/1072], Loss: 0.3299\n","Epoch [4/5], Step [859/1072], Loss: 0.3769\n","Epoch [4/5], Step [860/1072], Loss: 0.3529\n","Epoch [4/5], Step [861/1072], Loss: 0.3372\n","Epoch [4/5], Step [862/1072], Loss: 0.3111\n","Epoch [4/5], Step [863/1072], Loss: 0.3603\n","Epoch [4/5], Step [864/1072], Loss: 0.3421\n","Epoch [4/5], Step [865/1072], Loss: 0.3165\n","Epoch [4/5], Step [866/1072], Loss: 0.3048\n","Epoch [4/5], Step [867/1072], Loss: 0.3572\n","Epoch [4/5], Step [868/1072], Loss: 0.3401\n","Epoch [4/5], Step [869/1072], Loss: 0.3599\n","Epoch [4/5], Step [870/1072], Loss: 0.3597\n","Epoch [4/5], Step [871/1072], Loss: 0.3458\n","Epoch [4/5], Step [872/1072], Loss: 0.3290\n","Epoch [4/5], Step [873/1072], Loss: 0.3292\n","Epoch [4/5], Step [874/1072], Loss: 0.3246\n","Epoch [4/5], Step [875/1072], Loss: 0.3604\n","Epoch [4/5], Step [876/1072], Loss: 0.3587\n","Epoch [4/5], Step [877/1072], Loss: 0.3123\n","Epoch [4/5], Step [878/1072], Loss: 0.3345\n","Epoch [4/5], Step [879/1072], Loss: 0.3382\n","Epoch [4/5], Step [880/1072], Loss: 0.3322\n","Epoch [4/5], Step [881/1072], Loss: 0.3662\n","Epoch [4/5], Step [882/1072], Loss: 0.3389\n","Epoch [4/5], Step [883/1072], Loss: 0.3203\n","Epoch [4/5], Step [884/1072], Loss: 0.3484\n","Epoch [4/5], Step [885/1072], Loss: 0.3435\n","Epoch [4/5], Step [886/1072], Loss: 0.3567\n","Epoch [4/5], Step [887/1072], Loss: 0.3521\n","Epoch [4/5], Step [888/1072], Loss: 0.3171\n","Epoch [4/5], Step [889/1072], Loss: 0.3278\n","Epoch [4/5], Step [890/1072], Loss: 0.3527\n","Epoch [4/5], Step [891/1072], Loss: 0.3493\n","Epoch [4/5], Step [892/1072], Loss: 0.3301\n","Epoch [4/5], Step [893/1072], Loss: 0.3646\n","Epoch [4/5], Step [894/1072], Loss: 0.3428\n","Epoch [4/5], Step [895/1072], Loss: 0.3848\n","Epoch [4/5], Step [896/1072], Loss: 0.3256\n","Epoch [4/5], Step [897/1072], Loss: 0.3261\n","Epoch [4/5], Step [898/1072], Loss: 0.3313\n","Epoch [4/5], Step [899/1072], Loss: 0.3462\n","Epoch [4/5], Step [900/1072], Loss: 0.3446\n","Epoch [4/5], Step [901/1072], Loss: 0.3532\n","Epoch [4/5], Step [902/1072], Loss: 0.3444\n","Epoch [4/5], Step [903/1072], Loss: 0.3866\n","Epoch [4/5], Step [904/1072], Loss: 0.3302\n","Epoch [4/5], Step [905/1072], Loss: 0.3143\n","Epoch [4/5], Step [906/1072], Loss: 0.3510\n","Epoch [4/5], Step [907/1072], Loss: 0.3487\n","Epoch [4/5], Step [908/1072], Loss: 0.3254\n","Epoch [4/5], Step [909/1072], Loss: 0.3755\n","Epoch [4/5], Step [910/1072], Loss: 0.3455\n","Epoch [4/5], Step [911/1072], Loss: 0.3690\n","Epoch [4/5], Step [912/1072], Loss: 0.3415\n","Epoch [4/5], Step [913/1072], Loss: 0.3168\n","Epoch [4/5], Step [914/1072], Loss: 0.3299\n","Epoch [4/5], Step [915/1072], Loss: 0.3560\n","Epoch [4/5], Step [916/1072], Loss: 0.3376\n","Epoch [4/5], Step [917/1072], Loss: 0.3427\n","Epoch [4/5], Step [918/1072], Loss: 0.3296\n","Epoch [4/5], Step [919/1072], Loss: 0.3684\n","Epoch [4/5], Step [920/1072], Loss: 0.3260\n","Epoch [4/5], Step [921/1072], Loss: 0.3624\n","Epoch [4/5], Step [922/1072], Loss: 0.3203\n","Epoch [4/5], Step [923/1072], Loss: 0.3074\n","Epoch [4/5], Step [924/1072], Loss: 0.3322\n","Epoch [4/5], Step [925/1072], Loss: 0.3402\n","Epoch [4/5], Step [926/1072], Loss: 0.3383\n","Epoch [4/5], Step [927/1072], Loss: 0.3536\n","Epoch [4/5], Step [928/1072], Loss: 0.3270\n","Epoch [4/5], Step [929/1072], Loss: 0.3651\n","Epoch [4/5], Step [930/1072], Loss: 0.3378\n","Epoch [4/5], Step [931/1072], Loss: 0.3418\n","Epoch [4/5], Step [932/1072], Loss: 0.3282\n","Epoch [4/5], Step [933/1072], Loss: 0.3258\n","Epoch [4/5], Step [934/1072], Loss: 0.3273\n","Epoch [4/5], Step [935/1072], Loss: 0.3187\n","Epoch [4/5], Step [936/1072], Loss: 0.3227\n","Epoch [4/5], Step [937/1072], Loss: 0.3585\n","Epoch [4/5], Step [938/1072], Loss: 0.3252\n","Epoch [4/5], Step [939/1072], Loss: 0.3315\n","Epoch [4/5], Step [940/1072], Loss: 0.3203\n","Epoch [4/5], Step [941/1072], Loss: 0.3348\n","Epoch [4/5], Step [942/1072], Loss: 0.3301\n","Epoch [4/5], Step [943/1072], Loss: 0.3392\n","Epoch [4/5], Step [944/1072], Loss: 0.3576\n","Epoch [4/5], Step [945/1072], Loss: 0.3251\n","Epoch [4/5], Step [946/1072], Loss: 0.3300\n","Epoch [4/5], Step [947/1072], Loss: 0.3183\n","Epoch [4/5], Step [948/1072], Loss: 0.3347\n","Epoch [4/5], Step [949/1072], Loss: 0.3218\n","Epoch [4/5], Step [950/1072], Loss: 0.3595\n","Epoch [4/5], Step [951/1072], Loss: 0.3432\n","Epoch [4/5], Step [952/1072], Loss: 0.3486\n","Epoch [4/5], Step [953/1072], Loss: 0.3603\n","Epoch [4/5], Step [954/1072], Loss: 0.3544\n","Epoch [4/5], Step [955/1072], Loss: 0.3415\n","Epoch [4/5], Step [956/1072], Loss: 0.3022\n","Epoch [4/5], Step [957/1072], Loss: 0.3329\n","Epoch [4/5], Step [958/1072], Loss: 0.3456\n","Epoch [4/5], Step [959/1072], Loss: 0.3509\n","Epoch [4/5], Step [960/1072], Loss: 0.3513\n","Epoch [4/5], Step [961/1072], Loss: 0.3064\n","Epoch [4/5], Step [962/1072], Loss: 0.3343\n","Epoch [4/5], Step [963/1072], Loss: 0.3397\n","Epoch [4/5], Step [964/1072], Loss: 0.3180\n","Epoch [4/5], Step [965/1072], Loss: 0.3208\n","Epoch [4/5], Step [966/1072], Loss: 0.3378\n","Epoch [4/5], Step [967/1072], Loss: 0.3332\n","Epoch [4/5], Step [968/1072], Loss: 0.3610\n","Epoch [4/5], Step [969/1072], Loss: 0.3340\n","Epoch [4/5], Step [970/1072], Loss: 0.3637\n","Epoch [4/5], Step [971/1072], Loss: 0.3215\n","Epoch [4/5], Step [972/1072], Loss: 0.3829\n","Epoch [4/5], Step [973/1072], Loss: 0.3376\n","Epoch [4/5], Step [974/1072], Loss: 0.3281\n","Epoch [4/5], Step [975/1072], Loss: 0.3524\n","Epoch [4/5], Step [976/1072], Loss: 0.3350\n","Epoch [4/5], Step [977/1072], Loss: 0.3260\n","Epoch [4/5], Step [978/1072], Loss: 0.3553\n","Epoch [4/5], Step [979/1072], Loss: 0.3365\n","Epoch [4/5], Step [980/1072], Loss: 0.3579\n","Epoch [4/5], Step [981/1072], Loss: 0.3490\n","Epoch [4/5], Step [982/1072], Loss: 0.3281\n","Epoch [4/5], Step [983/1072], Loss: 0.3548\n","Epoch [4/5], Step [984/1072], Loss: 0.3076\n","Epoch [4/5], Step [985/1072], Loss: 0.3490\n","Epoch [4/5], Step [986/1072], Loss: 0.3549\n","Epoch [4/5], Step [987/1072], Loss: 0.3565\n","Epoch [4/5], Step [988/1072], Loss: 0.3383\n","Epoch [4/5], Step [989/1072], Loss: 0.3334\n","Epoch [4/5], Step [990/1072], Loss: 0.3565\n","Epoch [4/5], Step [991/1072], Loss: 0.3417\n","Epoch [4/5], Step [992/1072], Loss: 0.3382\n","Epoch [4/5], Step [993/1072], Loss: 0.3245\n","Epoch [4/5], Step [994/1072], Loss: 0.3320\n","Epoch [4/5], Step [995/1072], Loss: 0.3370\n","Epoch [4/5], Step [996/1072], Loss: 0.3448\n","Epoch [4/5], Step [997/1072], Loss: 0.3227\n","Epoch [4/5], Step [998/1072], Loss: 0.3351\n","Epoch [4/5], Step [999/1072], Loss: 0.3261\n","Epoch [4/5], Step [1000/1072], Loss: 0.3210\n","Epoch [4/5], Step [1001/1072], Loss: 0.3224\n","Epoch [4/5], Step [1002/1072], Loss: 0.3326\n","Epoch [4/5], Step [1003/1072], Loss: 0.3304\n","Epoch [4/5], Step [1004/1072], Loss: 0.3354\n","Epoch [4/5], Step [1005/1072], Loss: 0.3670\n","Epoch [4/5], Step [1006/1072], Loss: 0.3655\n","Epoch [4/5], Step [1007/1072], Loss: 0.3411\n","Epoch [4/5], Step [1008/1072], Loss: 0.3105\n","Epoch [4/5], Step [1009/1072], Loss: 0.3536\n","Epoch [4/5], Step [1010/1072], Loss: 0.3644\n","Epoch [4/5], Step [1011/1072], Loss: 0.3188\n","Epoch [4/5], Step [1012/1072], Loss: 0.3442\n","Epoch [4/5], Step [1013/1072], Loss: 0.3193\n","Epoch [4/5], Step [1014/1072], Loss: 0.3425\n","Epoch [4/5], Step [1015/1072], Loss: 0.3223\n","Epoch [4/5], Step [1016/1072], Loss: 0.3166\n","Epoch [4/5], Step [1017/1072], Loss: 0.3413\n","Epoch [4/5], Step [1018/1072], Loss: 0.3194\n","Epoch [4/5], Step [1019/1072], Loss: 0.3387\n","Epoch [4/5], Step [1020/1072], Loss: 0.3365\n","Epoch [4/5], Step [1021/1072], Loss: 0.3224\n","Epoch [4/5], Step [1022/1072], Loss: 0.3395\n","Epoch [4/5], Step [1023/1072], Loss: 0.3779\n","Epoch [4/5], Step [1024/1072], Loss: 0.3487\n","Epoch [4/5], Step [1025/1072], Loss: 0.3590\n","Epoch [4/5], Step [1026/1072], Loss: 0.3605\n","Epoch [4/5], Step [1027/1072], Loss: 0.3237\n","Epoch [4/5], Step [1028/1072], Loss: 0.3177\n","Epoch [4/5], Step [1029/1072], Loss: 0.3234\n","Epoch [4/5], Step [1030/1072], Loss: 0.3507\n","Epoch [4/5], Step [1031/1072], Loss: 0.3388\n","Epoch [4/5], Step [1032/1072], Loss: 0.3299\n","Epoch [4/5], Step [1033/1072], Loss: 0.3274\n","Epoch [4/5], Step [1034/1072], Loss: 0.3457\n","Epoch [4/5], Step [1035/1072], Loss: 0.3456\n","Epoch [4/5], Step [1036/1072], Loss: 0.3573\n","Epoch [4/5], Step [1037/1072], Loss: 0.3559\n","Epoch [4/5], Step [1038/1072], Loss: 0.3210\n","Epoch [4/5], Step [1039/1072], Loss: 0.3058\n","Epoch [4/5], Step [1040/1072], Loss: 0.3647\n","Epoch [4/5], Step [1041/1072], Loss: 0.3547\n","Epoch [4/5], Step [1042/1072], Loss: 0.2968\n","Epoch [4/5], Step [1043/1072], Loss: 0.3717\n","Epoch [4/5], Step [1044/1072], Loss: 0.3560\n","Epoch [4/5], Step [1045/1072], Loss: 0.3589\n","Epoch [4/5], Step [1046/1072], Loss: 0.3458\n","Epoch [4/5], Step [1047/1072], Loss: 0.3480\n","Epoch [4/5], Step [1048/1072], Loss: 0.3381\n","Epoch [4/5], Step [1049/1072], Loss: 0.3545\n","Epoch [4/5], Step [1050/1072], Loss: 0.3405\n","Epoch [4/5], Step [1051/1072], Loss: 0.3367\n","Epoch [4/5], Step [1052/1072], Loss: 0.3438\n","Epoch [4/5], Step [1053/1072], Loss: 0.3401\n","Epoch [4/5], Step [1054/1072], Loss: 0.3506\n","Epoch [4/5], Step [1055/1072], Loss: 0.3483\n","Epoch [4/5], Step [1056/1072], Loss: 0.3454\n","Epoch [4/5], Step [1057/1072], Loss: 0.3446\n","Epoch [4/5], Step [1058/1072], Loss: 0.3327\n","Epoch [4/5], Step [1059/1072], Loss: 0.3357\n","Epoch [4/5], Step [1060/1072], Loss: 0.3748\n","Epoch [4/5], Step [1061/1072], Loss: 0.3437\n","Epoch [4/5], Step [1062/1072], Loss: 0.3503\n","Epoch [4/5], Step [1063/1072], Loss: 0.3478\n","Epoch [4/5], Step [1064/1072], Loss: 0.3367\n","Epoch [4/5], Step [1065/1072], Loss: 0.3400\n","Epoch [4/5], Step [1066/1072], Loss: 0.3516\n","Epoch [4/5], Step [1067/1072], Loss: 0.3341\n","Epoch [4/5], Step [1068/1072], Loss: 0.3549\n","Epoch [4/5], Step [1069/1072], Loss: 0.3400\n","Epoch [4/5], Step [1070/1072], Loss: 0.3239\n","Epoch [4/5], Step [1071/1072], Loss: 0.3465\n","Epoch [4/5], Step [1072/1072], Loss: 0.3410\n","Epoch [5/5], Step [1/1072], Loss: 0.3056\n","Epoch [5/5], Step [2/1072], Loss: 0.3676\n","Epoch [5/5], Step [3/1072], Loss: 0.2971\n","Epoch [5/5], Step [4/1072], Loss: 0.3259\n","Epoch [5/5], Step [5/1072], Loss: 0.3107\n","Epoch [5/5], Step [6/1072], Loss: 0.3195\n","Epoch [5/5], Step [7/1072], Loss: 0.3145\n","Epoch [5/5], Step [8/1072], Loss: 0.3215\n","Epoch [5/5], Step [9/1072], Loss: 0.3114\n","Epoch [5/5], Step [10/1072], Loss: 0.3028\n","Epoch [5/5], Step [11/1072], Loss: 0.3223\n","Epoch [5/5], Step [12/1072], Loss: 0.3017\n","Epoch [5/5], Step [13/1072], Loss: 0.3015\n","Epoch [5/5], Step [14/1072], Loss: 0.3189\n","Epoch [5/5], Step [15/1072], Loss: 0.2915\n","Epoch [5/5], Step [16/1072], Loss: 0.3546\n","Epoch [5/5], Step [17/1072], Loss: 0.3149\n","Epoch [5/5], Step [18/1072], Loss: 0.3029\n","Epoch [5/5], Step [19/1072], Loss: 0.2841\n","Epoch [5/5], Step [20/1072], Loss: 0.2866\n","Epoch [5/5], Step [21/1072], Loss: 0.3054\n","Epoch [5/5], Step [22/1072], Loss: 0.3426\n","Epoch [5/5], Step [23/1072], Loss: 0.2913\n","Epoch [5/5], Step [24/1072], Loss: 0.2886\n","Epoch [5/5], Step [25/1072], Loss: 0.2879\n","Epoch [5/5], Step [26/1072], Loss: 0.3107\n","Epoch [5/5], Step [27/1072], Loss: 0.3236\n","Epoch [5/5], Step [28/1072], Loss: 0.3123\n","Epoch [5/5], Step [29/1072], Loss: 0.2981\n","Epoch [5/5], Step [30/1072], Loss: 0.2751\n","Epoch [5/5], Step [31/1072], Loss: 0.2965\n","Epoch [5/5], Step [32/1072], Loss: 0.2886\n","Epoch [5/5], Step [33/1072], Loss: 0.3274\n","Epoch [5/5], Step [34/1072], Loss: 0.3212\n","Epoch [5/5], Step [35/1072], Loss: 0.3271\n","Epoch [5/5], Step [36/1072], Loss: 0.3232\n","Epoch [5/5], Step [37/1072], Loss: 0.2985\n","Epoch [5/5], Step [38/1072], Loss: 0.3259\n","Epoch [5/5], Step [39/1072], Loss: 0.3050\n","Epoch [5/5], Step [40/1072], Loss: 0.3752\n","Epoch [5/5], Step [41/1072], Loss: 0.3065\n","Epoch [5/5], Step [42/1072], Loss: 0.3057\n","Epoch [5/5], Step [43/1072], Loss: 0.3225\n","Epoch [5/5], Step [44/1072], Loss: 0.3181\n","Epoch [5/5], Step [45/1072], Loss: 0.3123\n","Epoch [5/5], Step [46/1072], Loss: 0.3167\n","Epoch [5/5], Step [47/1072], Loss: 0.3316\n","Epoch [5/5], Step [48/1072], Loss: 0.3231\n","Epoch [5/5], Step [49/1072], Loss: 0.3345\n","Epoch [5/5], Step [50/1072], Loss: 0.3065\n","Epoch [5/5], Step [51/1072], Loss: 0.3397\n","Epoch [5/5], Step [52/1072], Loss: 0.3158\n","Epoch [5/5], Step [53/1072], Loss: 0.3014\n","Epoch [5/5], Step [54/1072], Loss: 0.3044\n","Epoch [5/5], Step [55/1072], Loss: 0.3075\n","Epoch [5/5], Step [56/1072], Loss: 0.3191\n","Epoch [5/5], Step [57/1072], Loss: 0.3106\n","Epoch [5/5], Step [58/1072], Loss: 0.2920\n","Epoch [5/5], Step [59/1072], Loss: 0.3096\n","Epoch [5/5], Step [60/1072], Loss: 0.2966\n","Epoch [5/5], Step [61/1072], Loss: 0.3122\n","Epoch [5/5], Step [62/1072], Loss: 0.3328\n","Epoch [5/5], Step [63/1072], Loss: 0.3085\n","Epoch [5/5], Step [64/1072], Loss: 0.3370\n","Epoch [5/5], Step [65/1072], Loss: 0.3139\n","Epoch [5/5], Step [66/1072], Loss: 0.2978\n","Epoch [5/5], Step [67/1072], Loss: 0.3036\n","Epoch [5/5], Step [68/1072], Loss: 0.3172\n","Epoch [5/5], Step [69/1072], Loss: 0.3068\n","Epoch [5/5], Step [70/1072], Loss: 0.3138\n","Epoch [5/5], Step [71/1072], Loss: 0.3023\n","Epoch [5/5], Step [72/1072], Loss: 0.3131\n","Epoch [5/5], Step [73/1072], Loss: 0.3407\n","Epoch [5/5], Step [74/1072], Loss: 0.3339\n","Epoch [5/5], Step [75/1072], Loss: 0.2949\n","Epoch [5/5], Step [76/1072], Loss: 0.3256\n","Epoch [5/5], Step [77/1072], Loss: 0.2825\n","Epoch [5/5], Step [78/1072], Loss: 0.3012\n","Epoch [5/5], Step [79/1072], Loss: 0.2996\n","Epoch [5/5], Step [80/1072], Loss: 0.3030\n","Epoch [5/5], Step [81/1072], Loss: 0.2953\n","Epoch [5/5], Step [82/1072], Loss: 0.3146\n","Epoch [5/5], Step [83/1072], Loss: 0.3178\n","Epoch [5/5], Step [84/1072], Loss: 0.2881\n","Epoch [5/5], Step [85/1072], Loss: 0.2984\n","Epoch [5/5], Step [86/1072], Loss: 0.3087\n","Epoch [5/5], Step [87/1072], Loss: 0.3028\n","Epoch [5/5], Step [88/1072], Loss: 0.3160\n","Epoch [5/5], Step [89/1072], Loss: 0.3312\n","Epoch [5/5], Step [90/1072], Loss: 0.3173\n","Epoch [5/5], Step [91/1072], Loss: 0.3035\n","Epoch [5/5], Step [92/1072], Loss: 0.3091\n","Epoch [5/5], Step [93/1072], Loss: 0.3189\n","Epoch [5/5], Step [94/1072], Loss: 0.2834\n","Epoch [5/5], Step [95/1072], Loss: 0.3126\n","Epoch [5/5], Step [96/1072], Loss: 0.3346\n","Epoch [5/5], Step [97/1072], Loss: 0.3270\n","Epoch [5/5], Step [98/1072], Loss: 0.3129\n","Epoch [5/5], Step [99/1072], Loss: 0.3193\n","Epoch [5/5], Step [100/1072], Loss: 0.3035\n","Epoch [5/5], Step [101/1072], Loss: 0.3156\n","Epoch [5/5], Step [102/1072], Loss: 0.3126\n","Epoch [5/5], Step [103/1072], Loss: 0.3190\n","Epoch [5/5], Step [104/1072], Loss: 0.3329\n","Epoch [5/5], Step [105/1072], Loss: 0.3139\n","Epoch [5/5], Step [106/1072], Loss: 0.3098\n","Epoch [5/5], Step [107/1072], Loss: 0.3421\n","Epoch [5/5], Step [108/1072], Loss: 0.3067\n","Epoch [5/5], Step [109/1072], Loss: 0.2953\n","Epoch [5/5], Step [110/1072], Loss: 0.3067\n","Epoch [5/5], Step [111/1072], Loss: 0.3220\n","Epoch [5/5], Step [112/1072], Loss: 0.3128\n","Epoch [5/5], Step [113/1072], Loss: 0.3271\n","Epoch [5/5], Step [114/1072], Loss: 0.3248\n","Epoch [5/5], Step [115/1072], Loss: 0.3345\n","Epoch [5/5], Step [116/1072], Loss: 0.3138\n","Epoch [5/5], Step [117/1072], Loss: 0.3195\n","Epoch [5/5], Step [118/1072], Loss: 0.3075\n","Epoch [5/5], Step [119/1072], Loss: 0.3319\n","Epoch [5/5], Step [120/1072], Loss: 0.3112\n","Epoch [5/5], Step [121/1072], Loss: 0.3133\n","Epoch [5/5], Step [122/1072], Loss: 0.3275\n","Epoch [5/5], Step [123/1072], Loss: 0.3223\n","Epoch [5/5], Step [124/1072], Loss: 0.3154\n","Epoch [5/5], Step [125/1072], Loss: 0.3059\n","Epoch [5/5], Step [126/1072], Loss: 0.3383\n","Epoch [5/5], Step [127/1072], Loss: 0.3002\n","Epoch [5/5], Step [128/1072], Loss: 0.2948\n","Epoch [5/5], Step [129/1072], Loss: 0.2949\n","Epoch [5/5], Step [130/1072], Loss: 0.3343\n","Epoch [5/5], Step [131/1072], Loss: 0.3450\n","Epoch [5/5], Step [132/1072], Loss: 0.2999\n","Epoch [5/5], Step [133/1072], Loss: 0.2971\n","Epoch [5/5], Step [134/1072], Loss: 0.3030\n","Epoch [5/5], Step [135/1072], Loss: 0.2968\n","Epoch [5/5], Step [136/1072], Loss: 0.3106\n","Epoch [5/5], Step [137/1072], Loss: 0.3066\n","Epoch [5/5], Step [138/1072], Loss: 0.3037\n","Epoch [5/5], Step [139/1072], Loss: 0.3147\n","Epoch [5/5], Step [140/1072], Loss: 0.2980\n","Epoch [5/5], Step [141/1072], Loss: 0.2923\n","Epoch [5/5], Step [142/1072], Loss: 0.3158\n","Epoch [5/5], Step [143/1072], Loss: 0.3364\n","Epoch [5/5], Step [144/1072], Loss: 0.2698\n","Epoch [5/5], Step [145/1072], Loss: 0.3439\n","Epoch [5/5], Step [146/1072], Loss: 0.3229\n","Epoch [5/5], Step [147/1072], Loss: 0.3002\n","Epoch [5/5], Step [148/1072], Loss: 0.3119\n","Epoch [5/5], Step [149/1072], Loss: 0.3153\n","Epoch [5/5], Step [150/1072], Loss: 0.3179\n","Epoch [5/5], Step [151/1072], Loss: 0.3249\n","Epoch [5/5], Step [152/1072], Loss: 0.3366\n","Epoch [5/5], Step [153/1072], Loss: 0.3326\n","Epoch [5/5], Step [154/1072], Loss: 0.3050\n","Epoch [5/5], Step [155/1072], Loss: 0.3200\n","Epoch [5/5], Step [156/1072], Loss: 0.3258\n","Epoch [5/5], Step [157/1072], Loss: 0.3365\n","Epoch [5/5], Step [158/1072], Loss: 0.3027\n","Epoch [5/5], Step [159/1072], Loss: 0.3172\n","Epoch [5/5], Step [160/1072], Loss: 0.2768\n","Epoch [5/5], Step [161/1072], Loss: 0.3242\n","Epoch [5/5], Step [162/1072], Loss: 0.3181\n","Epoch [5/5], Step [163/1072], Loss: 0.2667\n","Epoch [5/5], Step [164/1072], Loss: 0.3233\n","Epoch [5/5], Step [165/1072], Loss: 0.2896\n","Epoch [5/5], Step [166/1072], Loss: 0.3002\n","Epoch [5/5], Step [167/1072], Loss: 0.3259\n","Epoch [5/5], Step [168/1072], Loss: 0.2836\n","Epoch [5/5], Step [169/1072], Loss: 0.3225\n","Epoch [5/5], Step [170/1072], Loss: 0.2952\n","Epoch [5/5], Step [171/1072], Loss: 0.3185\n","Epoch [5/5], Step [172/1072], Loss: 0.3198\n","Epoch [5/5], Step [173/1072], Loss: 0.2978\n","Epoch [5/5], Step [174/1072], Loss: 0.3164\n","Epoch [5/5], Step [175/1072], Loss: 0.3161\n","Epoch [5/5], Step [176/1072], Loss: 0.3267\n","Epoch [5/5], Step [177/1072], Loss: 0.3099\n","Epoch [5/5], Step [178/1072], Loss: 0.3169\n","Epoch [5/5], Step [179/1072], Loss: 0.3322\n","Epoch [5/5], Step [180/1072], Loss: 0.3249\n","Epoch [5/5], Step [181/1072], Loss: 0.3302\n","Epoch [5/5], Step [182/1072], Loss: 0.3087\n","Epoch [5/5], Step [183/1072], Loss: 0.2822\n","Epoch [5/5], Step [184/1072], Loss: 0.2987\n","Epoch [5/5], Step [185/1072], Loss: 0.2970\n","Epoch [5/5], Step [186/1072], Loss: 0.3094\n","Epoch [5/5], Step [187/1072], Loss: 0.2984\n","Epoch [5/5], Step [188/1072], Loss: 0.3138\n","Epoch [5/5], Step [189/1072], Loss: 0.2972\n","Epoch [5/5], Step [190/1072], Loss: 0.2879\n","Epoch [5/5], Step [191/1072], Loss: 0.3297\n","Epoch [5/5], Step [192/1072], Loss: 0.3121\n","Epoch [5/5], Step [193/1072], Loss: 0.3382\n","Epoch [5/5], Step [194/1072], Loss: 0.3221\n","Epoch [5/5], Step [195/1072], Loss: 0.3447\n","Epoch [5/5], Step [196/1072], Loss: 0.3124\n","Epoch [5/5], Step [197/1072], Loss: 0.3036\n","Epoch [5/5], Step [198/1072], Loss: 0.3308\n","Epoch [5/5], Step [199/1072], Loss: 0.3400\n","Epoch [5/5], Step [200/1072], Loss: 0.2955\n","Epoch [5/5], Step [201/1072], Loss: 0.3087\n","Epoch [5/5], Step [202/1072], Loss: 0.3587\n","Epoch [5/5], Step [203/1072], Loss: 0.2860\n","Epoch [5/5], Step [204/1072], Loss: 0.3279\n","Epoch [5/5], Step [205/1072], Loss: 0.3378\n","Epoch [5/5], Step [206/1072], Loss: 0.3229\n","Epoch [5/5], Step [207/1072], Loss: 0.3292\n","Epoch [5/5], Step [208/1072], Loss: 0.3176\n","Epoch [5/5], Step [209/1072], Loss: 0.3432\n","Epoch [5/5], Step [210/1072], Loss: 0.3015\n","Epoch [5/5], Step [211/1072], Loss: 0.2945\n","Epoch [5/5], Step [212/1072], Loss: 0.2879\n","Epoch [5/5], Step [213/1072], Loss: 0.3545\n","Epoch [5/5], Step [214/1072], Loss: 0.3188\n","Epoch [5/5], Step [215/1072], Loss: 0.3221\n","Epoch [5/5], Step [216/1072], Loss: 0.3087\n","Epoch [5/5], Step [217/1072], Loss: 0.3176\n","Epoch [5/5], Step [218/1072], Loss: 0.3174\n","Epoch [5/5], Step [219/1072], Loss: 0.3381\n","Epoch [5/5], Step [220/1072], Loss: 0.3103\n","Epoch [5/5], Step [221/1072], Loss: 0.3278\n","Epoch [5/5], Step [222/1072], Loss: 0.3380\n","Epoch [5/5], Step [223/1072], Loss: 0.3110\n","Epoch [5/5], Step [224/1072], Loss: 0.2950\n","Epoch [5/5], Step [225/1072], Loss: 0.3169\n","Epoch [5/5], Step [226/1072], Loss: 0.2962\n","Epoch [5/5], Step [227/1072], Loss: 0.3128\n","Epoch [5/5], Step [228/1072], Loss: 0.3150\n","Epoch [5/5], Step [229/1072], Loss: 0.3215\n","Epoch [5/5], Step [230/1072], Loss: 0.3318\n","Epoch [5/5], Step [231/1072], Loss: 0.3170\n","Epoch [5/5], Step [232/1072], Loss: 0.3045\n","Epoch [5/5], Step [233/1072], Loss: 0.3062\n","Epoch [5/5], Step [234/1072], Loss: 0.3249\n","Epoch [5/5], Step [235/1072], Loss: 0.3138\n","Epoch [5/5], Step [236/1072], Loss: 0.3262\n","Epoch [5/5], Step [237/1072], Loss: 0.3591\n","Epoch [5/5], Step [238/1072], Loss: 0.3260\n","Epoch [5/5], Step [239/1072], Loss: 0.3338\n","Epoch [5/5], Step [240/1072], Loss: 0.3100\n","Epoch [5/5], Step [241/1072], Loss: 0.3108\n","Epoch [5/5], Step [242/1072], Loss: 0.3105\n","Epoch [5/5], Step [243/1072], Loss: 0.2907\n","Epoch [5/5], Step [244/1072], Loss: 0.2974\n","Epoch [5/5], Step [245/1072], Loss: 0.2911\n","Epoch [5/5], Step [246/1072], Loss: 0.3053\n","Epoch [5/5], Step [247/1072], Loss: 0.3036\n","Epoch [5/5], Step [248/1072], Loss: 0.2948\n","Epoch [5/5], Step [249/1072], Loss: 0.3368\n","Epoch [5/5], Step [250/1072], Loss: 0.3079\n","Epoch [5/5], Step [251/1072], Loss: 0.3036\n","Epoch [5/5], Step [252/1072], Loss: 0.3384\n","Epoch [5/5], Step [253/1072], Loss: 0.3320\n","Epoch [5/5], Step [254/1072], Loss: 0.3037\n","Epoch [5/5], Step [255/1072], Loss: 0.3168\n","Epoch [5/5], Step [256/1072], Loss: 0.3080\n","Epoch [5/5], Step [257/1072], Loss: 0.3355\n","Epoch [5/5], Step [258/1072], Loss: 0.3281\n","Epoch [5/5], Step [259/1072], Loss: 0.3171\n","Epoch [5/5], Step [260/1072], Loss: 0.2883\n","Epoch [5/5], Step [261/1072], Loss: 0.3015\n","Epoch [5/5], Step [262/1072], Loss: 0.3070\n","Epoch [5/5], Step [263/1072], Loss: 0.3216\n","Epoch [5/5], Step [264/1072], Loss: 0.3143\n","Epoch [5/5], Step [265/1072], Loss: 0.3405\n","Epoch [5/5], Step [266/1072], Loss: 0.3094\n","Epoch [5/5], Step [267/1072], Loss: 0.3372\n","Epoch [5/5], Step [268/1072], Loss: 0.3338\n","Epoch [5/5], Step [269/1072], Loss: 0.3144\n","Epoch [5/5], Step [270/1072], Loss: 0.3092\n","Epoch [5/5], Step [271/1072], Loss: 0.2953\n","Epoch [5/5], Step [272/1072], Loss: 0.3323\n","Epoch [5/5], Step [273/1072], Loss: 0.3120\n","Epoch [5/5], Step [274/1072], Loss: 0.2931\n","Epoch [5/5], Step [275/1072], Loss: 0.3139\n","Epoch [5/5], Step [276/1072], Loss: 0.2788\n","Epoch [5/5], Step [277/1072], Loss: 0.3130\n","Epoch [5/5], Step [278/1072], Loss: 0.3633\n","Epoch [5/5], Step [279/1072], Loss: 0.3108\n","Epoch [5/5], Step [280/1072], Loss: 0.3311\n","Epoch [5/5], Step [281/1072], Loss: 0.3259\n","Epoch [5/5], Step [282/1072], Loss: 0.3049\n","Epoch [5/5], Step [283/1072], Loss: 0.2903\n","Epoch [5/5], Step [284/1072], Loss: 0.3186\n","Epoch [5/5], Step [285/1072], Loss: 0.3800\n","Epoch [5/5], Step [286/1072], Loss: 0.3171\n","Epoch [5/5], Step [287/1072], Loss: 0.3353\n","Epoch [5/5], Step [288/1072], Loss: 0.3104\n","Epoch [5/5], Step [289/1072], Loss: 0.3154\n","Epoch [5/5], Step [290/1072], Loss: 0.3373\n","Epoch [5/5], Step [291/1072], Loss: 0.3110\n","Epoch [5/5], Step [292/1072], Loss: 0.3113\n","Epoch [5/5], Step [293/1072], Loss: 0.2992\n","Epoch [5/5], Step [294/1072], Loss: 0.3253\n","Epoch [5/5], Step [295/1072], Loss: 0.3144\n","Epoch [5/5], Step [296/1072], Loss: 0.2853\n","Epoch [5/5], Step [297/1072], Loss: 0.3323\n","Epoch [5/5], Step [298/1072], Loss: 0.2669\n","Epoch [5/5], Step [299/1072], Loss: 0.3226\n","Epoch [5/5], Step [300/1072], Loss: 0.3129\n","Epoch [5/5], Step [301/1072], Loss: 0.3024\n","Epoch [5/5], Step [302/1072], Loss: 0.3356\n","Epoch [5/5], Step [303/1072], Loss: 0.2795\n","Epoch [5/5], Step [304/1072], Loss: 0.3209\n","Epoch [5/5], Step [305/1072], Loss: 0.3333\n","Epoch [5/5], Step [306/1072], Loss: 0.3188\n","Epoch [5/5], Step [307/1072], Loss: 0.2973\n","Epoch [5/5], Step [308/1072], Loss: 0.3313\n","Epoch [5/5], Step [309/1072], Loss: 0.3279\n","Epoch [5/5], Step [310/1072], Loss: 0.3223\n","Epoch [5/5], Step [311/1072], Loss: 0.3184\n","Epoch [5/5], Step [312/1072], Loss: 0.2853\n","Epoch [5/5], Step [313/1072], Loss: 0.3252\n","Epoch [5/5], Step [314/1072], Loss: 0.3257\n","Epoch [5/5], Step [315/1072], Loss: 0.3224\n","Epoch [5/5], Step [316/1072], Loss: 0.3210\n","Epoch [5/5], Step [317/1072], Loss: 0.3361\n","Epoch [5/5], Step [318/1072], Loss: 0.3192\n","Epoch [5/5], Step [319/1072], Loss: 0.3184\n","Epoch [5/5], Step [320/1072], Loss: 0.3170\n","Epoch [5/5], Step [321/1072], Loss: 0.3375\n","Epoch [5/5], Step [322/1072], Loss: 0.3201\n","Epoch [5/5], Step [323/1072], Loss: 0.3298\n","Epoch [5/5], Step [324/1072], Loss: 0.3137\n","Epoch [5/5], Step [325/1072], Loss: 0.3397\n","Epoch [5/5], Step [326/1072], Loss: 0.3211\n","Epoch [5/5], Step [327/1072], Loss: 0.3026\n","Epoch [5/5], Step [328/1072], Loss: 0.3019\n","Epoch [5/5], Step [329/1072], Loss: 0.3370\n","Epoch [5/5], Step [330/1072], Loss: 0.3215\n","Epoch [5/5], Step [331/1072], Loss: 0.3102\n","Epoch [5/5], Step [332/1072], Loss: 0.3240\n","Epoch [5/5], Step [333/1072], Loss: 0.3204\n","Epoch [5/5], Step [334/1072], Loss: 0.2913\n","Epoch [5/5], Step [335/1072], Loss: 0.3247\n","Epoch [5/5], Step [336/1072], Loss: 0.3176\n","Epoch [5/5], Step [337/1072], Loss: 0.3043\n","Epoch [5/5], Step [338/1072], Loss: 0.3215\n","Epoch [5/5], Step [339/1072], Loss: 0.3081\n","Epoch [5/5], Step [340/1072], Loss: 0.3186\n","Epoch [5/5], Step [341/1072], Loss: 0.3158\n","Epoch [5/5], Step [342/1072], Loss: 0.3263\n","Epoch [5/5], Step [343/1072], Loss: 0.3178\n","Epoch [5/5], Step [344/1072], Loss: 0.3129\n","Epoch [5/5], Step [345/1072], Loss: 0.3279\n","Epoch [5/5], Step [346/1072], Loss: 0.3166\n","Epoch [5/5], Step [347/1072], Loss: 0.2940\n","Epoch [5/5], Step [348/1072], Loss: 0.3077\n","Epoch [5/5], Step [349/1072], Loss: 0.3303\n","Epoch [5/5], Step [350/1072], Loss: 0.2958\n","Epoch [5/5], Step [351/1072], Loss: 0.3110\n","Epoch [5/5], Step [352/1072], Loss: 0.2967\n","Epoch [5/5], Step [353/1072], Loss: 0.3271\n","Epoch [5/5], Step [354/1072], Loss: 0.3091\n","Epoch [5/5], Step [355/1072], Loss: 0.3328\n","Epoch [5/5], Step [356/1072], Loss: 0.3093\n","Epoch [5/5], Step [357/1072], Loss: 0.3062\n","Epoch [5/5], Step [358/1072], Loss: 0.2963\n","Epoch [5/5], Step [359/1072], Loss: 0.3109\n","Epoch [5/5], Step [360/1072], Loss: 0.3092\n","Epoch [5/5], Step [361/1072], Loss: 0.3228\n","Epoch [5/5], Step [362/1072], Loss: 0.3117\n","Epoch [5/5], Step [363/1072], Loss: 0.3312\n","Epoch [5/5], Step [364/1072], Loss: 0.3119\n","Epoch [5/5], Step [365/1072], Loss: 0.2885\n","Epoch [5/5], Step [366/1072], Loss: 0.3487\n","Epoch [5/5], Step [367/1072], Loss: 0.3220\n","Epoch [5/5], Step [368/1072], Loss: 0.2917\n","Epoch [5/5], Step [369/1072], Loss: 0.3269\n","Epoch [5/5], Step [370/1072], Loss: 0.2920\n","Epoch [5/5], Step [371/1072], Loss: 0.3117\n","Epoch [5/5], Step [372/1072], Loss: 0.3289\n","Epoch [5/5], Step [373/1072], Loss: 0.3020\n","Epoch [5/5], Step [374/1072], Loss: 0.3319\n","Epoch [5/5], Step [375/1072], Loss: 0.3206\n","Epoch [5/5], Step [376/1072], Loss: 0.3139\n","Epoch [5/5], Step [377/1072], Loss: 0.2901\n","Epoch [5/5], Step [378/1072], Loss: 0.3003\n","Epoch [5/5], Step [379/1072], Loss: 0.3187\n","Epoch [5/5], Step [380/1072], Loss: 0.3193\n","Epoch [5/5], Step [381/1072], Loss: 0.3247\n","Epoch [5/5], Step [382/1072], Loss: 0.3042\n","Epoch [5/5], Step [383/1072], Loss: 0.3104\n","Epoch [5/5], Step [384/1072], Loss: 0.3291\n","Epoch [5/5], Step [385/1072], Loss: 0.3228\n","Epoch [5/5], Step [386/1072], Loss: 0.3310\n","Epoch [5/5], Step [387/1072], Loss: 0.3429\n","Epoch [5/5], Step [388/1072], Loss: 0.2968\n","Epoch [5/5], Step [389/1072], Loss: 0.3239\n","Epoch [5/5], Step [390/1072], Loss: 0.3173\n","Epoch [5/5], Step [391/1072], Loss: 0.3343\n","Epoch [5/5], Step [392/1072], Loss: 0.3086\n","Epoch [5/5], Step [393/1072], Loss: 0.3331\n","Epoch [5/5], Step [394/1072], Loss: 0.3267\n","Epoch [5/5], Step [395/1072], Loss: 0.3227\n","Epoch [5/5], Step [396/1072], Loss: 0.3105\n","Epoch [5/5], Step [397/1072], Loss: 0.2783\n","Epoch [5/5], Step [398/1072], Loss: 0.3207\n","Epoch [5/5], Step [399/1072], Loss: 0.2992\n","Epoch [5/5], Step [400/1072], Loss: 0.3062\n","Epoch [5/5], Step [401/1072], Loss: 0.3192\n","Epoch [5/5], Step [402/1072], Loss: 0.3118\n","Epoch [5/5], Step [403/1072], Loss: 0.3291\n","Epoch [5/5], Step [404/1072], Loss: 0.3233\n","Epoch [5/5], Step [405/1072], Loss: 0.3461\n","Epoch [5/5], Step [406/1072], Loss: 0.2973\n","Epoch [5/5], Step [407/1072], Loss: 0.3070\n","Epoch [5/5], Step [408/1072], Loss: 0.3129\n","Epoch [5/5], Step [409/1072], Loss: 0.3349\n","Epoch [5/5], Step [410/1072], Loss: 0.2985\n","Epoch [5/5], Step [411/1072], Loss: 0.2939\n","Epoch [5/5], Step [412/1072], Loss: 0.2958\n","Epoch [5/5], Step [413/1072], Loss: 0.3279\n","Epoch [5/5], Step [414/1072], Loss: 0.2980\n","Epoch [5/5], Step [415/1072], Loss: 0.3204\n","Epoch [5/5], Step [416/1072], Loss: 0.3048\n","Epoch [5/5], Step [417/1072], Loss: 0.3168\n","Epoch [5/5], Step [418/1072], Loss: 0.3068\n","Epoch [5/5], Step [419/1072], Loss: 0.3256\n","Epoch [5/5], Step [420/1072], Loss: 0.3161\n","Epoch [5/5], Step [421/1072], Loss: 0.3100\n","Epoch [5/5], Step [422/1072], Loss: 0.3176\n","Epoch [5/5], Step [423/1072], Loss: 0.2846\n","Epoch [5/5], Step [424/1072], Loss: 0.3312\n","Epoch [5/5], Step [425/1072], Loss: 0.3217\n","Epoch [5/5], Step [426/1072], Loss: 0.3033\n","Epoch [5/5], Step [427/1072], Loss: 0.3348\n","Epoch [5/5], Step [428/1072], Loss: 0.3066\n","Epoch [5/5], Step [429/1072], Loss: 0.3243\n","Epoch [5/5], Step [430/1072], Loss: 0.2984\n","Epoch [5/5], Step [431/1072], Loss: 0.3223\n","Epoch [5/5], Step [432/1072], Loss: 0.3343\n","Epoch [5/5], Step [433/1072], Loss: 0.3163\n","Epoch [5/5], Step [434/1072], Loss: 0.3017\n","Epoch [5/5], Step [435/1072], Loss: 0.3282\n","Epoch [5/5], Step [436/1072], Loss: 0.3077\n","Epoch [5/5], Step [437/1072], Loss: 0.3054\n","Epoch [5/5], Step [438/1072], Loss: 0.2917\n","Epoch [5/5], Step [439/1072], Loss: 0.2935\n","Epoch [5/5], Step [440/1072], Loss: 0.2985\n","Epoch [5/5], Step [441/1072], Loss: 0.3321\n","Epoch [5/5], Step [442/1072], Loss: 0.3539\n","Epoch [5/5], Step [443/1072], Loss: 0.3465\n","Epoch [5/5], Step [444/1072], Loss: 0.3446\n","Epoch [5/5], Step [445/1072], Loss: 0.3278\n","Epoch [5/5], Step [446/1072], Loss: 0.3098\n","Epoch [5/5], Step [447/1072], Loss: 0.3107\n","Epoch [5/5], Step [448/1072], Loss: 0.3278\n","Epoch [5/5], Step [449/1072], Loss: 0.3121\n","Epoch [5/5], Step [450/1072], Loss: 0.3203\n","Epoch [5/5], Step [451/1072], Loss: 0.3409\n","Epoch [5/5], Step [452/1072], Loss: 0.3270\n","Epoch [5/5], Step [453/1072], Loss: 0.3269\n","Epoch [5/5], Step [454/1072], Loss: 0.2974\n","Epoch [5/5], Step [455/1072], Loss: 0.3245\n","Epoch [5/5], Step [456/1072], Loss: 0.3223\n","Epoch [5/5], Step [457/1072], Loss: 0.3116\n","Epoch [5/5], Step [458/1072], Loss: 0.3146\n","Epoch [5/5], Step [459/1072], Loss: 0.3411\n","Epoch [5/5], Step [460/1072], Loss: 0.3361\n","Epoch [5/5], Step [461/1072], Loss: 0.3019\n","Epoch [5/5], Step [462/1072], Loss: 0.3099\n","Epoch [5/5], Step [463/1072], Loss: 0.3255\n","Epoch [5/5], Step [464/1072], Loss: 0.3151\n","Epoch [5/5], Step [465/1072], Loss: 0.3114\n","Epoch [5/5], Step [466/1072], Loss: 0.3240\n","Epoch [5/5], Step [467/1072], Loss: 0.3030\n","Epoch [5/5], Step [468/1072], Loss: 0.3356\n","Epoch [5/5], Step [469/1072], Loss: 0.3340\n","Epoch [5/5], Step [470/1072], Loss: 0.3229\n","Epoch [5/5], Step [471/1072], Loss: 0.3102\n","Epoch [5/5], Step [472/1072], Loss: 0.2788\n","Epoch [5/5], Step [473/1072], Loss: 0.3230\n","Epoch [5/5], Step [474/1072], Loss: 0.3282\n","Epoch [5/5], Step [475/1072], Loss: 0.3320\n","Epoch [5/5], Step [476/1072], Loss: 0.3271\n","Epoch [5/5], Step [477/1072], Loss: 0.2810\n","Epoch [5/5], Step [478/1072], Loss: 0.3223\n","Epoch [5/5], Step [479/1072], Loss: 0.3041\n","Epoch [5/5], Step [480/1072], Loss: 0.2958\n","Epoch [5/5], Step [481/1072], Loss: 0.3210\n","Epoch [5/5], Step [482/1072], Loss: 0.3332\n","Epoch [5/5], Step [483/1072], Loss: 0.2819\n","Epoch [5/5], Step [484/1072], Loss: 0.3251\n","Epoch [5/5], Step [485/1072], Loss: 0.3165\n","Epoch [5/5], Step [486/1072], Loss: 0.3452\n","Epoch [5/5], Step [487/1072], Loss: 0.3209\n","Epoch [5/5], Step [488/1072], Loss: 0.3259\n","Epoch [5/5], Step [489/1072], Loss: 0.2761\n","Epoch [5/5], Step [490/1072], Loss: 0.3011\n","Epoch [5/5], Step [491/1072], Loss: 0.3465\n","Epoch [5/5], Step [492/1072], Loss: 0.3325\n","Epoch [5/5], Step [493/1072], Loss: 0.2968\n","Epoch [5/5], Step [494/1072], Loss: 0.2763\n","Epoch [5/5], Step [495/1072], Loss: 0.3528\n","Epoch [5/5], Step [496/1072], Loss: 0.3219\n","Epoch [5/5], Step [497/1072], Loss: 0.3293\n","Epoch [5/5], Step [498/1072], Loss: 0.3578\n","Epoch [5/5], Step [499/1072], Loss: 0.3426\n","Epoch [5/5], Step [500/1072], Loss: 0.3288\n","Epoch [5/5], Step [501/1072], Loss: 0.2969\n","Epoch [5/5], Step [502/1072], Loss: 0.3085\n","Epoch [5/5], Step [503/1072], Loss: 0.3076\n","Epoch [5/5], Step [504/1072], Loss: 0.3119\n","Epoch [5/5], Step [505/1072], Loss: 0.3152\n","Epoch [5/5], Step [506/1072], Loss: 0.3685\n","Epoch [5/5], Step [507/1072], Loss: 0.3224\n","Epoch [5/5], Step [508/1072], Loss: 0.3010\n","Epoch [5/5], Step [509/1072], Loss: 0.3191\n","Epoch [5/5], Step [510/1072], Loss: 0.3414\n","Epoch [5/5], Step [511/1072], Loss: 0.3231\n","Epoch [5/5], Step [512/1072], Loss: 0.3298\n","Epoch [5/5], Step [513/1072], Loss: 0.3338\n","Epoch [5/5], Step [514/1072], Loss: 0.3308\n","Epoch [5/5], Step [515/1072], Loss: 0.3105\n","Epoch [5/5], Step [516/1072], Loss: 0.3267\n","Epoch [5/5], Step [517/1072], Loss: 0.3257\n","Epoch [5/5], Step [518/1072], Loss: 0.3217\n","Epoch [5/5], Step [519/1072], Loss: 0.3157\n","Epoch [5/5], Step [520/1072], Loss: 0.3239\n","Epoch [5/5], Step [521/1072], Loss: 0.2951\n","Epoch [5/5], Step [522/1072], Loss: 0.2933\n","Epoch [5/5], Step [523/1072], Loss: 0.3197\n","Epoch [5/5], Step [524/1072], Loss: 0.3022\n","Epoch [5/5], Step [525/1072], Loss: 0.2863\n","Epoch [5/5], Step [526/1072], Loss: 0.3710\n","Epoch [5/5], Step [527/1072], Loss: 0.2994\n","Epoch [5/5], Step [528/1072], Loss: 0.3254\n","Epoch [5/5], Step [529/1072], Loss: 0.3252\n","Epoch [5/5], Step [530/1072], Loss: 0.3131\n","Epoch [5/5], Step [531/1072], Loss: 0.3337\n","Epoch [5/5], Step [532/1072], Loss: 0.3148\n","Epoch [5/5], Step [533/1072], Loss: 0.3287\n","Epoch [5/5], Step [534/1072], Loss: 0.3068\n","Epoch [5/5], Step [535/1072], Loss: 0.2986\n","Epoch [5/5], Step [536/1072], Loss: 0.3264\n","Epoch [5/5], Step [537/1072], Loss: 0.3024\n","Epoch [5/5], Step [538/1072], Loss: 0.3121\n","Epoch [5/5], Step [539/1072], Loss: 0.3234\n","Epoch [5/5], Step [540/1072], Loss: 0.3272\n","Epoch [5/5], Step [541/1072], Loss: 0.3322\n","Epoch [5/5], Step [542/1072], Loss: 0.3195\n","Epoch [5/5], Step [543/1072], Loss: 0.3217\n","Epoch [5/5], Step [544/1072], Loss: 0.3046\n","Epoch [5/5], Step [545/1072], Loss: 0.3115\n","Epoch [5/5], Step [546/1072], Loss: 0.2772\n","Epoch [5/5], Step [547/1072], Loss: 0.3315\n","Epoch [5/5], Step [548/1072], Loss: 0.3144\n","Epoch [5/5], Step [549/1072], Loss: 0.3134\n","Epoch [5/5], Step [550/1072], Loss: 0.2990\n","Epoch [5/5], Step [551/1072], Loss: 0.3359\n","Epoch [5/5], Step [552/1072], Loss: 0.3217\n","Epoch [5/5], Step [553/1072], Loss: 0.3154\n","Epoch [5/5], Step [554/1072], Loss: 0.3247\n","Epoch [5/5], Step [555/1072], Loss: 0.2807\n","Epoch [5/5], Step [556/1072], Loss: 0.3193\n","Epoch [5/5], Step [557/1072], Loss: 0.3138\n","Epoch [5/5], Step [558/1072], Loss: 0.3050\n","Epoch [5/5], Step [559/1072], Loss: 0.3258\n","Epoch [5/5], Step [560/1072], Loss: 0.3073\n","Epoch [5/5], Step [561/1072], Loss: 0.3186\n","Epoch [5/5], Step [562/1072], Loss: 0.3339\n","Epoch [5/5], Step [563/1072], Loss: 0.3025\n","Epoch [5/5], Step [564/1072], Loss: 0.3106\n","Epoch [5/5], Step [565/1072], Loss: 0.3615\n","Epoch [5/5], Step [566/1072], Loss: 0.3325\n","Epoch [5/5], Step [567/1072], Loss: 0.3055\n","Epoch [5/5], Step [568/1072], Loss: 0.2834\n","Epoch [5/5], Step [569/1072], Loss: 0.3332\n","Epoch [5/5], Step [570/1072], Loss: 0.3344\n","Epoch [5/5], Step [571/1072], Loss: 0.3238\n","Epoch [5/5], Step [572/1072], Loss: 0.3223\n","Epoch [5/5], Step [573/1072], Loss: 0.3128\n","Epoch [5/5], Step [574/1072], Loss: 0.3013\n","Epoch [5/5], Step [575/1072], Loss: 0.3162\n","Epoch [5/5], Step [576/1072], Loss: 0.2879\n","Epoch [5/5], Step [577/1072], Loss: 0.3318\n","Epoch [5/5], Step [578/1072], Loss: 0.3391\n","Epoch [5/5], Step [579/1072], Loss: 0.3121\n","Epoch [5/5], Step [580/1072], Loss: 0.3215\n","Epoch [5/5], Step [581/1072], Loss: 0.2929\n","Epoch [5/5], Step [582/1072], Loss: 0.3076\n","Epoch [5/5], Step [583/1072], Loss: 0.3302\n","Epoch [5/5], Step [584/1072], Loss: 0.3334\n","Epoch [5/5], Step [585/1072], Loss: 0.3022\n","Epoch [5/5], Step [586/1072], Loss: 0.3057\n","Epoch [5/5], Step [587/1072], Loss: 0.3226\n","Epoch [5/5], Step [588/1072], Loss: 0.2981\n","Epoch [5/5], Step [589/1072], Loss: 0.3224\n","Epoch [5/5], Step [590/1072], Loss: 0.3083\n","Epoch [5/5], Step [591/1072], Loss: 0.3164\n","Epoch [5/5], Step [592/1072], Loss: 0.2876\n","Epoch [5/5], Step [593/1072], Loss: 0.3382\n","Epoch [5/5], Step [594/1072], Loss: 0.3459\n","Epoch [5/5], Step [595/1072], Loss: 0.2895\n","Epoch [5/5], Step [596/1072], Loss: 0.3259\n","Epoch [5/5], Step [597/1072], Loss: 0.2938\n","Epoch [5/5], Step [598/1072], Loss: 0.3061\n","Epoch [5/5], Step [599/1072], Loss: 0.3079\n","Epoch [5/5], Step [600/1072], Loss: 0.3038\n","Epoch [5/5], Step [601/1072], Loss: 0.3249\n","Epoch [5/5], Step [602/1072], Loss: 0.3242\n","Epoch [5/5], Step [603/1072], Loss: 0.3127\n","Epoch [5/5], Step [604/1072], Loss: 0.3093\n","Epoch [5/5], Step [605/1072], Loss: 0.3273\n","Epoch [5/5], Step [606/1072], Loss: 0.3222\n","Epoch [5/5], Step [607/1072], Loss: 0.3362\n","Epoch [5/5], Step [608/1072], Loss: 0.2980\n","Epoch [5/5], Step [609/1072], Loss: 0.3005\n","Epoch [5/5], Step [610/1072], Loss: 0.2997\n","Epoch [5/5], Step [611/1072], Loss: 0.2881\n","Epoch [5/5], Step [612/1072], Loss: 0.3185\n","Epoch [5/5], Step [613/1072], Loss: 0.2872\n","Epoch [5/5], Step [614/1072], Loss: 0.3270\n","Epoch [5/5], Step [615/1072], Loss: 0.3580\n","Epoch [5/5], Step [616/1072], Loss: 0.3221\n","Epoch [5/5], Step [617/1072], Loss: 0.3245\n","Epoch [5/5], Step [618/1072], Loss: 0.3152\n","Epoch [5/5], Step [619/1072], Loss: 0.3219\n","Epoch [5/5], Step [620/1072], Loss: 0.3109\n","Epoch [5/5], Step [621/1072], Loss: 0.3272\n","Epoch [5/5], Step [622/1072], Loss: 0.2934\n","Epoch [5/5], Step [623/1072], Loss: 0.3034\n","Epoch [5/5], Step [624/1072], Loss: 0.3460\n","Epoch [5/5], Step [625/1072], Loss: 0.3055\n","Epoch [5/5], Step [626/1072], Loss: 0.3126\n","Epoch [5/5], Step [627/1072], Loss: 0.3131\n","Epoch [5/5], Step [628/1072], Loss: 0.3218\n","Epoch [5/5], Step [629/1072], Loss: 0.3444\n","Epoch [5/5], Step [630/1072], Loss: 0.3173\n","Epoch [5/5], Step [631/1072], Loss: 0.3124\n","Epoch [5/5], Step [632/1072], Loss: 0.3116\n","Epoch [5/5], Step [633/1072], Loss: 0.2996\n","Epoch [5/5], Step [634/1072], Loss: 0.3163\n","Epoch [5/5], Step [635/1072], Loss: 0.3393\n","Epoch [5/5], Step [636/1072], Loss: 0.3075\n","Epoch [5/5], Step [637/1072], Loss: 0.3100\n","Epoch [5/5], Step [638/1072], Loss: 0.3323\n","Epoch [5/5], Step [639/1072], Loss: 0.3316\n","Epoch [5/5], Step [640/1072], Loss: 0.3330\n","Epoch [5/5], Step [641/1072], Loss: 0.3171\n","Epoch [5/5], Step [642/1072], Loss: 0.3011\n","Epoch [5/5], Step [643/1072], Loss: 0.3273\n","Epoch [5/5], Step [644/1072], Loss: 0.3132\n","Epoch [5/5], Step [645/1072], Loss: 0.3164\n","Epoch [5/5], Step [646/1072], Loss: 0.3313\n","Epoch [5/5], Step [647/1072], Loss: 0.3089\n","Epoch [5/5], Step [648/1072], Loss: 0.2970\n","Epoch [5/5], Step [649/1072], Loss: 0.3129\n","Epoch [5/5], Step [650/1072], Loss: 0.3316\n","Epoch [5/5], Step [651/1072], Loss: 0.3217\n","Epoch [5/5], Step [652/1072], Loss: 0.3512\n","Epoch [5/5], Step [653/1072], Loss: 0.3320\n","Epoch [5/5], Step [654/1072], Loss: 0.3206\n","Epoch [5/5], Step [655/1072], Loss: 0.2941\n","Epoch [5/5], Step [656/1072], Loss: 0.3482\n","Epoch [5/5], Step [657/1072], Loss: 0.3343\n","Epoch [5/5], Step [658/1072], Loss: 0.2974\n","Epoch [5/5], Step [659/1072], Loss: 0.3084\n","Epoch [5/5], Step [660/1072], Loss: 0.2878\n","Epoch [5/5], Step [661/1072], Loss: 0.2896\n","Epoch [5/5], Step [662/1072], Loss: 0.3435\n","Epoch [5/5], Step [663/1072], Loss: 0.3151\n","Epoch [5/5], Step [664/1072], Loss: 0.3068\n","Epoch [5/5], Step [665/1072], Loss: 0.3603\n","Epoch [5/5], Step [666/1072], Loss: 0.3232\n","Epoch [5/5], Step [667/1072], Loss: 0.3269\n","Epoch [5/5], Step [668/1072], Loss: 0.3211\n","Epoch [5/5], Step [669/1072], Loss: 0.3262\n","Epoch [5/5], Step [670/1072], Loss: 0.3062\n","Epoch [5/5], Step [671/1072], Loss: 0.3154\n","Epoch [5/5], Step [672/1072], Loss: 0.3551\n","Epoch [5/5], Step [673/1072], Loss: 0.3082\n","Epoch [5/5], Step [674/1072], Loss: 0.3120\n","Epoch [5/5], Step [675/1072], Loss: 0.3036\n","Epoch [5/5], Step [676/1072], Loss: 0.2874\n","Epoch [5/5], Step [677/1072], Loss: 0.3136\n","Epoch [5/5], Step [678/1072], Loss: 0.2904\n","Epoch [5/5], Step [679/1072], Loss: 0.3377\n","Epoch [5/5], Step [680/1072], Loss: 0.2916\n","Epoch [5/5], Step [681/1072], Loss: 0.3348\n","Epoch [5/5], Step [682/1072], Loss: 0.3362\n","Epoch [5/5], Step [683/1072], Loss: 0.3186\n","Epoch [5/5], Step [684/1072], Loss: 0.3370\n","Epoch [5/5], Step [685/1072], Loss: 0.3710\n","Epoch [5/5], Step [686/1072], Loss: 0.3194\n","Epoch [5/5], Step [687/1072], Loss: 0.3078\n","Epoch [5/5], Step [688/1072], Loss: 0.2965\n","Epoch [5/5], Step [689/1072], Loss: 0.3138\n","Epoch [5/5], Step [690/1072], Loss: 0.3151\n","Epoch [5/5], Step [691/1072], Loss: 0.3263\n","Epoch [5/5], Step [692/1072], Loss: 0.3332\n","Epoch [5/5], Step [693/1072], Loss: 0.3135\n","Epoch [5/5], Step [694/1072], Loss: 0.3354\n","Epoch [5/5], Step [695/1072], Loss: 0.3282\n","Epoch [5/5], Step [696/1072], Loss: 0.3050\n","Epoch [5/5], Step [697/1072], Loss: 0.3350\n","Epoch [5/5], Step [698/1072], Loss: 0.3259\n","Epoch [5/5], Step [699/1072], Loss: 0.3098\n","Epoch [5/5], Step [700/1072], Loss: 0.3475\n","Epoch [5/5], Step [701/1072], Loss: 0.3313\n","Epoch [5/5], Step [702/1072], Loss: 0.3466\n","Epoch [5/5], Step [703/1072], Loss: 0.3291\n","Epoch [5/5], Step [704/1072], Loss: 0.3071\n","Epoch [5/5], Step [705/1072], Loss: 0.3002\n","Epoch [5/5], Step [706/1072], Loss: 0.3210\n","Epoch [5/5], Step [707/1072], Loss: 0.3190\n","Epoch [5/5], Step [708/1072], Loss: 0.3250\n","Epoch [5/5], Step [709/1072], Loss: 0.3330\n","Epoch [5/5], Step [710/1072], Loss: 0.2858\n","Epoch [5/5], Step [711/1072], Loss: 0.3038\n","Epoch [5/5], Step [712/1072], Loss: 0.3135\n","Epoch [5/5], Step [713/1072], Loss: 0.3186\n","Epoch [5/5], Step [714/1072], Loss: 0.3228\n","Epoch [5/5], Step [715/1072], Loss: 0.3374\n","Epoch [5/5], Step [716/1072], Loss: 0.3011\n","Epoch [5/5], Step [717/1072], Loss: 0.3466\n","Epoch [5/5], Step [718/1072], Loss: 0.3605\n","Epoch [5/5], Step [719/1072], Loss: 0.3137\n","Epoch [5/5], Step [720/1072], Loss: 0.3205\n","Epoch [5/5], Step [721/1072], Loss: 0.3342\n","Epoch [5/5], Step [722/1072], Loss: 0.3263\n","Epoch [5/5], Step [723/1072], Loss: 0.3088\n","Epoch [5/5], Step [724/1072], Loss: 0.3159\n","Epoch [5/5], Step [725/1072], Loss: 0.3120\n","Epoch [5/5], Step [726/1072], Loss: 0.3140\n","Epoch [5/5], Step [727/1072], Loss: 0.3385\n","Epoch [5/5], Step [728/1072], Loss: 0.3413\n","Epoch [5/5], Step [729/1072], Loss: 0.3654\n","Epoch [5/5], Step [730/1072], Loss: 0.3099\n","Epoch [5/5], Step [731/1072], Loss: 0.3240\n","Epoch [5/5], Step [732/1072], Loss: 0.3118\n","Epoch [5/5], Step [733/1072], Loss: 0.3012\n","Epoch [5/5], Step [734/1072], Loss: 0.3155\n","Epoch [5/5], Step [735/1072], Loss: 0.3242\n","Epoch [5/5], Step [736/1072], Loss: 0.3313\n","Epoch [5/5], Step [737/1072], Loss: 0.3118\n","Epoch [5/5], Step [738/1072], Loss: 0.3003\n","Epoch [5/5], Step [739/1072], Loss: 0.2971\n","Epoch [5/5], Step [740/1072], Loss: 0.2931\n","Epoch [5/5], Step [741/1072], Loss: 0.3358\n","Epoch [5/5], Step [742/1072], Loss: 0.3101\n","Epoch [5/5], Step [743/1072], Loss: 0.2982\n","Epoch [5/5], Step [744/1072], Loss: 0.2944\n","Epoch [5/5], Step [745/1072], Loss: 0.3031\n","Epoch [5/5], Step [746/1072], Loss: 0.2852\n","Epoch [5/5], Step [747/1072], Loss: 0.3356\n","Epoch [5/5], Step [748/1072], Loss: 0.2962\n","Epoch [5/5], Step [749/1072], Loss: 0.3071\n","Epoch [5/5], Step [750/1072], Loss: 0.3041\n","Epoch [5/5], Step [751/1072], Loss: 0.3171\n","Epoch [5/5], Step [752/1072], Loss: 0.3187\n","Epoch [5/5], Step [753/1072], Loss: 0.3055\n","Epoch [5/5], Step [754/1072], Loss: 0.3234\n","Epoch [5/5], Step [755/1072], Loss: 0.3470\n","Epoch [5/5], Step [756/1072], Loss: 0.3082\n","Epoch [5/5], Step [757/1072], Loss: 0.3161\n","Epoch [5/5], Step [758/1072], Loss: 0.3137\n","Epoch [5/5], Step [759/1072], Loss: 0.3091\n","Epoch [5/5], Step [760/1072], Loss: 0.3242\n","Epoch [5/5], Step [761/1072], Loss: 0.3089\n","Epoch [5/5], Step [762/1072], Loss: 0.3195\n","Epoch [5/5], Step [763/1072], Loss: 0.3446\n","Epoch [5/5], Step [764/1072], Loss: 0.3396\n","Epoch [5/5], Step [765/1072], Loss: 0.3067\n","Epoch [5/5], Step [766/1072], Loss: 0.3206\n","Epoch [5/5], Step [767/1072], Loss: 0.3066\n","Epoch [5/5], Step [768/1072], Loss: 0.2813\n","Epoch [5/5], Step [769/1072], Loss: 0.3023\n","Epoch [5/5], Step [770/1072], Loss: 0.2848\n","Epoch [5/5], Step [771/1072], Loss: 0.3149\n","Epoch [5/5], Step [772/1072], Loss: 0.2909\n","Epoch [5/5], Step [773/1072], Loss: 0.3374\n","Epoch [5/5], Step [774/1072], Loss: 0.3108\n","Epoch [5/5], Step [775/1072], Loss: 0.2958\n","Epoch [5/5], Step [776/1072], Loss: 0.3418\n","Epoch [5/5], Step [777/1072], Loss: 0.3215\n","Epoch [5/5], Step [778/1072], Loss: 0.2984\n","Epoch [5/5], Step [779/1072], Loss: 0.3093\n","Epoch [5/5], Step [780/1072], Loss: 0.3171\n","Epoch [5/5], Step [781/1072], Loss: 0.3512\n","Epoch [5/5], Step [782/1072], Loss: 0.3075\n","Epoch [5/5], Step [783/1072], Loss: 0.3360\n","Epoch [5/5], Step [784/1072], Loss: 0.3247\n","Epoch [5/5], Step [785/1072], Loss: 0.2859\n","Epoch [5/5], Step [786/1072], Loss: 0.3027\n","Epoch [5/5], Step [787/1072], Loss: 0.3242\n","Epoch [5/5], Step [788/1072], Loss: 0.3234\n","Epoch [5/5], Step [789/1072], Loss: 0.3177\n","Epoch [5/5], Step [790/1072], Loss: 0.2766\n","Epoch [5/5], Step [791/1072], Loss: 0.3007\n","Epoch [5/5], Step [792/1072], Loss: 0.3334\n","Epoch [5/5], Step [793/1072], Loss: 0.3372\n","Epoch [5/5], Step [794/1072], Loss: 0.2916\n","Epoch [5/5], Step [795/1072], Loss: 0.3013\n","Epoch [5/5], Step [796/1072], Loss: 0.3181\n","Epoch [5/5], Step [797/1072], Loss: 0.3400\n","Epoch [5/5], Step [798/1072], Loss: 0.3035\n","Epoch [5/5], Step [799/1072], Loss: 0.3258\n","Epoch [5/5], Step [800/1072], Loss: 0.3169\n","Epoch [5/5], Step [801/1072], Loss: 0.3250\n","Epoch [5/5], Step [802/1072], Loss: 0.3361\n","Epoch [5/5], Step [803/1072], Loss: 0.3078\n","Epoch [5/5], Step [804/1072], Loss: 0.3268\n","Epoch [5/5], Step [805/1072], Loss: 0.3157\n","Epoch [5/5], Step [806/1072], Loss: 0.3419\n","Epoch [5/5], Step [807/1072], Loss: 0.3465\n","Epoch [5/5], Step [808/1072], Loss: 0.3164\n","Epoch [5/5], Step [809/1072], Loss: 0.2900\n","Epoch [5/5], Step [810/1072], Loss: 0.3393\n","Epoch [5/5], Step [811/1072], Loss: 0.3315\n","Epoch [5/5], Step [812/1072], Loss: 0.3237\n","Epoch [5/5], Step [813/1072], Loss: 0.3191\n","Epoch [5/5], Step [814/1072], Loss: 0.3224\n","Epoch [5/5], Step [815/1072], Loss: 0.3442\n","Epoch [5/5], Step [816/1072], Loss: 0.3078\n","Epoch [5/5], Step [817/1072], Loss: 0.3194\n","Epoch [5/5], Step [818/1072], Loss: 0.2982\n","Epoch [5/5], Step [819/1072], Loss: 0.3318\n","Epoch [5/5], Step [820/1072], Loss: 0.3291\n","Epoch [5/5], Step [821/1072], Loss: 0.3464\n","Epoch [5/5], Step [822/1072], Loss: 0.3258\n","Epoch [5/5], Step [823/1072], Loss: 0.3203\n","Epoch [5/5], Step [824/1072], Loss: 0.3399\n","Epoch [5/5], Step [825/1072], Loss: 0.3331\n","Epoch [5/5], Step [826/1072], Loss: 0.3420\n","Epoch [5/5], Step [827/1072], Loss: 0.3231\n","Epoch [5/5], Step [828/1072], Loss: 0.3454\n","Epoch [5/5], Step [829/1072], Loss: 0.3334\n","Epoch [5/5], Step [830/1072], Loss: 0.3166\n","Epoch [5/5], Step [831/1072], Loss: 0.3382\n","Epoch [5/5], Step [832/1072], Loss: 0.3387\n","Epoch [5/5], Step [833/1072], Loss: 0.3424\n","Epoch [5/5], Step [834/1072], Loss: 0.3523\n","Epoch [5/5], Step [835/1072], Loss: 0.3088\n","Epoch [5/5], Step [836/1072], Loss: 0.3257\n","Epoch [5/5], Step [837/1072], Loss: 0.3223\n","Epoch [5/5], Step [838/1072], Loss: 0.3069\n","Epoch [5/5], Step [839/1072], Loss: 0.3556\n","Epoch [5/5], Step [840/1072], Loss: 0.3148\n","Epoch [5/5], Step [841/1072], Loss: 0.2978\n","Epoch [5/5], Step [842/1072], Loss: 0.3210\n","Epoch [5/5], Step [843/1072], Loss: 0.3052\n","Epoch [5/5], Step [844/1072], Loss: 0.3363\n","Epoch [5/5], Step [845/1072], Loss: 0.3094\n","Epoch [5/5], Step [846/1072], Loss: 0.3261\n","Epoch [5/5], Step [847/1072], Loss: 0.3442\n","Epoch [5/5], Step [848/1072], Loss: 0.3105\n","Epoch [5/5], Step [849/1072], Loss: 0.3455\n","Epoch [5/5], Step [850/1072], Loss: 0.3097\n","Epoch [5/5], Step [851/1072], Loss: 0.3220\n","Epoch [5/5], Step [852/1072], Loss: 0.3493\n","Epoch [5/5], Step [853/1072], Loss: 0.3229\n","Epoch [5/5], Step [854/1072], Loss: 0.3113\n","Epoch [5/5], Step [855/1072], Loss: 0.3107\n","Epoch [5/5], Step [856/1072], Loss: 0.3149\n","Epoch [5/5], Step [857/1072], Loss: 0.3242\n","Epoch [5/5], Step [858/1072], Loss: 0.3086\n","Epoch [5/5], Step [859/1072], Loss: 0.2944\n","Epoch [5/5], Step [860/1072], Loss: 0.3193\n","Epoch [5/5], Step [861/1072], Loss: 0.2939\n","Epoch [5/5], Step [862/1072], Loss: 0.3149\n","Epoch [5/5], Step [863/1072], Loss: 0.3068\n","Epoch [5/5], Step [864/1072], Loss: 0.3252\n","Epoch [5/5], Step [865/1072], Loss: 0.3163\n","Epoch [5/5], Step [866/1072], Loss: 0.3024\n","Epoch [5/5], Step [867/1072], Loss: 0.3253\n","Epoch [5/5], Step [868/1072], Loss: 0.2991\n","Epoch [5/5], Step [869/1072], Loss: 0.3365\n","Epoch [5/5], Step [870/1072], Loss: 0.3307\n","Epoch [5/5], Step [871/1072], Loss: 0.3101\n","Epoch [5/5], Step [872/1072], Loss: 0.2963\n","Epoch [5/5], Step [873/1072], Loss: 0.3220\n","Epoch [5/5], Step [874/1072], Loss: 0.3113\n","Epoch [5/5], Step [875/1072], Loss: 0.3115\n","Epoch [5/5], Step [876/1072], Loss: 0.3266\n","Epoch [5/5], Step [877/1072], Loss: 0.3063\n","Epoch [5/5], Step [878/1072], Loss: 0.3433\n","Epoch [5/5], Step [879/1072], Loss: 0.3198\n","Epoch [5/5], Step [880/1072], Loss: 0.3141\n","Epoch [5/5], Step [881/1072], Loss: 0.3235\n","Epoch [5/5], Step [882/1072], Loss: 0.3360\n","Epoch [5/5], Step [883/1072], Loss: 0.2970\n","Epoch [5/5], Step [884/1072], Loss: 0.3077\n","Epoch [5/5], Step [885/1072], Loss: 0.3031\n","Epoch [5/5], Step [886/1072], Loss: 0.3195\n","Epoch [5/5], Step [887/1072], Loss: 0.3095\n","Epoch [5/5], Step [888/1072], Loss: 0.3099\n","Epoch [5/5], Step [889/1072], Loss: 0.3219\n","Epoch [5/5], Step [890/1072], Loss: 0.3016\n","Epoch [5/5], Step [891/1072], Loss: 0.3201\n","Epoch [5/5], Step [892/1072], Loss: 0.3324\n","Epoch [5/5], Step [893/1072], Loss: 0.3167\n","Epoch [5/5], Step [894/1072], Loss: 0.2925\n","Epoch [5/5], Step [895/1072], Loss: 0.3371\n","Epoch [5/5], Step [896/1072], Loss: 0.3195\n","Epoch [5/5], Step [897/1072], Loss: 0.3062\n","Epoch [5/5], Step [898/1072], Loss: 0.3228\n","Epoch [5/5], Step [899/1072], Loss: 0.3509\n","Epoch [5/5], Step [900/1072], Loss: 0.3277\n","Epoch [5/5], Step [901/1072], Loss: 0.3399\n","Epoch [5/5], Step [902/1072], Loss: 0.3305\n","Epoch [5/5], Step [903/1072], Loss: 0.3180\n","Epoch [5/5], Step [904/1072], Loss: 0.2860\n","Epoch [5/5], Step [905/1072], Loss: 0.3171\n","Epoch [5/5], Step [906/1072], Loss: 0.3345\n","Epoch [5/5], Step [907/1072], Loss: 0.3230\n","Epoch [5/5], Step [908/1072], Loss: 0.3156\n","Epoch [5/5], Step [909/1072], Loss: 0.3134\n","Epoch [5/5], Step [910/1072], Loss: 0.3057\n","Epoch [5/5], Step [911/1072], Loss: 0.3311\n","Epoch [5/5], Step [912/1072], Loss: 0.3016\n","Epoch [5/5], Step [913/1072], Loss: 0.3084\n","Epoch [5/5], Step [914/1072], Loss: 0.3368\n","Epoch [5/5], Step [915/1072], Loss: 0.3026\n","Epoch [5/5], Step [916/1072], Loss: 0.2956\n","Epoch [5/5], Step [917/1072], Loss: 0.3319\n","Epoch [5/5], Step [918/1072], Loss: 0.3039\n","Epoch [5/5], Step [919/1072], Loss: 0.3379\n","Epoch [5/5], Step [920/1072], Loss: 0.3092\n","Epoch [5/5], Step [921/1072], Loss: 0.2952\n","Epoch [5/5], Step [922/1072], Loss: 0.3065\n","Epoch [5/5], Step [923/1072], Loss: 0.3174\n","Epoch [5/5], Step [924/1072], Loss: 0.3035\n","Epoch [5/5], Step [925/1072], Loss: 0.3015\n","Epoch [5/5], Step [926/1072], Loss: 0.2884\n","Epoch [5/5], Step [927/1072], Loss: 0.3415\n","Epoch [5/5], Step [928/1072], Loss: 0.2941\n","Epoch [5/5], Step [929/1072], Loss: 0.3260\n","Epoch [5/5], Step [930/1072], Loss: 0.3131\n","Epoch [5/5], Step [931/1072], Loss: 0.3377\n","Epoch [5/5], Step [932/1072], Loss: 0.3240\n","Epoch [5/5], Step [933/1072], Loss: 0.3178\n","Epoch [5/5], Step [934/1072], Loss: 0.3316\n","Epoch [5/5], Step [935/1072], Loss: 0.3225\n","Epoch [5/5], Step [936/1072], Loss: 0.3298\n","Epoch [5/5], Step [937/1072], Loss: 0.3322\n","Epoch [5/5], Step [938/1072], Loss: 0.3058\n","Epoch [5/5], Step [939/1072], Loss: 0.3226\n","Epoch [5/5], Step [940/1072], Loss: 0.3110\n","Epoch [5/5], Step [941/1072], Loss: 0.3353\n","Epoch [5/5], Step [942/1072], Loss: 0.3176\n","Epoch [5/5], Step [943/1072], Loss: 0.2959\n","Epoch [5/5], Step [944/1072], Loss: 0.2832\n","Epoch [5/5], Step [945/1072], Loss: 0.3158\n","Epoch [5/5], Step [946/1072], Loss: 0.3211\n","Epoch [5/5], Step [947/1072], Loss: 0.3522\n","Epoch [5/5], Step [948/1072], Loss: 0.3204\n","Epoch [5/5], Step [949/1072], Loss: 0.3133\n","Epoch [5/5], Step [950/1072], Loss: 0.3073\n","Epoch [5/5], Step [951/1072], Loss: 0.3070\n","Epoch [5/5], Step [952/1072], Loss: 0.3313\n","Epoch [5/5], Step [953/1072], Loss: 0.3336\n","Epoch [5/5], Step [954/1072], Loss: 0.3261\n","Epoch [5/5], Step [955/1072], Loss: 0.3122\n","Epoch [5/5], Step [956/1072], Loss: 0.3148\n","Epoch [5/5], Step [957/1072], Loss: 0.3096\n","Epoch [5/5], Step [958/1072], Loss: 0.3028\n","Epoch [5/5], Step [959/1072], Loss: 0.3505\n","Epoch [5/5], Step [960/1072], Loss: 0.3095\n","Epoch [5/5], Step [961/1072], Loss: 0.3097\n","Epoch [5/5], Step [962/1072], Loss: 0.3087\n","Epoch [5/5], Step [963/1072], Loss: 0.3072\n","Epoch [5/5], Step [964/1072], Loss: 0.3151\n","Epoch [5/5], Step [965/1072], Loss: 0.3192\n","Epoch [5/5], Step [966/1072], Loss: 0.3041\n","Epoch [5/5], Step [967/1072], Loss: 0.3094\n","Epoch [5/5], Step [968/1072], Loss: 0.3233\n","Epoch [5/5], Step [969/1072], Loss: 0.3271\n","Epoch [5/5], Step [970/1072], Loss: 0.3209\n","Epoch [5/5], Step [971/1072], Loss: 0.3179\n","Epoch [5/5], Step [972/1072], Loss: 0.2941\n","Epoch [5/5], Step [973/1072], Loss: 0.3020\n","Epoch [5/5], Step [974/1072], Loss: 0.3206\n","Epoch [5/5], Step [975/1072], Loss: 0.3145\n","Epoch [5/5], Step [976/1072], Loss: 0.3421\n","Epoch [5/5], Step [977/1072], Loss: 0.3128\n","Epoch [5/5], Step [978/1072], Loss: 0.3276\n","Epoch [5/5], Step [979/1072], Loss: 0.3328\n","Epoch [5/5], Step [980/1072], Loss: 0.3057\n","Epoch [5/5], Step [981/1072], Loss: 0.3060\n","Epoch [5/5], Step [982/1072], Loss: 0.3415\n","Epoch [5/5], Step [983/1072], Loss: 0.3199\n","Epoch [5/5], Step [984/1072], Loss: 0.3181\n","Epoch [5/5], Step [985/1072], Loss: 0.3156\n","Epoch [5/5], Step [986/1072], Loss: 0.3182\n","Epoch [5/5], Step [987/1072], Loss: 0.3345\n","Epoch [5/5], Step [988/1072], Loss: 0.3108\n","Epoch [5/5], Step [989/1072], Loss: 0.3251\n","Epoch [5/5], Step [990/1072], Loss: 0.3324\n","Epoch [5/5], Step [991/1072], Loss: 0.3219\n","Epoch [5/5], Step [992/1072], Loss: 0.2892\n","Epoch [5/5], Step [993/1072], Loss: 0.3405\n","Epoch [5/5], Step [994/1072], Loss: 0.3207\n","Epoch [5/5], Step [995/1072], Loss: 0.3458\n","Epoch [5/5], Step [996/1072], Loss: 0.3292\n","Epoch [5/5], Step [997/1072], Loss: 0.3385\n","Epoch [5/5], Step [998/1072], Loss: 0.3315\n","Epoch [5/5], Step [999/1072], Loss: 0.3038\n","Epoch [5/5], Step [1000/1072], Loss: 0.3196\n","Epoch [5/5], Step [1001/1072], Loss: 0.3326\n","Epoch [5/5], Step [1002/1072], Loss: 0.3113\n","Epoch [5/5], Step [1003/1072], Loss: 0.3203\n","Epoch [5/5], Step [1004/1072], Loss: 0.3074\n","Epoch [5/5], Step [1005/1072], Loss: 0.3296\n","Epoch [5/5], Step [1006/1072], Loss: 0.3102\n","Epoch [5/5], Step [1007/1072], Loss: 0.3118\n","Epoch [5/5], Step [1008/1072], Loss: 0.3182\n","Epoch [5/5], Step [1009/1072], Loss: 0.2952\n","Epoch [5/5], Step [1010/1072], Loss: 0.3068\n","Epoch [5/5], Step [1011/1072], Loss: 0.2998\n","Epoch [5/5], Step [1012/1072], Loss: 0.3286\n","Epoch [5/5], Step [1013/1072], Loss: 0.2988\n","Epoch [5/5], Step [1014/1072], Loss: 0.2925\n","Epoch [5/5], Step [1015/1072], Loss: 0.2978\n","Epoch [5/5], Step [1016/1072], Loss: 0.3274\n","Epoch [5/5], Step [1017/1072], Loss: 0.3145\n","Epoch [5/5], Step [1018/1072], Loss: 0.3251\n","Epoch [5/5], Step [1019/1072], Loss: 0.3342\n","Epoch [5/5], Step [1020/1072], Loss: 0.3366\n","Epoch [5/5], Step [1021/1072], Loss: 0.3260\n","Epoch [5/5], Step [1022/1072], Loss: 0.3308\n","Epoch [5/5], Step [1023/1072], Loss: 0.3280\n","Epoch [5/5], Step [1024/1072], Loss: 0.3130\n","Epoch [5/5], Step [1025/1072], Loss: 0.3390\n","Epoch [5/5], Step [1026/1072], Loss: 0.3220\n","Epoch [5/5], Step [1027/1072], Loss: 0.3000\n","Epoch [5/5], Step [1028/1072], Loss: 0.3282\n","Epoch [5/5], Step [1029/1072], Loss: 0.3233\n","Epoch [5/5], Step [1030/1072], Loss: 0.3370\n","Epoch [5/5], Step [1031/1072], Loss: 0.3373\n","Epoch [5/5], Step [1032/1072], Loss: 0.3287\n","Epoch [5/5], Step [1033/1072], Loss: 0.3129\n","Epoch [5/5], Step [1034/1072], Loss: 0.3115\n","Epoch [5/5], Step [1035/1072], Loss: 0.3329\n","Epoch [5/5], Step [1036/1072], Loss: 0.3508\n","Epoch [5/5], Step [1037/1072], Loss: 0.3019\n","Epoch [5/5], Step [1038/1072], Loss: 0.2822\n","Epoch [5/5], Step [1039/1072], Loss: 0.3093\n","Epoch [5/5], Step [1040/1072], Loss: 0.2940\n","Epoch [5/5], Step [1041/1072], Loss: 0.2926\n","Epoch [5/5], Step [1042/1072], Loss: 0.3225\n","Epoch [5/5], Step [1043/1072], Loss: 0.3236\n","Epoch [5/5], Step [1044/1072], Loss: 0.3261\n","Epoch [5/5], Step [1045/1072], Loss: 0.3547\n","Epoch [5/5], Step [1046/1072], Loss: 0.3337\n","Epoch [5/5], Step [1047/1072], Loss: 0.3105\n","Epoch [5/5], Step [1048/1072], Loss: 0.3030\n","Epoch [5/5], Step [1049/1072], Loss: 0.3392\n","Epoch [5/5], Step [1050/1072], Loss: 0.3355\n","Epoch [5/5], Step [1051/1072], Loss: 0.3043\n","Epoch [5/5], Step [1052/1072], Loss: 0.2925\n","Epoch [5/5], Step [1053/1072], Loss: 0.3191\n","Epoch [5/5], Step [1054/1072], Loss: 0.3147\n","Epoch [5/5], Step [1055/1072], Loss: 0.3281\n","Epoch [5/5], Step [1056/1072], Loss: 0.3205\n","Epoch [5/5], Step [1057/1072], Loss: 0.3062\n","Epoch [5/5], Step [1058/1072], Loss: 0.3037\n","Epoch [5/5], Step [1059/1072], Loss: 0.2897\n","Epoch [5/5], Step [1060/1072], Loss: 0.2950\n","Epoch [5/5], Step [1061/1072], Loss: 0.3101\n","Epoch [5/5], Step [1062/1072], Loss: 0.3375\n","Epoch [5/5], Step [1063/1072], Loss: 0.3223\n","Epoch [5/5], Step [1064/1072], Loss: 0.3146\n","Epoch [5/5], Step [1065/1072], Loss: 0.3134\n","Epoch [5/5], Step [1066/1072], Loss: 0.3152\n","Epoch [5/5], Step [1067/1072], Loss: 0.3150\n","Epoch [5/5], Step [1068/1072], Loss: 0.3006\n","Epoch [5/5], Step [1069/1072], Loss: 0.3203\n","Epoch [5/5], Step [1070/1072], Loss: 0.3370\n","Epoch [5/5], Step [1071/1072], Loss: 0.3040\n","Epoch [5/5], Step [1072/1072], Loss: 0.2789\n"]}],"source":["# Initialize the model, loss function, and optimizer\n","loss_criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(abstract_model.parameters(), lr=0.001)\n","\n","# Move the model to GPU/CPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","abstract_model.to(device)\n","\n","# Train the model for 20 epochs\n","# Note: The batch size has already been accounted for in the data loader\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","  abstract_model.train()\n","  for i, (text, labels) in enumerate(train_loader_text):\n","    # Moving the data to GPU if available\n","    text, labels = text.to(device), labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = abstract_model(text) # Forward pass\n","    loss = loss_criterion(outputs, labels) # Error / loss computation\n","    loss.backward() # Gradient Computation\n","    optimizer.step() # Backpropagation\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader_text)}], Loss: {loss.item():.4f}')\n","\n","#Save the final model\n","torch.save(abstract_model.state_dict(), '/content/drive/MyDrive/NLP_Final_Project/abstract_model.pth')"]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"yadG5y2A_j1R"}},{"cell_type":"code","execution_count":48,"metadata":{"id":"UDp43iHXau6G","executionInfo":{"status":"ok","timestamp":1700539506012,"user_tz":360,"elapsed":5967,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}}},"outputs":[],"source":["# Initializing the Model Settings\n","embedding_size = 200\n","hidden_size = 64\n","num_layers = 2\n","\n","# Create an instance of the abstract model and load the saved model\n","abstract_model = AbstractsModel(input_size, embedding_size, hidden_size, num_layers)\n","abstract_model.load_state_dict(torch.load('/content/drive/MyDrive/NLP_Final_Project/abstract_model.pth'))\n","\n","# Loading the model to GPU/CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","abstract_model.to(device)\n","\n","# Put the model in evaluation mode\n","abstract_model.eval()\n","y_pred = []\n","y_true = []\n","\n","# Evaluate the model on test data\n","with torch.no_grad():\n","    for text, labels in test_loader_text:\n","        text = text.to(device)\n","        outputs = abstract_model(text)\n","        y_pred.extend(outputs.detach().cpu().numpy())\n","        y_true.extend(labels.detach().cpu().numpy())\n","\n","# Calculating the label\n","y_pred = np.argmax(y_pred, axis=1)\n","y_true = np.argmax(y_true, axis=1)"]},{"cell_type":"code","source":["print(classification_report(y_true, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYz3whmFDDVm","executionInfo":{"status":"ok","timestamp":1700539545353,"user_tz":360,"elapsed":517,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"a1f40cf6-36e6-484f-bcc7-a3ada4416e3c"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.65      0.68      0.66      2586\n","           1       0.77      0.62      0.69      2377\n","           2       0.91      0.93      0.92      9629\n","           3       0.90      0.91      0.90     10262\n","           4       0.82      0.80      0.81      4414\n","\n","    accuracy                           0.86     29268\n","   macro avg       0.81      0.79      0.80     29268\n","weighted avg       0.86      0.86      0.86     29268\n","\n"]}]},{"cell_type":"code","source":["print(confusion_matrix(y_true, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBcwRtNHDFoc","executionInfo":{"status":"ok","timestamp":1700539568783,"user_tz":360,"elapsed":132,"user":{"displayName":"Awais Naeem","userId":"07439048610289892144"}},"outputId":"aa456878-2583-4451-85b7-581129aa9458"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1752  362  152   35  285]\n"," [ 623 1485  158   13   98]\n"," [  75   59 8962  470   63]\n"," [  33    4  545 9327  353]\n"," [ 228   15   60  574 3537]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fuAgwAWQDHgv"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}