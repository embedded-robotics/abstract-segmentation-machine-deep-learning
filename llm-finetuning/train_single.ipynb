{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"classify single sequence\"\n",
    "labels = ['RESULTS', 'METHODS', 'CONCLUSIONS', 'OBJECTIVE', 'BACKGROUND']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {}\n",
    "label2id = {}\n",
    "label_i2i = {}\n",
    "label_i2label = {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "  id2label[i] = label\n",
    "  label2id[label] = i\n",
    "  label_i2i[f\"LABEL_{i}\"] = i\n",
    "  label_i2label[f\"LABEL_{i}\"] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"Dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"Dataset/test.csv\")\n",
    "df_val = pd.read_csv(\"Dataset/dev.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForSequenceClassification\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForSequenceClassification.from_pretrained(\"t5-small\", num_labels=len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prep(df):\n",
    "  text = [f\"{task_prefix}: {text}\" for text in df['Text'].to_list()]\n",
    "  inputs = [tokenizer(f\"{task_prefix}: {text}\", padding=True, truncation=True, return_tensors=\"pt\").input_ids[0] for text in df['Text'].to_list()]\n",
    "  y = [labels.index(y) for y in df['Label'].to_list()]\n",
    "  return Dataset.from_dict({\"input_ids\": inputs, \"text\": text, \"labels\": y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X & y parts\n",
    "ds_train = prep(df_test)\n",
    "ds_test = prep(df_test)\n",
    "ds_val = prep(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [853, 4921, 712, 5932, 10, 1404, 2071, 20853, 2842, 11, 6716, 33, 8, 741, 13, 46, 3, 49, 52, 782, 1162, 5817, 257, 13, 8, 10090, 1990, 6615, 11, 3, 9, 381, 13, 20197, 7, 13, 10090, 43, 2932, 118, 14650, 21, 1181, 18, 15329, 2874, 3, 5, 1], 'text': 'classify single sequence: Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions .', 'labels': 4}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 00:49:11.757415: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-01 00:49:11.775698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 00:49:11.775718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 00:49:11.776233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 00:49:11.779429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 00:49:12.204245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5-small-single-sent-classify\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    #eval_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 07:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3660, training_loss=0.6719908187949593, metrics={'train_runtime': 447.234, 'train_samples_per_second': 130.884, 'train_steps_per_second': 8.184, 'total_flos': 1610493835362048.0, 'train_loss': 0.6719908187949593, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify single sequence: Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions .\n",
      "4\n",
      "{'input_ids': tensor([[  853,  4921,   712,  5932,    10,  1404,  2071, 20853,  2842,    11,\n",
      "          6716,    33,     8,   741,    13,    46,     3,    49,    52,   782,\n",
      "          1162,  5817,   257,    13,     8, 10090,  1990,  6615,    11,     3,\n",
      "             9,   381,    13, 20197,     7,    13, 10090,    43,  2932,   118,\n",
      "         14650,    21,  1181,    18, 15329,  2874,     3,     5,     1]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n",
      "4\n",
      "BACKGROUND\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "  text = ds_test[0]['text']\n",
    "  print(text)\n",
    "  label = ds_test[0]['labels']\n",
    "  print(label)\n",
    "  inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  print(inputs)\n",
    "  logits = model(**inputs).logits\n",
    "  predicted_class_id = logits.argmax().item()\n",
    "  print(predicted_class_id)\n",
    "  #pred_class = model.config.id2label[predicted_class_id]\n",
    "  #print(pred_class)\n",
    "  pred_class = id2label[predicted_class_id]\n",
    "  print(pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y_actual = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i in range(len(ds_test)):\n",
    "    text = ds_test[i]['text']\n",
    "    label_id = ds_test[i]['labels']\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    y_actual.append(id2label[label_id])\n",
    "    y_pred.append(id2label[predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8123206232062321"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_actual, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  BACKGROUND       0.59      0.42      0.49      2586\n",
      " CONCLUSIONS       0.69      0.73      0.71      4414\n",
      "     METHODS       0.87      0.92      0.90      9629\n",
      "   OBJECTIVE       0.69      0.67      0.68      2377\n",
      "     RESULTS       0.87      0.88      0.87     10262\n",
      "\n",
      "    accuracy                           0.81     29268\n",
      "   macro avg       0.74      0.72      0.73     29268\n",
      "weighted avg       0.81      0.81      0.81     29268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_actual, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1074  730  197  529   56]\n",
      " [ 301 3235   96   34  748]\n",
      " [  79   46 8868  125  511]\n",
      " [ 337  260  167 1585   28]\n",
      " [  18  404  815   12 9013]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_actual, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
