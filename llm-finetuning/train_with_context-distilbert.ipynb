{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['RESULTS', 'METHODS', 'CONCLUSIONS', 'OBJECTIVE', 'BACKGROUND']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {}\n",
    "label2id = {}\n",
    "label_i2i = {}\n",
    "label_i2label = {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "  id2label[i] = label\n",
    "  label2id[label] = i\n",
    "  label_i2i[f\"LABEL_{i}\"] = i\n",
    "  label_i2label[f\"LABEL_{i}\"] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"Dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"Dataset/test.csv\")\n",
    "df_val = pd.read_csv(\"Dataset/dev.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Context'] = df_train[['Abstract Name','Text']].groupby(['Abstract Name'])['Text'].transform(lambda x: ' '.join(x))\n",
    "df_test['Context'] = df_test[['Abstract Name','Text']].groupby(['Abstract Name'])['Text'].transform(lambda x: ' '.join(x))\n",
    "df_val['Context'] = df_val[['Abstract Name','Text']].groupby(['Abstract Name'])['Text'].transform(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Input_Text'] = df_train[['Text','Context']].apply(lambda x: ' [SEP] '.join(x), axis=1)\n",
    "df_test['Input_Text'] = df_test[['Text','Context']].apply(lambda x: ' [SEP] '.join(x), axis=1)\n",
    "df_val['Input_Text'] = df_val[['Text','Context']].apply(lambda x: ' [SEP] '.join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prep(df):\n",
    "  text = [f\"{text}\" for text in df['Input_Text'].to_list()]\n",
    "  inputs = [tokenizer(f\"{text}\", padding=True, truncation=True, return_tensors=\"pt\").input_ids[0] for text in df['Text'].to_list()]\n",
    "  y = [labels.index(y) for y in df['Label'].to_list()]\n",
    "  return Dataset.from_dict({\"input_ids\": inputs, \"text\": text, \"labels\": y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X & y parts\n",
    "ds_train = prep(df_test)\n",
    "ds_test = prep(df_test)\n",
    "ds_val = prep(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2116, 26835, 2594, 6194, 1998, 7870, 2024, 1996, 2765, 1997, 2019, 9413, 20793, 3560, 13791, 1997, 1996, 13711, 16690, 1998, 1037, 2193, 1997, 25456, 1997, 13711, 2031, 2947, 2042, 8920, 2005, 3424, 1011, 20187, 4506, 1012, 102], 'text': 'Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions . [SEP] Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions . It was recently demonstrated that supraphysiological concentrations of the endogenous complement inhibitor MAp44 ( also denoted MAP1 ) protect against myocardial reperfusion injury . In the present study , we examined the association between outcome after acute myocardial infarction ( MI ) and the plasma levels of MAp44 and its related proteins MASP-1 and MASP-3 in patients with first-time MI . In addition , we compared plasma levels of MAp44 , MASP-1 , and MASP-3 in MI patients to levels in a healthy control group . A total of 192 MI patients and 140 control persons were included . Plasma samples were obtained and analysed with time-resolved immunofluorometric assays determining the plasma levels of MAp44 , MASP-1 , and MASP-3 . The myocardial outcomes ( salvage index and final infarct size ) were measured by gated single-photon emission CT. . MI patients had 18 % higher plasma levels of MAp44 ( IQR 11-25 % ) as compared to the healthy control group ( p < 0.001 . However , neither salvage index ( Spearman rho -0.1 , p = 0.28 ) nor final infarct size ( Spearman rho 0.02 , p = 0.83 ) correlated with plasma levels of MAp44 . Likewise , MASP-1 and MASP-3 were elevated in MI patients ( p = 0.002 and p < 0.001 ) , but the levels were not correlated to outcome . Plasma levels of MAp44 , MASP-1 , and MASP-3 are significantly higher in patients with MI compared to healthy control persons , but are not associated with short-term outcome measured as salvage index and final infarct .', 'labels': 4}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Eval during training disabled due to high memory\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-multi-sent-classify\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    #eval_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 04:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3660, training_loss=0.4224070418727854, metrics={'train_runtime': 297.2368, 'train_samples_per_second': 196.934, 'train_steps_per_second': 12.313, 'total_flos': 1275041051517000.0, 'train_loss': 0.4224070418727854, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions . [SEP] Many pathogenic processes and diseases are the result of an erroneous activation of the complement cascade and a number of inhibitors of complement have thus been examined for anti-inflammatory actions . It was recently demonstrated that supraphysiological concentrations of the endogenous complement inhibitor MAp44 ( also denoted MAP1 ) protect against myocardial reperfusion injury . In the present study , we examined the association between outcome after acute myocardial infarction ( MI ) and the plasma levels of MAp44 and its related proteins MASP-1 and MASP-3 in patients with first-time MI . In addition , we compared plasma levels of MAp44 , MASP-1 , and MASP-3 in MI patients to levels in a healthy control group . A total of 192 MI patients and 140 control persons were included . Plasma samples were obtained and analysed with time-resolved immunofluorometric assays determining the plasma levels of MAp44 , MASP-1 , and MASP-3 . The myocardial outcomes ( salvage index and final infarct size ) were measured by gated single-photon emission CT. . MI patients had 18 % higher plasma levels of MAp44 ( IQR 11-25 % ) as compared to the healthy control group ( p < 0.001 . However , neither salvage index ( Spearman rho -0.1 , p = 0.28 ) nor final infarct size ( Spearman rho 0.02 , p = 0.83 ) correlated with plasma levels of MAp44 . Likewise , MASP-1 and MASP-3 were elevated in MI patients ( p = 0.002 and p < 0.001 ) , but the levels were not correlated to outcome . Plasma levels of MAp44 , MASP-1 , and MASP-3 are significantly higher in patients with MI compared to healthy control persons , but are not associated with short-term outcome measured as salvage index and final infarct .\n",
      "4\n",
      "{'input_ids': tensor([[  101,  2116, 26835,  2594,  6194,  1998,  7870,  2024,  1996,  2765,\n",
      "          1997,  2019,  9413, 20793,  3560, 13791,  1997,  1996, 13711, 16690,\n",
      "          1998,  1037,  2193,  1997, 25456,  1997, 13711,  2031,  2947,  2042,\n",
      "          8920,  2005,  3424,  1011, 20187,  4506,  1012,   102,  2116, 26835,\n",
      "          2594,  6194,  1998,  7870,  2024,  1996,  2765,  1997,  2019,  9413,\n",
      "         20793,  3560, 13791,  1997,  1996, 13711, 16690,  1998,  1037,  2193,\n",
      "          1997, 25456,  1997, 13711,  2031,  2947,  2042,  8920,  2005,  3424,\n",
      "          1011, 20187,  4506,  1012,  2009,  2001,  3728,  7645,  2008, 10514,\n",
      "         18098,  9331, 10536, 20763,  9966, 14061,  1997,  1996,  2203, 23924,\n",
      "          3560, 13711, 24054,  4949, 22932,  1006,  2036, 19537,  4949,  2487,\n",
      "          1007,  4047,  2114,  2026, 24755, 25070, 16360,  2121, 20523,  4544,\n",
      "          1012,  1999,  1996,  2556,  2817,  1010,  2057,  8920,  1996,  2523,\n",
      "          2090,  9560,  2044, 11325,  2026, 24755, 25070,  1999, 14971,  7542,\n",
      "          1006,  2771,  1007,  1998,  1996, 12123,  3798,  1997,  4949, 22932,\n",
      "          1998,  2049,  3141,  8171, 16137,  2361,  1011,  1015,  1998, 16137,\n",
      "          2361,  1011,  1017,  1999,  5022,  2007,  2034,  1011,  2051,  2771,\n",
      "          1012,  1999,  2804,  1010,  2057,  4102, 12123,  3798,  1997,  4949,\n",
      "         22932,  1010, 16137,  2361,  1011,  1015,  1010,  1998, 16137,  2361,\n",
      "          1011,  1017,  1999,  2771,  5022,  2000,  3798,  1999,  1037,  7965,\n",
      "          2491,  2177,  1012,  1037,  2561,  1997, 17613,  2771,  5022,  1998,\n",
      "          8574,  2491,  5381,  2020,  2443,  1012, 12123,  8168,  2020,  4663,\n",
      "          1998, 20302, 23274,  2094,  2007,  2051,  1011, 10395, 10047, 23041,\n",
      "         11253,  7630, 14604, 12589,  4632, 22916, 12515,  1996, 12123,  3798,\n",
      "          1997,  4949, 22932,  1010, 16137,  2361,  1011,  1015,  1010,  1998,\n",
      "         16137,  2361,  1011,  1017,  1012,  1996,  2026, 24755, 25070, 13105,\n",
      "          1006, 18340,  5950,  1998,  2345,  1999, 14971,  6593,  2946,  1007,\n",
      "          2020,  7594,  2011,  4796,  2094,  2309,  1011, 26383, 15760, 14931,\n",
      "          1012,  1012,  2771,  5022,  2018,  2324,  1003,  3020, 12123,  3798,\n",
      "          1997,  4949, 22932,  1006, 26264,  2099,  2340,  1011,  2423,  1003,\n",
      "          1007,  2004,  4102,  2000,  1996,  7965,  2491,  2177,  1006,  1052,\n",
      "          1026,  1014,  1012, 25604,  1012,  2174,  1010,  4445, 18340,  5950,\n",
      "          1006, 12341,  2386,  1054,  6806,  1011,  1014,  1012,  1015,  1010,\n",
      "          1052,  1027,  1014,  1012,  2654,  1007,  4496,  2345,  1999, 14971,\n",
      "          6593,  2946,  1006, 12341,  2386,  1054,  6806,  1014,  1012,  6185,\n",
      "          1010,  1052,  1027,  1014,  1012,  6640,  1007, 23900,  2007, 12123,\n",
      "          3798,  1997,  4949, 22932,  1012, 10655,  1010, 16137,  2361,  1011,\n",
      "          1015,  1998, 16137,  2361,  1011,  1017,  2020,  8319,  1999,  2771,\n",
      "          5022,  1006,  1052,  1027,  1014,  1012,  4002,  2475,  1998,  1052,\n",
      "          1026,  1014,  1012, 25604,  1007,  1010,  2021,  1996,  3798,  2020,\n",
      "          2025, 23900,  2000,  9560,  1012, 12123,  3798,  1997,  4949, 22932,\n",
      "          1010, 16137,  2361,  1011,  1015,  1010,  1998, 16137,  2361,  1011,\n",
      "          1017,  2024,  6022,  3020,  1999,  5022,  2007,  2771,  4102,  2000,\n",
      "          7965,  2491,  5381,  1010,  2021,  2024,  2025,  3378,  2007,  2460,\n",
      "          1011,  2744,  9560,  7594,  2004, 18340,  5950,  1998,  2345,  1999,\n",
      "         14971,  6593,  1012,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "0\n",
      "RESULTS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "  text = ds_test[0]['text']\n",
    "  print(text)\n",
    "  label = ds_test[0]['labels']\n",
    "  print(label)\n",
    "  inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  print(inputs)\n",
    "  logits = model(**inputs).logits\n",
    "  predicted_class_id = logits.argmax().item()\n",
    "  print(predicted_class_id)\n",
    "  #pred_class = model.config.id2label[predicted_class_id]\n",
    "  #print(pred_class)\n",
    "  pred_class = id2label[predicted_class_id]\n",
    "  print(pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y_actual = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i in range(len(ds_test)):\n",
    "    text = ds_test[i]['text']\n",
    "    label_id = ds_test[i]['labels']\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    y_actual.append(id2label[label_id])\n",
    "    y_pred.append(id2label[predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4776547765477655"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_actual, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  BACKGROUND       0.31      0.21      0.25      2586\n",
      " CONCLUSIONS       0.38      0.06      0.11      4414\n",
      "     METHODS       0.52      0.57      0.55      9629\n",
      "   OBJECTIVE       0.76      0.20      0.31      2377\n",
      "     RESULTS       0.46      0.70      0.56     10262\n",
      "\n",
      "    accuracy                           0.48     29268\n",
      "   macro avg       0.49      0.35      0.35     29268\n",
      "weighted avg       0.48      0.48      0.44     29268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_actual, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 554   97  565   61 1309]\n",
      " [ 327  272 1033   19 2763]\n",
      " [ 391   92 5483   30 3633]\n",
      " [ 221   30  876  466  784]\n",
      " [ 305  226 2487   39 7205]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_actual, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
